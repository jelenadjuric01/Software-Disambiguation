Version 1 - corpus made from Benchmark.xslx (https://github.com/SoftwareUnderstanding/SoftwareDisambiguationBenchmark/blob/56bf84109d4be642a16037e5cf8cdd9862e7aa7e/benchmark/Benchmark.xlsx) and added star mentions (https://github.com/SoftwareUnderstanding/SoftwareDisambiguationBenchmark/blob/56bf84109d4be642a16037e5cf8cdd9862e7aa7e/benchmark/to_merge/benchmark_star.csv), both files were cleaned and updated for ground truths, candidate URLs only have URLs from ground truth
Version 2 - everything from Version 1 with added cleaned Kai Li Prism mentions (https://github.com/jelenadjuric01/Software-Disambiguation/blob/d5f61f9996c82aa7428ef5297936204eaf340dfc/corpus/Kai%20Li%20Prism.xlsx), and added random sampled examples from CZI (https://datadryad.org/dataset/doi:10.5061/dryad.6wwpzgn2c) linked dataset (github, cran and pypi) as well as sampled examples for top 10 Python (numpy, tensorflow, scikit-learn, pandas, matplotlib,requests,beautifulsoup4, flask, django, pytorch) and R packages (ggplot2, dplyr, data.table, tidyr, readr,stringr,lubridate, shiny, rmarkdown, knitr) also in this version are included positive and negative examples, as well as candidate URLs fetched from internet, websites are added in ground truth, but for them fetching metadata and calculating similarities wasn't implemented 
Version 3 - it is the same as version 2, but all URLs which are not GitHub, CRAN or PyPi are removed from ground truth as well as from candidates, for this version programming language metric was added
Version 3.1 - the same as version 3, but with added synonyms 
Version 3.2 - the same as version 3.1, but keywords from paper aren't fetched using openAlex tool, but using RAKE on paragraph surrounding software mention