Version 1 - corpus made from Benchmark.xslx (https://github.com/SoftwareUnderstanding/SoftwareDisambiguationBenchmark/blob/56bf84109d4be642a16037e5cf8cdd9862e7aa7e/benchmark/Benchmark.xlsx) and added star mentions (https://github.com/SoftwareUnderstanding/SoftwareDisambiguationBenchmark/blob/56bf84109d4be642a16037e5cf8cdd9862e7aa7e/benchmark/to_merge/benchmark_star.csv), both files were cleaned and updated for ground truths, candidate URLs only have URLs from ground truth
Version 2 - everything from Version 1 with added cleaned Kai Li Prism mentions (https://github.com/jelenadjuric01/Software-Disambiguation/blob/d5f61f9996c82aa7428ef5297936204eaf340dfc/corpus/Kai%20Li%20Prism.xlsx), and added random sampled examples from CZI (https://datadryad.org/dataset/doi:10.5061/dryad.6wwpzgn2c) linked dataset (github, cran and pypi) as well as sampled examples for top 10 Python (numpy, tensorflow, scikit-learn, pandas, matplotlib,requests,beautifulsoup4, flask, django, pytorch) and R packages (ggplot2, dplyr, data.table, tidyr, readr,stringr,lubridate, shiny, rmarkdown, knitr) also in this version are included positive and negative examples, as well as candidate URLs fetched from internet, websites are added in ground truth, but for them fetching metadata and calculating similarities wasn't implemented 
Version 3 - it is the same as version 2, but all URLs which are not GitHub, CRAN or PyPi are removed from ground truth as well as from candidates, for this version programming language metric was added
Version 3.1 - the same as version 3, but with added synonyms 
Version 3.2 - the same as version 3.1, but keywords from paper aren't fetched using openAlex tool, but using RAKE on paragraph surrounding software mention
Version 3.3 - the same as version 3.2, but for missing keywords from github we use RAKE 
Version 3.4 - the same as version 3.3, but instead of using TOPIC classifiers for missing keywords from PyPi, we use Rake
Version 3.5 - the same as version 3.4, only when Rake returns missing keywords, we fetch TOPIC classifiers like before
Version 3.6 - the same as version 3.5, but for PyPI metadata, if keywords are missing we first fetch TOPIC and if that is missing, then we use Rake
Version 3.7 - the same as version 3.6, but now we use SBERT model to calculate similarities between keywords, not BERT and RoBERTA
Version 3.8 - the same as version 3.6, but now we calculate similarities between names using Levenshtein 
Version 3.9 - the same as version 3.6, but the synonym similarity is calculated using Levenshtein
Version 3.10 - mix of version 3.8 and 3.9, all similarities like in the version 3.6, but name and synonyms calculated using Levenshtein
Version 3.11 - mix of versions 3.7 and 3.10, using SBERT for keywords, Levenshtein for name and synonyms
Version 3.12 - the same as version 3.6, but using BERT for paragraph similarity
Version 3.13 - the same as version 3.12, but we remove keywords completely and discard all PyPI URLs with short description (less than 400 characters) and no GitHub URL, if there is a GitHub URL, it adds it to candidate URLs
Version 3.14 - the same as version 3.12, but we remove keywords completely and discard all GitHub URLs that have empty README file
Version 3.15 - the same as version 3.12, but we remove keywords completely and discard PyPI URLs that have no GitHub and description shorter than 400 characters and all GitHubs that have empty README files