{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching URL candidate from GitHub, PyPI, CRAN and then finally from Google, excluding results from the previous three.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmlrpc.client\n",
    "from time import sleep\n",
    "from typing import List, Dict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googlesearch-python in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from googlesearch-python) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install googlesearch-python beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "GITHUB_API_URL = \"https://api.github.com/search/repositories\"\n",
    "CRAN_BASE_URL = \"https://cran.r-project.org/web/packages/{name}/index.html\"\n",
    "PYPI_SEARCH_URL = \"https://pypi.org/search/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please set the GITHUB_TOKEN environment variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m GITHUB_TOKEN \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGITHUB_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m GITHUB_TOKEN:\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease set the GITHUB_TOKEN environment variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Please set the GITHUB_TOKEN environment variable."
     ]
    }
   ],
   "source": [
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"Please set the GITHUB_TOKEN environment variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_github_urls(name: str, per_page: int = 5) -> List[str]:\n",
    "    \"\"\"Return up to `per_page` GitHub repo URLs matching `name`, using token if provided.\"\"\"\n",
    "    params = {\"q\": name, \"sort\": \"stars\", \"order\": \"desc\", \"per_page\": per_page}\n",
    "    headers = {}\n",
    "    if GITHUB_TOKEN:\n",
    "        headers[\"Authorization\"] = f\"token {GITHUB_TOKEN}\"\n",
    "\n",
    "    resp = requests.get(GITHUB_API_URL, params=params, headers=headers, timeout=10)\n",
    "    if resp.status_code == 401:\n",
    "        raise RuntimeError(\"GitHub API Unauthorized (401). Check your GITHUB_TOKEN environment variable.\")\n",
    "    resp.raise_for_status()\n",
    "    items = resp.json().get(\"items\", [])\n",
    "    return [item[\"html_url\"] for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pypi_urls(name: str, max_results: int = 5) -> List[str]:\n",
    "    \"\"\"Return up to `max_results` PyPI project URLs matching `name`, by scraping search results.\"\"\"\n",
    "    resp = requests.get(PYPI_SEARCH_URL, params={\"q\": name}, timeout=10)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    snippets = soup.select(\"a.package-snippet\")\n",
    "    urls = []\n",
    "    for snippet in snippets[:max_results]:\n",
    "        href = snippet.get(\"href\")\n",
    "        if href:\n",
    "            urls.append(f\"https://pypi.org{href}\")\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_cran_url(name: str) -> List[str]:\n",
    "    \"\"\"Return the CRAN package URL if it exists, else empty list.\"\"\"\n",
    "    url = CRAN_BASE_URL.format(name=name)\n",
    "    resp = requests.head(url, allow_redirects=True, timeout=5)\n",
    "    if resp.status_code == 200:\n",
    "        return [url]\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_google_urls(name: str, num_results: int = 5, sleep_interval: float = 2.0) -> List[str]:\n",
    "    \"\"\"\n",
    "    Use googlesearch to find other URLs, excluding GitHub, PyPI, and CRAN.\n",
    "    \"\"\"\n",
    "    query = f\"{name} -site:github.com -site:pypi.org -site:cran.r-project.org\"\n",
    "    # `num_results` and `sleep_interval` are supported by googlesearch-python\n",
    "    return list(search(query, num_results=num_results, sleep_interval=sleep_interval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_candidate_urls(software_names: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    For each software name, fetch candidate URLs in this order:\n",
    "      1. GitHub\n",
    "      2. PyPI\n",
    "      3. CRAN\n",
    "      4. General Google search (excluding above domains)\n",
    "    \"\"\"\n",
    "    all_results: Dict[str, List[str]] = {}\n",
    "    for name in software_names:\n",
    "        results = []\n",
    "\n",
    "        # GitHub\n",
    "        try:\n",
    "            results += fetch_github_urls(name)\n",
    "        except Exception as e:\n",
    "            print(f\"[!] GitHub fetch failed for '{name}': {e}\")\n",
    "\n",
    "        # PyPI\n",
    "        try:\n",
    "            results += fetch_pypi_urls(name)\n",
    "        except Exception as e:\n",
    "            print(f\"[!] PyPI fetch failed for '{name}': {e}\")\n",
    "\n",
    "        # CRAN\n",
    "        try:\n",
    "            results += fetch_cran_url(name)\n",
    "        except Exception as e:\n",
    "            print(f\"[!] CRAN check failed for '{name}': {e}\")\n",
    "\n",
    "        # Google\n",
    "        try:\n",
    "            sleep(1)\n",
    "            results += fetch_google_urls(name)\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Google search failed for '{name}': {e}\")\n",
    "\n",
    "        # dedupe, preserve order\n",
    "        seen = set()\n",
    "        deduped = []\n",
    "        for url in results:\n",
    "            if url not in seen:\n",
    "                seen.add(url)\n",
    "                deduped.append(url)\n",
    "        all_results[name] = deduped\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] GitHub fetch failed for 'tensorflow': 401 Client Error: Unauthorized for url: https://api.github.com/search/repositories?q=tensorflow&sort=stars&order=desc&per_page=5\n",
      "[!] PyPI fetch failed for 'tensorflow': <Fault -32500: \"RuntimeError: PyPI no longer supports 'pip search' (or XML-RPC search). Please use https://pypi.org/search (via a browser) instead. See https://warehouse.pypa.io/api-reference/xml-rpc.html#deprecated-methods for more information.\">\n",
      "[!] Google search failed for 'tensorflow': search() got an unexpected keyword argument 'pause'\n",
      "[!] GitHub fetch failed for 'dplyr': 401 Client Error: Unauthorized for url: https://api.github.com/search/repositories?q=dplyr&sort=stars&order=desc&per_page=5\n",
      "[!] PyPI fetch failed for 'dplyr': <Fault -32500: \"RuntimeError: PyPI no longer supports 'pip search' (or XML-RPC search). Please use https://pypi.org/search (via a browser) instead. See https://warehouse.pypa.io/api-reference/xml-rpc.html#deprecated-methods for more information.\">\n",
      "[!] Google search failed for 'dplyr': search() got an unexpected keyword argument 'pause'\n",
      "[!] GitHub fetch failed for 'requests': 401 Client Error: Unauthorized for url: https://api.github.com/search/repositories?q=requests&sort=stars&order=desc&per_page=5\n",
      "[!] PyPI fetch failed for 'requests': <Fault -32500: \"RuntimeError: PyPI no longer supports 'pip search' (or XML-RPC search). Please use https://pypi.org/search (via a browser) instead. See https://warehouse.pypa.io/api-reference/xml-rpc.html#deprecated-methods for more information.\">\n",
      "[!] Google search failed for 'requests': search() got an unexpected keyword argument 'pause'\n",
      "\n",
      "tensorflow:\n",
      "  - https://cran.r-project.org/web/packages/tensorflow/index.html\n",
      "\n",
      "dplyr:\n",
      "  - https://cran.r-project.org/web/packages/dplyr/index.html\n",
      "\n",
      "requests:\n"
     ]
    }
   ],
   "source": [
    "software_list = [\"tensorflow\", \"dplyr\", \"requests\"]\n",
    "candidates = fetch_candidate_urls(software_list)\n",
    "for name, urls in candidates.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for u in urls:\n",
    "        print(\"  -\", u)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
