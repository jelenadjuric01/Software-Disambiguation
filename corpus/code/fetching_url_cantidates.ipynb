{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching URL candidate from GitHub, PyPI, CRAN and then finally from Google, excluding results from the previous three.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmlrpc.client\n",
    "from time import sleep\n",
    "from typing import List, Dict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googlesearch-python in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from googlesearch-python) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install googlesearch-python beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "GITHUB_API_URL = \"https://api.github.com/search/repositories\"\n",
    "CRAN_BASE_URL = \"https://cran.r-project.org/web/packages/{name}/index.html\"\n",
    "PYPI_JSON_API    = \"https://pypi.org/pypi/{name}/json\"\n",
    "PYPI_PROJECT_URL = \"https://pypi.org/project/{name}/\"\n",
    "PYPI_SEARCH_URL  = \"https://pypi.org/search/?q={query}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"Please set the GITHUB_TOKEN environment variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_github_urls(name: str, per_page: int = 5) -> List[str]:\n",
    "    \"\"\"Return up to `per_page` GitHub repo URLs matching `name`, using token if provided.\"\"\"\n",
    "    params = {\"q\": name, \"sort\": \"stars\", \"order\": \"desc\", \"per_page\": per_page}\n",
    "    headers = {}\n",
    "    if GITHUB_TOKEN:\n",
    "        headers[\"Authorization\"] = f\"token {GITHUB_TOKEN}\"\n",
    "\n",
    "    resp = requests.get(GITHUB_API_URL, params=params, headers=headers, timeout=10)\n",
    "    if resp.status_code == 401:\n",
    "        raise RuntimeError(\"GitHub API Unauthorized (401). Check your GITHUB_TOKEN environment variable.\")\n",
    "    resp.raise_for_status()\n",
    "    items = resp.json().get(\"items\", [])\n",
    "    return [item[\"html_url\"] for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_pkg_re = re.compile(r\"^/project/([^/]+)/?$\")\n",
    "\n",
    "def fetch_pypi_urls(name: str, max_results: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Return up to `max_results` *existing* PyPI project root URLs for `name`,\n",
    "    preserving whatever final URL PyPI redirects you to.\n",
    "    \"\"\"\n",
    "    raw_urls: List[str] = []\n",
    "\n",
    "    # 1) Try the JSON API (gives you the canonical root if it exists)\n",
    "    try:\n",
    "        api_url = PYPI_JSON_API.format(name=name)\n",
    "        resp = requests.get(api_url, timeout=10)\n",
    "        if resp.status_code == 200:\n",
    "            raw_urls.append(f\"https://pypi.org/project/{name}/\")\n",
    "    except requests.RequestException:\n",
    "        pass\n",
    "\n",
    "    # 2) Scrape the official PyPI search page\n",
    "    try:\n",
    "        resp = requests.get(PYPI_SEARCH_URL.format(query=name), timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        snippets = soup.select(\"a.package-snippet\")[:max_results]\n",
    "        for a in snippets:\n",
    "            href = a.get(\"href\", \"\")\n",
    "            if _pkg_re.match(href):\n",
    "                raw_urls.append(urljoin(\"https://pypi.org\", href))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) Normalize + HEAD‐check + dedupe\n",
    "    seen = set()\n",
    "    valid = []\n",
    "    for url in raw_urls:\n",
    "        # ensure trailing slash\n",
    "        if not url.endswith(\"/\"):\n",
    "            url += \"/\"\n",
    "        try:\n",
    "            head = requests.head(url, allow_redirects=True, timeout=5)\n",
    "            if head.status_code != 200:\n",
    "                continue\n",
    "            real = head.url\n",
    "            if not real.endswith(\"/\"):\n",
    "                real += \"/\"\n",
    "        except requests.RequestException:\n",
    "            continue\n",
    "\n",
    "        if real not in seen:\n",
    "            seen.add(real)\n",
    "            valid.append(real)\n",
    "            if len(valid) >= max_results:\n",
    "                break\n",
    "\n",
    "    return valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "_pkg_re = re.compile(r\"https?://cran\\.r-project\\.org/web/packages/([^/]+)/\")\n",
    "\n",
    "\n",
    "def fetch_cran_urls(name: str, max_results: int = 5) -> List[str]:\n",
    "    \"\"\"Return up to `max_results` CRAN package URLs matching `name`.\"\"\"\n",
    "    raw_urls: List[str] = []\n",
    "\n",
    "    # 1) Exact lookup via GET\n",
    "    exact_url = CRAN_BASE_URL.format(name=name)\n",
    "    try:\n",
    "        resp = requests.get(exact_url, allow_redirects=True, timeout=10)\n",
    "        if resp.status_code == 200:\n",
    "            raw_urls.append(exact_url)\n",
    "    except requests.RequestException:\n",
    "        pass\n",
    "\n",
    "    # 2) Fuzzy search via Google for any CRAN/web/packages URLs\n",
    "    query = f\"site:cran.r-project.org/web/packages {name}\"\n",
    "    try:\n",
    "        for url in search(query, num_results=max_results, sleep_interval=2.0):\n",
    "            if \"cran.r-project.org/web/packages/\" in url:\n",
    "                raw_urls.append(url)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) Extract package names and rebuild canonical index URLs\n",
    "    seen = set()\n",
    "    canonical = []\n",
    "    for u in raw_urls:\n",
    "        m = _pkg_re.match(u)\n",
    "        if m:\n",
    "            pkg = m.group(1)\n",
    "            if pkg not in seen:\n",
    "                seen.add(pkg)\n",
    "                canonical.append(CRAN_BASE_URL.format(name=pkg))\n",
    "        # else: skip any URL that isn’t a top-level package path\n",
    "\n",
    "    # 4) Limit to max_results\n",
    "    return canonical[:max_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domains to exclude in general web search\n",
    "EXCLUDE_SITES = [\n",
    "    \"github.com\",\n",
    "    \"pypi.org\",\n",
    "    \"cran.r-project.org\",\n",
    "    \"youtube.com\",\n",
    "    \"youtu.be\",\n",
    "    \"medium.com\",\n",
    "    \"stackoverflow.com\",\n",
    "    \"reddit.com\",\n",
    "    \"twitter.com\",\n",
    "    \"facebook.com\",\n",
    "    \"linkedin.com\",\n",
    "    \"geeksforgeeks.org\",\n",
    "    \"w3schools.com\",\n",
    "    \"tutorialspoint.com\"\n",
    "]\n",
    "def fetch_google_urls(name: str, num_results: int = 5, sleep_interval: float = 2.0) -> List[str]:\n",
    "    \"\"\"\n",
    "    Use googlesearch to find other URLs for `name`, excluding known non-software domains.\n",
    "\n",
    "    `EXCLUDE_SITES` contains domains to omit (e.g., YouTube, social media, Q&A sites).\n",
    "    \"\"\"\n",
    "    # Build exclude portion of query\n",
    "    exclude_queries = \" \".join(f\"-site:{domain}\" for domain in EXCLUDE_SITES)\n",
    "    query = f\"{name} {exclude_queries}\"\n",
    "    # `num_results` and `sleep_interval` are supported parameters\n",
    "    return list(search(query, num_results=num_results, sleep_interval=sleep_interval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_candidate_urls(software_names: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    For each software name, fetch candidate URLs in this order:\n",
    "      1. GitHub\n",
    "      2. PyPI\n",
    "      3. CRAN\n",
    "      4. General Google search (excluding above domains)\n",
    "    \"\"\"\n",
    "    all_results: Dict[str, List[str]] = {}\n",
    "    for name in software_names:\n",
    "        results = []\n",
    "\n",
    "        # GitHub\n",
    "        try:\n",
    "            results += fetch_github_urls(name)\n",
    "        except Exception as e:\n",
    "            print(f\"[!] GitHub fetch failed for '{name}': {e}\")\n",
    "\n",
    "        # PyPI\n",
    "        try:\n",
    "            results += fetch_pypi_urls(name)\n",
    "        except Exception as e:\n",
    "            print(f\"[!] PyPI fetch failed for '{name}': {e}\")\n",
    "\n",
    "        # CRAN\n",
    "        try:\n",
    "            results += fetch_cran_urls(name)\n",
    "        except Exception as e:\n",
    "            print(f\"[!] CRAN check failed for '{name}': {e}\")\n",
    "\n",
    "        # Google\n",
    "        try:\n",
    "            sleep(1)\n",
    "            results += fetch_google_urls(name)\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Google search failed for '{name}': {e}\")\n",
    "\n",
    "        # dedupe, preserve order\n",
    "        seen = set()\n",
    "        deduped = []\n",
    "        for url in results:\n",
    "            if url not in seen:\n",
    "                seen.add(url)\n",
    "                deduped.append(url)\n",
    "        all_results[name] = deduped\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensorflow:\n",
      "  - https://github.com/tensorflow/tensorflow\n",
      "  - https://github.com/huggingface/transformers\n",
      "  - https://github.com/tensorflow/models\n",
      "  - https://github.com/fighting41love/funNLP\n",
      "  - https://github.com/keras-team/keras\n",
      "  - https://pypi.org/project/tensorflow/\n",
      "  - https://www.tensorflow.org/\n",
      "  - https://en.wikipedia.org/wiki/TensorFlow\n",
      "  - https://developers.google.com/learn/pathways/tensorflow\n",
      "  - https://ai.google.dev/edge/litert\n",
      "  - https://developer.apple.com/metal/tensorflow-plugin/\n",
      "\n",
      "dPLyr:\n",
      "  - https://github.com/tidyverse/dplyr\n",
      "  - https://github.com/machow/siuba\n",
      "  - https://github.com/sparklyr/sparklyr\n",
      "  - https://github.com/kieferk/dfply\n",
      "  - https://github.com/business-science/tidyquant\n",
      "  - https://dplyr.tidyverse.org/\n",
      "  - https://www.google.com/search?num=7\n",
      "  - https://en.wikipedia.org/wiki/Dplyr\n",
      "  - https://datacarpentry.github.io/R-genomics/04-dplyr\n",
      "  - https://www.kaggle.com/code/jessemostipak/dive-into-dplyr-tutorial-1\n",
      "\n",
      "ReqUests:\n",
      "  - https://github.com/git/git\n",
      "  - https://github.com/psf/requests\n",
      "  - https://github.com/request/request\n",
      "  - https://github.com/guzzle/guzzle\n",
      "  - https://github.com/yeongpin/cursor-free-vip\n",
      "  - https://pypi.org/project/requests/\n",
      "  - https://requests.readthedocs.io/\n",
      "  - https://realpython.com/python-requests/\n",
      "  - https://en.wikipedia.org/wiki/Requests_(software)\n",
      "  - https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Methods\n",
      "  - https://laravel.com/docs/12.x/requests\n"
     ]
    }
   ],
   "source": [
    "software_list = [\"Tensorflow\", \"dPLyr\", \"ReqUests\"]\n",
    "candidates = fetch_candidate_urls(software_list)\n",
    "for name, urls in candidates.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for u in urls:\n",
    "        print(\"  -\", u)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
