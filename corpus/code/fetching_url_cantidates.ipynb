{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching URL candidate from GitHub, PyPI, CRAN and then finally from Google, excluding results from the previous three.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "from typing import List, Dict\n",
    "import os\n",
    "import pandas as pd\n",
    "import difflib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googlesearch-python in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from googlesearch-python) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install googlesearch-python beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import xmlrpc.client\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"Please set the GITHUB_TOKEN environment variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_API_URL = \"https://api.github.com/search/repositories\"\n",
    "\n",
    "def fetch_github_urls(name: str, per_page: int = 5) -> List[str]:\n",
    "    \"\"\"Return up to `per_page` GitHub repo URLs matching `name`, using token if provided.\"\"\"\n",
    "    params = {\"q\": name, \"sort\": \"stars\", \"order\": \"desc\", \"per_page\": per_page}\n",
    "    headers = {}\n",
    "    if GITHUB_TOKEN:\n",
    "        headers[\"Authorization\"] = f\"token {GITHUB_TOKEN}\"\n",
    "\n",
    "    resp = requests.get(GITHUB_API_URL, params=params, headers=headers, timeout=10)\n",
    "    if resp.status_code == 401:\n",
    "        raise RuntimeError(\"GitHub API Unauthorized (401). Check your GITHUB_TOKEN environment variable.\")\n",
    "    resp.raise_for_status()\n",
    "    items = resp.json().get(\"items\", [])\n",
    "    return [item[\"html_url\"] for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYPI_JSON_URL    = \"https://pypi.org/pypi/{pkg}/json\"\n",
    "PYPI_PROJECT_URL = \"https://pypi.org/project/{pkg}/\"\n",
    "\n",
    "@lru_cache(maxsize=512)\n",
    "def _get_pypi_info(pkg: str, timeout: float = 10.0) -> Dict:\n",
    "    \"\"\"\n",
    "    Fetches the JSON info block for `pkg`, or returns {} on error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        r = requests.get(PYPI_JSON_URL.format(pkg=pkg), timeout=timeout)\n",
    "        if r.status_code == 200:\n",
    "            return r.json().get(\"info\", {})\n",
    "    except requests.RequestException:\n",
    "        pass\n",
    "    return {}\n",
    "\n",
    "@lru_cache(maxsize=256)\n",
    "def fetch_pypi_urls(\n",
    "    pkg_name: str,\n",
    "    max_results: int = 5,\n",
    "    timeout: float = 10.0\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    1) Exact lookup via JSON API → returns info['package_url'] (or info['project_url'])\n",
    "    2) Fuzzy lookup via XML‐RPC + JSON API per hit\n",
    "    \"\"\"\n",
    "    urls: List[str] = []\n",
    "\n",
    "    # 1) Exact match\n",
    "    info = _get_pypi_info(pkg_name, timeout)\n",
    "    if info:\n",
    "        url = info.get(\"package_url\") or info.get(\"project_url\")\n",
    "        if url:\n",
    "            urls.append(url)\n",
    "\n",
    "    if len(urls) >= max_results:\n",
    "        return urls[:max_results]\n",
    "\n",
    "    # 2) Fuzzy search\n",
    "    try:\n",
    "        client = xmlrpc.client.ServerProxy(\"https://pypi.org/pypi\")\n",
    "        hits = client.search({\"name\": pkg_name}, \"or\")\n",
    "        seen = set(pkg_name.lower())\n",
    "\n",
    "        for hit in hits:\n",
    "            name = hit.get(\"name\")\n",
    "            key  = name.lower() if name else None\n",
    "            if not key or key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "\n",
    "            # pull its JSON info to get the true URL\n",
    "            info = _get_pypi_info(name, timeout)\n",
    "            if info:\n",
    "                url = info.get(\"package_url\") or info.get(\"project_url\")\n",
    "                if url:\n",
    "                    urls.append(url)\n",
    "                    if len(urls) >= max_results:\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "            # fallback (should rarely be needed)\n",
    "            urls.append(PYPI_PROJECT_URL.format(pkg=name))\n",
    "            if len(urls) >= max_results:\n",
    "                break\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return urls[:max_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAN_PACKAGES_URL = \"https://cran.r-project.org/src/contrib/PACKAGES\"\n",
    "CRAN_BASE_URL     = \"https://cran.r-project.org/web/packages/{pkg}/index.html\"\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _load_cran_packages(timeout: float = 10.0) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch and parse the CRAN PACKAGES index into a list of package names.\n",
    "    Cached in memory so we only download it once.\n",
    "    \"\"\"\n",
    "    resp = requests.get(CRAN_PACKAGES_URL, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    pkgs = []\n",
    "    for line in resp.text.splitlines():\n",
    "        if line.startswith(\"Package:\"):\n",
    "            pkgs.append(line.split(\":\", 1)[1].strip())\n",
    "    return pkgs\n",
    "\n",
    "@lru_cache(maxsize=256)\n",
    "def fetch_cran_urls(\n",
    "    name: str,\n",
    "    max_results: int = 5,\n",
    "    timeout: float = 10.0\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Return up to `max_results` canonical CRAN URLs for packages matching `name`:\n",
    "      1) exact match\n",
    "      2) substring match\n",
    "      3) fuzzy match via difflib\n",
    "    \"\"\"\n",
    "    pkgs = _load_cran_packages(timeout)\n",
    "    urls: List[str] = []\n",
    "    name_lower = name.lower()\n",
    "\n",
    "    # 1) Exact\n",
    "    if name in pkgs:\n",
    "        urls.append(CRAN_BASE_URL.format(pkg=name))\n",
    "\n",
    "    # 2) Substring\n",
    "    if len(urls) < max_results:\n",
    "        subs = [p for p in pkgs if name_lower in p.lower() and p != name]\n",
    "        for p in subs:\n",
    "            if len(urls) >= max_results:\n",
    "                break\n",
    "            urls.append(CRAN_BASE_URL.format(pkg=p))\n",
    "\n",
    "    # 3) Fuzzy\n",
    "    if len(urls) < max_results:\n",
    "        # cutoff=0.6 is a sensible default; tweak as needed\n",
    "        fuzzy = difflib.get_close_matches(name, pkgs, n=max_results, cutoff=0.6)\n",
    "        for p in fuzzy:\n",
    "            if len(urls) >= max_results:\n",
    "                break\n",
    "            if p not in [u.split(\"/\")[-2] for u in urls]:\n",
    "                urls.append(CRAN_BASE_URL.format(pkg=p))\n",
    "\n",
    "    return urls[:max_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.environ[\"GOOGLE_API_KEY\"]\n",
    "CSE_ID         = os.environ[\"GOOGLE_CSE_ID\"]\n",
    "EXCLUDE_SITES = [\n",
    "    \"github.com\",\n",
    "    \"pypi.org\",\n",
    "    \"cran.r-project.org\",\n",
    "    \"youtube.com\",\n",
    "    \"youtu.be\",\n",
    "    \"medium.com\",\n",
    "    \"stackoverflow.com\",\n",
    "    \"reddit.com\",\n",
    "    \"twitter.com\",\n",
    "    \"facebook.com\",\n",
    "    \"linkedin.com\",\n",
    "    \"geeksforgeeks.org\",\n",
    "    \"w3schools.com\",\n",
    "    \"tutorialspoint.com\"\n",
    "]\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def fetch_google_urls(\n",
    "    name: str,\n",
    "    num_results: int = 10\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch up to `num_results` web URLs for `name` via the\n",
    "    Google Custom Search JSON API, excluding any in EXCLUDE_SITES.\n",
    "    \"\"\"\n",
    "    # build the \"-site:...\" string\n",
    "    exclude_query = \" \".join(f\"-site:{d}\" for d in EXCLUDE_SITES)\n",
    "    query = f\"{name} {exclude_query}\"\n",
    "\n",
    "    urls: List[str] = []\n",
    "    page_size = 10  # API max per request\n",
    "\n",
    "    for start in range(1, num_results + 1, page_size):\n",
    "        params = {\n",
    "            \"key\":   GOOGLE_API_KEY,\n",
    "            \"cx\":    CSE_ID,\n",
    "            \"q\":     query,\n",
    "            \"start\": start,\n",
    "            \"num\":   min(page_size, num_results - len(urls)),\n",
    "        }\n",
    "        r = requests.get(\"https://www.googleapis.com/customsearch/v1\", params=params, timeout=5)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        for item in data.get(\"items\", []):\n",
    "            urls.append(item[\"link\"])\n",
    "            if len(urls) >= num_results:\n",
    "                break\n",
    "        if len(urls) >= num_results:\n",
    "            break\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_candidate_urls(name: str) -> set[str]:\n",
    "    \"\"\"\n",
    "    For each software name, fetch candidate URLs in this order:\n",
    "      1. GitHub\n",
    "      2. PyPI\n",
    "      3. CRAN\n",
    "      4. General Google search (excluding above domains)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # GitHub\n",
    "    try:\n",
    "        results += fetch_github_urls(name)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] GitHub fetch failed for '{name}': {e}\")\n",
    "\n",
    "    # PyPI\n",
    "    try:\n",
    "        results += fetch_pypi_urls(name)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] PyPI fetch failed for '{name}': {e}\")\n",
    "\n",
    "    # CRAN\n",
    "    try:\n",
    "        results += fetch_cran_urls(name)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] CRAN check failed for '{name}': {e}\")\n",
    "\n",
    "    # Google\n",
    "    try:\n",
    "        sleep(1)\n",
    "        results += fetch_google_urls(name)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Google search failed for '{name}': {e}\")\n",
    "\n",
    "    # dedupe, preserve order\n",
    "    return set(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'names = [\"TensOrflow\",\\'tidyr\\',\\'reQuests\\']\\nfor name in names:\\n    print(f\"Candidate URLs for \\'{name}\\':\")\\n    urls = fetch_candidate_urls(name)\\n    for url in urls:\\n        print(f\"  - {url}\")\\n    print()'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"names = [\"TensOrflow\",'tidyr','reQuests']\n",
    "for name in names:\n",
    "    print(f\"Candidate URLs for '{name}':\")\n",
    "    urls = fetch_candidate_urls(name)\n",
    "    for url in urls:\n",
    "        print(f\"  - {url}\")\n",
    "    print()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] GitHub fetch failed for 'BeautifulSoup': 403 Client Error: Forbidden for url: https://api.github.com/search/repositories?q=BeautifulSoup&sort=stars&order=desc&per_page=5\n",
      "[!] GitHub fetch failed for 'BeautifulSoup': 403 Client Error: Forbidden for url: https://api.github.com/search/repositories?q=BeautifulSoup&sort=stars&order=desc&per_page=5\n",
      "[!] GitHub fetch failed for 'BeautifulSoup': 403 Client Error: Forbidden for url: https://api.github.com/search/repositories?q=BeautifulSoup&sort=stars&order=desc&per_page=5\n",
      "[!] GitHub fetch failed for 'BeautifulSoup': 403 Client Error: Forbidden for url: https://api.github.com/search/repositories?q=BeautifulSoup&sort=stars&order=desc&per_page=5\n",
      "[!] GitHub fetch failed for 'BeautifulSoup': 403 Client Error: Forbidden for url: https://api.github.com/search/repositories?q=BeautifulSoup&sort=stars&order=desc&per_page=5\n",
      "[!] GitHub fetch failed for 'BeautifulSoup': 403 Client Error: Forbidden for url: https://api.github.com/search/repositories?q=BeautifulSoup&sort=stars&order=desc&per_page=5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[229], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m url:\n\u001b[0;32m     13\u001b[0m             candidates[name]\u001b[38;5;241m.\u001b[39madd(url)\n\u001b[1;32m---> 14\u001b[0m             candidates[name]\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mfetch_candidate_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(candidates)\n",
      "Cell \u001b[1;32mIn[227], line 31\u001b[0m, in \u001b[0;36mfetch_candidate_urls\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Google\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m#results += fetch_google_urls(name)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus = pd.read_excel(\"../corpus_v2.xlsx\")\n",
    "unique_names = corpus['name'].unique()\n",
    "candidates = {name: set() for name in unique_names}\n",
    "\n",
    "for _, row in corpus.iterrows():\n",
    "    name = row['name']\n",
    "    urls_str = row.get('candidate_urls', '')\n",
    "    if not isinstance(urls_str, str):\n",
    "        continue\n",
    "    for url in urls_str.split(','):\n",
    "        url = url.strip()\n",
    "        if url:\n",
    "            candidates[name].add(url)\n",
    "            candidates[name].update(fetch_candidate_urls(name))\n",
    "print(candidates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
