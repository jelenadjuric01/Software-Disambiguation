{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching URL candidate from GitHub, PyPI, CRAN and then finally from Google, excluding results from the previous three. The notebook begins by  defining four source‐specific fetchers (for GitHub, PyPI, CRAN and general Google searches), a handful of URL‐normalization and deduplication utilities, and simple cache‐loading/saving routines. At its heart is the function fetch_candidate_urls(name: str) -> set[str], which takes a software name and returns a deduplicated set of URLs by calling, in order: (1) fetch_github_urls(name), which queries GitHub’s Search Repositories API with a “{name} in:name” filter, sorts results by star count descending (so the most popular repos come first), and retries up to a configurable limit when rate-limited; (2) fetch_pypi_urls(name), which first attempts an exact JSON-API lookup (https://pypi.org/pypi/{pkg}/json) to retrieve the package URL, then falls back on fuzzy searches—returning closest name-matches in descending order of string similarity—until it reaches its max_results cap; (3) fetch_cran_urls(name), which loads and caches CRAN’s master PACKAGES index, returns an exact package-page link if available, then uses difflib.get_close_matches (with a 0.6 cutoff) to find near names sorted by match quality, up to its limit; and (4) fetch_google_urls(name), which issues a Google Custom Search API call (rotating through multiple API keys, paginating with start/num parameters), excludes known domains (GitHub, PyPI, CRAN) to avoid duplicates, and returns the top results by Google’s default relevance. After collecting all candidates, it simply converts the combined list into a Python set, removing duplicates (though without preserving cross‐source ranking), and hands that back. Finally, an orchestrator function reads an Excel corpus, merges these fetched URLs with any cache of previously found candidates, deduplicates, updates the DataFrame, and writes the results back to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import List, Dict, Set\n",
    "import os\n",
    "import pandas as pd\n",
    "import difflib\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googlesearch-python in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from googlesearch-python) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install googlesearch-python beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import xmlrpc.client\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "GITHUB_API_URL = \"https://api.github.com/search/repositories\"\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"Please set the GITHUB_TOKEN environment variable.\")\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "    \"Accept\":        \"application/vnd.github.v3+json\",\n",
    "    \"User-Agent\":    \"my-software-disambiguator\"  # any non-empty string\n",
    "}\n",
    "\n",
    "def fetch_github_urls(\n",
    "    name: str,\n",
    "    per_page: int = 5,\n",
    "    max_retries: int = 3\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Return up to `per_page` GitHub repo URLs matching `name`, handling rate limits.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"q\":        f\"{name} in:name\",\n",
    "        \"sort\":     \"stars\",\n",
    "        \"order\":    \"desc\",\n",
    "        \"per_page\": per_page\n",
    "    }\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        resp = requests.get(GITHUB_API_URL, params=params, headers=HEADERS, timeout=10)\n",
    "        # 403 could be a rate-limit on the Search API\n",
    "        if resp.status_code == 403:\n",
    "            reset_ts = int(resp.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait = max(reset_ts - time.time(), 1)\n",
    "            print(f\"[Attempt {attempt}] Rate limited. Sleeping {int(wait)}s until reset…\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        # a 401 means bad token, 404 would be weird, anything else we raise\n",
    "        resp.raise_for_status()\n",
    "        items = resp.json().get(\"items\", [])\n",
    "        return [item[\"html_url\"] for item in items]\n",
    "\n",
    "    # If we exhausted retries\n",
    "    raise RuntimeError(f\"GitHub search for '{name}' failed after {max_retries} attempts (last status: {resp.status_code})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYPI_JSON_URL    = \"https://pypi.org/pypi/{pkg}/json\"\n",
    "PYPI_PROJECT_URL = \"https://pypi.org/project/{pkg}/\"\n",
    "\n",
    "@lru_cache(maxsize=512)\n",
    "def _get_pypi_info(pkg: str, timeout: float = 10.0) -> Dict:\n",
    "    \"\"\"\n",
    "    Fetches the JSON info block for `pkg`, or returns {} on error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        r = requests.get(PYPI_JSON_URL.format(pkg=pkg), timeout=timeout)\n",
    "        if r.status_code == 200:\n",
    "            return r.json().get(\"info\", {})\n",
    "    except requests.RequestException:\n",
    "        pass\n",
    "    return {}\n",
    "\n",
    "@lru_cache(maxsize=256)\n",
    "def fetch_pypi_urls(\n",
    "    pkg_name: str,\n",
    "    max_results: int = 5,\n",
    "    timeout: float = 10.0\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    1) Exact lookup via JSON API → returns info['package_url'] (or info['project_url'])\n",
    "    2) Fuzzy lookup via XML‐RPC + JSON API per hit\n",
    "    \"\"\"\n",
    "    urls: List[str] = []\n",
    "\n",
    "    # 1) Exact match\n",
    "    info = _get_pypi_info(pkg_name, timeout)\n",
    "    if info:\n",
    "        url = info.get(\"package_url\") or info.get(\"project_url\")\n",
    "        if url:\n",
    "            urls.append(url)\n",
    "\n",
    "    if len(urls) >= max_results:\n",
    "        return urls[:max_results]\n",
    "\n",
    "    # 2) Fuzzy search\n",
    "    try:\n",
    "        client = xmlrpc.client.ServerProxy(\"https://pypi.org/pypi\")\n",
    "        hits = client.search({\"name\": pkg_name}, \"or\")\n",
    "        seen = set(pkg_name.lower())\n",
    "\n",
    "        for hit in hits:\n",
    "            name = hit.get(\"name\")\n",
    "            key  = name.lower() if name else None\n",
    "            if not key or key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "\n",
    "            # pull its JSON info to get the true URL\n",
    "            info = _get_pypi_info(name, timeout)\n",
    "            if info:\n",
    "                url = info.get(\"package_url\") or info.get(\"project_url\")\n",
    "                if url:\n",
    "                    urls.append(url)\n",
    "                    if len(urls) >= max_results:\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "            # fallback (should rarely be needed)\n",
    "            urls.append(PYPI_PROJECT_URL.format(pkg=name))\n",
    "            if len(urls) >= max_results:\n",
    "                break\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return urls[:max_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAN_PACKAGES_URL = \"https://cran.r-project.org/src/contrib/PACKAGES\"\n",
    "CRAN_BASE_URL     = \"https://cran.r-project.org/web/packages/{pkg}/index.html\"\n",
    "CRAN_SHORT_URL    = \"https://cran.r-project.org/package={pkg}\"\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _load_cran_packages(timeout: float = 10.0) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch and parse the CRAN PACKAGES index into a list of package names.\n",
    "    Cached in memory so we only download it once.\n",
    "    \"\"\"\n",
    "    resp = requests.get(CRAN_PACKAGES_URL, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    pkgs = []\n",
    "    for line in resp.text.splitlines():\n",
    "        if line.startswith(\"Package:\"):\n",
    "            pkgs.append(line.split(\":\", 1)[1].strip())\n",
    "    return pkgs\n",
    "\n",
    "@lru_cache(maxsize=256)\n",
    "def fetch_cran_urls(\n",
    "    name: str,\n",
    "    max_results: int = 5,\n",
    "    timeout: float = 10.0\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Return up to `max_results` canonical CRAN URLs for packages matching `name`:\n",
    "      1) exact match\n",
    "      2) substring match\n",
    "      3) fuzzy match via difflib\n",
    "    \"\"\"\n",
    "    pkgs = _load_cran_packages(timeout)\n",
    "    urls: List[str] = []\n",
    "    name_lower = name.lower()\n",
    "\n",
    "    # 1) Exact\n",
    "    if name in pkgs:\n",
    "        urls.append(CRAN_SHORT_URL.format(pkg=name))\n",
    "\n",
    "    # 2) Substring\n",
    "    if len(urls) < max_results:\n",
    "        subs = [p for p in pkgs if name_lower in p.lower() and p != name]\n",
    "        for p in subs:\n",
    "            if len(urls) >= max_results:\n",
    "                break\n",
    "            urls.append(CRAN_SHORT_URL.format(pkg=p))\n",
    "\n",
    "    # 3) Fuzzy\n",
    "    if len(urls) < max_results:\n",
    "        # cutoff=0.6 is a sensible default; tweak as needed\n",
    "        fuzzy = difflib.get_close_matches(name, pkgs, n=max_results, cutoff=0.6)\n",
    "        for p in fuzzy:\n",
    "            if len(urls) >= max_results:\n",
    "                break\n",
    "            if p not in [u.split(\"/\")[-2] for u in urls]:\n",
    "                urls.append(CRAN_SHORT_URL.format(pkg=p))\n",
    "\n",
    "    return urls[:max_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_URL = \"https://www.googleapis.com/customsearch/v1\"\n",
    "\n",
    "API_KEYS = [\n",
    "    os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    os.getenv(\"GOOGLE_API_KEY1\"),\n",
    "    os.getenv(\"GOOGLE_API_KEY2\"),\n",
    "    os.getenv(\"GOOGLE_API_KEY3\")\n",
    "]\n",
    "\n",
    "API_KEYS = [k for k in API_KEYS if k]\n",
    "\n",
    "CSE_ID = os.environ[\"GOOGLE_CSE_ID\"]\n",
    "EXCLUDE_SITES = [\n",
    "    \"github.com\",\n",
    "    \"pypi.org\",\n",
    "    \"cran.r-project.org\",\n",
    "    \"youtube.com\",\n",
    "    \"youtu.be\",\n",
    "    \"medium.com\",\n",
    "    \"stackoverflow.com\",\n",
    "    \"reddit.com\",\n",
    "    \"twitter.com\",\n",
    "    \"facebook.com\",\n",
    "    \"linkedin.com\",\n",
    "    \"geeksforgeeks.org\",\n",
    "    \"w3schools.com\",\n",
    "    \"tutorialspoint.com\"\n",
    "]\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def fetch_google_urls(name: str, num_results: int = 5) -> List[str]:\n",
    "    exclude_query = \" \".join(f\"-site:{d}\" for d in EXCLUDE_SITES)\n",
    "    query = f\"{name} {exclude_query}\"\n",
    "    key_idx = 0\n",
    "    urls = []\n",
    "    page_size = 10\n",
    "    nkeys = len(API_KEYS)\n",
    "    # loop over pages of results\n",
    "    for start in range(1, num_results + 1, page_size):\n",
    "        params = {\n",
    "            \"key\":   API_KEYS[key_idx],\n",
    "            \"cx\":    CSE_ID,\n",
    "            \"q\":     query,\n",
    "            \"start\": start,\n",
    "            \"num\":   min(page_size, num_results - len(urls)),\n",
    "        }\n",
    "\n",
    "        # try each key up to nkeys times\n",
    "        for attempt in range(nkeys):\n",
    "            resp = requests.get(GOOGLE_API_URL, params=params, timeout=5)\n",
    "\n",
    "            # rate-limited? rotate key and retry\n",
    "            if resp.status_code == 429:\n",
    "                retry_after = int(resp.headers.get(\"Retry-After\", 0))\n",
    "                wait = retry_after if retry_after > 0 else 2 ** attempt\n",
    "                print(f\"[{name!r}] key#{key_idx} 429 → sleeping {wait}s…\")\n",
    "                time.sleep(wait)\n",
    "                key_idx = (key_idx + 1) % nkeys\n",
    "                params[\"key\"] = API_KEYS[key_idx]\n",
    "                continue\n",
    "\n",
    "            # other errors raise\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            # success!\n",
    "            data = resp.json()\n",
    "            for item in data.get(\"items\", []):\n",
    "                urls.append(item[\"link\"])\n",
    "                print(f\"[{name!r}] key#{key_idx} → {item['link']}\")\n",
    "                if len(urls) >= num_results:\n",
    "                    break\n",
    "            break  # out of retry-loop\n",
    "\n",
    "        if len(urls) >= num_results:\n",
    "            break  # out of paging-loop\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_candidate_urls(name: str) -> set[str]:\n",
    "    \"\"\"\n",
    "    For each software name, fetch candidate URLs in this order:\n",
    "      1. GitHub\n",
    "      2. PyPI\n",
    "      3. CRAN\n",
    "      4. General Google search (excluding above domains)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # GitHub\n",
    "    try:\n",
    "        results += fetch_github_urls(name)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] GitHub fetch failed for '{name}': {e}\")\n",
    "\n",
    "    # PyPI\n",
    "    try:\n",
    "        results += fetch_pypi_urls(name)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] PyPI fetch failed for '{name}': {e}\")\n",
    "\n",
    "    # CRAN\n",
    "    try:\n",
    "        results += fetch_cran_urls(name)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] CRAN check failed for '{name}': {e}\")\n",
    "\n",
    "    # Google\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        results += fetch_google_urls(name)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Google search failed for '{name}': {e}\")\n",
    "\n",
    "    # dedupe, preserve order\n",
    "    return set(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'names = [\"TensOrflow\",\\'tidyr\\',\\'reQuests\\']\\nfor name in names:\\n    print(f\"Candidate URLs for \\'{name}\\':\")\\n    urls = fetch_candidate_urls(name)\\n    for url in urls:\\n        print(f\"  - {url}\")\\n    print()'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"names = [\"TensOrflow\",'tidyr','reQuests']\n",
    "for name in names:\n",
    "    print(f\"Candidate URLs for '{name}':\")\n",
    "    urls = fetch_candidate_urls(name)\n",
    "    for url in urls:\n",
    "        print(f\"  - {url}\")\n",
    "    print()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_candidates(path: str) -> Dict[str, Set[str]]:\n",
    "    \"\"\"Load a JSON cache of {name: [urls…]}, return {name: set(urls)…}.\"\"\"\n",
    "    if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"⚠️ Warning: corrupt JSON cache; starting fresh.\")\n",
    "                data = {}\n",
    "    else:\n",
    "        data = {}\n",
    "\n",
    "    # convert lists→sets\n",
    "    return {name: set(urls) for name, urls in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_candidates(candidates: Dict[str, Set[str]], path: str):\n",
    "    \"\"\"Convert sets→lists and write out a pretty JSON file.\"\"\"\n",
    "    serializable = {name: sorted(list(urls)) for name, urls in candidates.items()}\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(serializable, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_candidate_cache(\n",
    "    corpus: pd.DataFrame,\n",
    "    fetcher,                # your fetch_candidate_urls(name) function\n",
    "    cache_path: str\n",
    ") -> Dict[str, Set[str]]:\n",
    "    # 1) load existing\n",
    "    candidates = load_candidates(cache_path)\n",
    "\n",
    "    # 2) iterate unique names\n",
    "    for name in corpus['name'].unique():\n",
    "        # initialize if needed\n",
    "        if name not in candidates:\n",
    "            candidates[name] = set()\n",
    "\n",
    "        # 3) add any pre-existing URLs from your dataframe\n",
    "        urls_cell = corpus.loc[corpus['name'] == name, 'candidate_urls'].dropna().astype(str)\n",
    "        for cell in urls_cell:\n",
    "            for u in cell.split(','):\n",
    "                u = u.strip()\n",
    "                if u:\n",
    "                    candidates[name].add(u)\n",
    "\n",
    "        # 4) fetch & add new ones\n",
    "        new = set(fetcher(name))\n",
    "        # only do the network hit if there’s something new to add\n",
    "        if not new.issubset(candidates[name]):\n",
    "            candidates[name].update(new)\n",
    "\n",
    "    # 5) persist back to JSON\n",
    "    save_candidates(candidates, cache_path)\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, urlunparse\n",
    "from typing import Dict, Iterable, List\n",
    "\n",
    "def normalize_url(u: str) -> str:\n",
    "    p = urlparse(u)\n",
    "    scheme = \"https\"\n",
    "    netloc = p.netloc.lower()\n",
    "    path = p.path.rstrip(\"/\")\n",
    "    # drop params, query, fragment\n",
    "    return urlunparse((scheme, netloc, path, \"\", \"\", \"\"))\n",
    "\n",
    "def dedupe_candidates(candidates: Dict[str, Iterable[str]]) -> None:\n",
    "    \"\"\"\n",
    "    For each key in `candidates`, normalize its URLs and drop duplicates,\n",
    "    preferring the https version when http & https both appear.\n",
    "    Modifies `candidates` in place, replacing each value with a List[str].\n",
    "    \"\"\"\n",
    "    for key, urls in candidates.items():\n",
    "        seen: Dict[str, str] = {}\n",
    "        for u in urls:\n",
    "            norm = normalize_url(u)\n",
    "            if norm not in seen:\n",
    "                # first time we see this normalized URL,\n",
    "                # store the original\n",
    "                seen[norm] = u\n",
    "            else:\n",
    "                # if we already have an http version, but now see an https one, upgrade it\n",
    "                if u.startswith(\"https\") and not seen[norm].startswith(\"https\"):\n",
    "                    seen[norm] = u\n",
    "        # replace with de-duplicated list\n",
    "        candidates[key] = list(seen.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'names = [\"TensOrflow\",\\'tidyr\\',\\'reQuests\\']\\nfor name in names:\\n    print(f\"Candidate URLs for \\'{name}\\':\")\\n    urls = fetch_candidate_urls(name)\\n    for url in urls:\\n        print(f\"  - {url}\")\\n    print()'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"names = [\"TensOrflow\",'tidyr','reQuests']\n",
    "for name in names:\n",
    "    print(f\"Candidate URLs for '{name}':\")\n",
    "    urls = fetch_candidate_urls(name)\n",
    "    for url in urls:\n",
    "        print(f\"  - {url}\")\n",
    "    print()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus = pd.read_excel(\"../corpus_v2.xlsx\")\\ncache_file = \"../candidate_urls.json\"\\n\\ncandidates = update_candidate_cache(corpus, fetch_candidate_urls, cache_file)\\nprint(f\"Cached URLs for {len(candidates)} names.\")'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"corpus = pd.read_excel(\"../corpus_v2.xlsx\")\n",
    "cache_file = \"../candidate_urls.json\"\n",
    "\n",
    "candidates = update_candidate_cache(corpus, fetch_candidate_urls, cache_file)\n",
    "print(f\"Cached URLs for {len(candidates)} names.\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 {'https://scanpy.readthedocs.io/en/stable/generated/scanpy.external.pp.mnn_correct.html', 'https://github.com/granatumx/gbox-mnnpy', 'https://github.com/chriscainx/mnnpy', 'https://www.amazon.es/Anatomy-Muscles-Pictures-Education-20X27inch/dp/B0888LZ94B', 'https://github.com/aysalama/mnnpython', 'https://pypi.org/project/mnnpy/', 'https://cran.r-project.org/package=mnonr', 'https://cran.r-project.org/package=knnp', 'https://cran.r-project.org/package=nn2poly', 'https://scanpy.readthedocs.io/en/1.9.x/generated/scanpy.external.pp.mnn_correct.html', 'https://anaconda.org/bioconda/mnnpy', 'https://cran.r-project.org/package=nplyr', 'http://github.com/chriscainx/mnnpy', 'https://www.amazon.se/-/en/Anatomy-Muscles-Pictures-Education-16x20inch/dp/B0888N3YVH', 'https://cran.r-project.org/package=mpoly'}\n",
      "14 ['https://scanpy.readthedocs.io/en/stable/generated/scanpy.external.pp.mnn_correct.html', 'https://github.com/granatumx/gbox-mnnpy', 'https://github.com/chriscainx/mnnpy', 'https://www.amazon.es/Anatomy-Muscles-Pictures-Education-20X27inch/dp/B0888LZ94B', 'https://github.com/aysalama/mnnpython', 'https://pypi.org/project/mnnpy/', 'https://cran.r-project.org/package=mnonr', 'https://cran.r-project.org/package=knnp', 'https://cran.r-project.org/package=nn2poly', 'https://scanpy.readthedocs.io/en/1.9.x/generated/scanpy.external.pp.mnn_correct.html', 'https://anaconda.org/bioconda/mnnpy', 'https://cran.r-project.org/package=nplyr', 'https://www.amazon.se/-/en/Anatomy-Muscles-Pictures-Education-16x20inch/dp/B0888N3YVH', 'https://cran.r-project.org/package=mpoly']\n"
     ]
    }
   ],
   "source": [
    "corpus     = pd.read_excel(\"../corpus_v2.xlsx\")\n",
    "candidates = load_candidates(\"../candidate_urls.json\")\n",
    "\n",
    "\n",
    "dedupe_candidates(candidates)\n",
    "\n",
    "# now http://chriscainx/mnnpy is merged into the https:// one\n",
    "save_candidates(candidates, \"../candidate_urls.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = load_candidates(\"../candidate_urls.json\")\n",
    "corpus = pd.read_excel(\"../corpus_v2.xlsx\")\n",
    "corpus['candidate_urls'] = corpus['name'].map(candidates).astype(str)\n",
    "corpus['candidate_urls'] = corpus['candidate_urls'].str.replace(\"{\", \"\").str.replace(\"}\", \"\").str.replace(\"[\", \"\").str.replace(\"]\", \"\").str.replace(\"'\", \"\").str.replace('\"', '').str.replace(\",\", \",\").str.replace(\" \", \"\") # remove unwanted characters\n",
    "corpus['candidate_urls'] = corpus['candidate_urls'].str.replace(\"'\", \"\").str.replace('\"', '').str.replace(\",\", \",\").str.replace(\" \", \"\") # remove unwanted characters\n",
    "corpus.to_excel(\"../corpus_v2.xlsx\", index=False) # save the updated corpus with candidate URLs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
