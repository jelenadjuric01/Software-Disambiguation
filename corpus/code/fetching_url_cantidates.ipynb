{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching URL candidate from GitHub, PyPI, CRAN and then finally from Google, excluding results from the previous three.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import List, Dict, Set\n",
    "import os\n",
    "import pandas as pd\n",
    "import difflib\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googlesearch-python in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from googlesearch-python) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install googlesearch-python beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import xmlrpc.client\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "GITHUB_API_URL = \"https://api.github.com/search/repositories\"\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"Please set the GITHUB_TOKEN environment variable.\")\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "    \"Accept\":        \"application/vnd.github.v3+json\",\n",
    "    \"User-Agent\":    \"my-software-disambiguator\"  # any non-empty string\n",
    "}\n",
    "\n",
    "def fetch_github_urls(\n",
    "    name: str,\n",
    "    per_page: int = 5,\n",
    "    max_retries: int = 3\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Return up to `per_page` GitHub repo URLs matching `name`, handling rate limits.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"q\":        f\"{name} in:name\",\n",
    "        \"sort\":     \"stars\",\n",
    "        \"order\":    \"desc\",\n",
    "        \"per_page\": per_page\n",
    "    }\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        resp = requests.get(GITHUB_API_URL, params=params, headers=HEADERS, timeout=10)\n",
    "        # 403 could be a rate-limit on the Search API\n",
    "        if resp.status_code == 403:\n",
    "            reset_ts = int(resp.headers.get(\"X-RateLimit-Reset\", time.time() + 60))\n",
    "            wait = max(reset_ts - time.time(), 1)\n",
    "            print(f\"[Attempt {attempt}] Rate limited. Sleeping {int(wait)}s until reset…\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        # a 401 means bad token, 404 would be weird, anything else we raise\n",
    "        resp.raise_for_status()\n",
    "        items = resp.json().get(\"items\", [])\n",
    "        return [item[\"html_url\"] for item in items]\n",
    "\n",
    "    # If we exhausted retries\n",
    "    raise RuntimeError(f\"GitHub search for '{name}' failed after {max_retries} attempts (last status: {resp.status_code})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYPI_JSON_URL    = \"https://pypi.org/pypi/{pkg}/json\"\n",
    "PYPI_PROJECT_URL = \"https://pypi.org/project/{pkg}/\"\n",
    "\n",
    "@lru_cache(maxsize=512)\n",
    "def _get_pypi_info(pkg: str, timeout: float = 10.0) -> Dict:\n",
    "    \"\"\"\n",
    "    Fetches the JSON info block for `pkg`, or returns {} on error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        r = requests.get(PYPI_JSON_URL.format(pkg=pkg), timeout=timeout)\n",
    "        if r.status_code == 200:\n",
    "            return r.json().get(\"info\", {})\n",
    "    except requests.RequestException:\n",
    "        pass\n",
    "    return {}\n",
    "\n",
    "@lru_cache(maxsize=256)\n",
    "def fetch_pypi_urls(\n",
    "    pkg_name: str,\n",
    "    max_results: int = 5,\n",
    "    timeout: float = 10.0\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    1) Exact lookup via JSON API → returns info['package_url'] (or info['project_url'])\n",
    "    2) Fuzzy lookup via XML‐RPC + JSON API per hit\n",
    "    \"\"\"\n",
    "    urls: List[str] = []\n",
    "\n",
    "    # 1) Exact match\n",
    "    info = _get_pypi_info(pkg_name, timeout)\n",
    "    if info:\n",
    "        url = info.get(\"package_url\") or info.get(\"project_url\")\n",
    "        if url:\n",
    "            urls.append(url)\n",
    "\n",
    "    if len(urls) >= max_results:\n",
    "        return urls[:max_results]\n",
    "\n",
    "    # 2) Fuzzy search\n",
    "    try:\n",
    "        client = xmlrpc.client.ServerProxy(\"https://pypi.org/pypi\")\n",
    "        hits = client.search({\"name\": pkg_name}, \"or\")\n",
    "        seen = set(pkg_name.lower())\n",
    "\n",
    "        for hit in hits:\n",
    "            name = hit.get(\"name\")\n",
    "            key  = name.lower() if name else None\n",
    "            if not key or key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "\n",
    "            # pull its JSON info to get the true URL\n",
    "            info = _get_pypi_info(name, timeout)\n",
    "            if info:\n",
    "                url = info.get(\"package_url\") or info.get(\"project_url\")\n",
    "                if url:\n",
    "                    urls.append(url)\n",
    "                    if len(urls) >= max_results:\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "            # fallback (should rarely be needed)\n",
    "            urls.append(PYPI_PROJECT_URL.format(pkg=name))\n",
    "            if len(urls) >= max_results:\n",
    "                break\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return urls[:max_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAN_PACKAGES_URL = \"https://cran.r-project.org/src/contrib/PACKAGES\"\n",
    "CRAN_BASE_URL     = \"https://cran.r-project.org/web/packages/{pkg}/index.html\"\n",
    "CRAN_SHORT_URL    = \"https://cran.r-project.org/package={pkg}\"\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _load_cran_packages(timeout: float = 10.0) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch and parse the CRAN PACKAGES index into a list of package names.\n",
    "    Cached in memory so we only download it once.\n",
    "    \"\"\"\n",
    "    resp = requests.get(CRAN_PACKAGES_URL, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    pkgs = []\n",
    "    for line in resp.text.splitlines():\n",
    "        if line.startswith(\"Package:\"):\n",
    "            pkgs.append(line.split(\":\", 1)[1].strip())\n",
    "    return pkgs\n",
    "\n",
    "@lru_cache(maxsize=256)\n",
    "def fetch_cran_urls(\n",
    "    name: str,\n",
    "    max_results: int = 5,\n",
    "    timeout: float = 10.0\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Return up to `max_results` canonical CRAN URLs for packages matching `name`:\n",
    "      1) exact match\n",
    "      2) substring match\n",
    "      3) fuzzy match via difflib\n",
    "    \"\"\"\n",
    "    pkgs = _load_cran_packages(timeout)\n",
    "    urls: List[str] = []\n",
    "    name_lower = name.lower()\n",
    "\n",
    "    # 1) Exact\n",
    "    if name in pkgs:\n",
    "        urls.append(CRAN_SHORT_URL.format(pkg=name))\n",
    "\n",
    "    # 2) Substring\n",
    "    if len(urls) < max_results:\n",
    "        subs = [p for p in pkgs if name_lower in p.lower() and p != name]\n",
    "        for p in subs:\n",
    "            if len(urls) >= max_results:\n",
    "                break\n",
    "            urls.append(CRAN_SHORT_URL.format(pkg=p))\n",
    "\n",
    "    # 3) Fuzzy\n",
    "    if len(urls) < max_results:\n",
    "        # cutoff=0.6 is a sensible default; tweak as needed\n",
    "        fuzzy = difflib.get_close_matches(name, pkgs, n=max_results, cutoff=0.6)\n",
    "        for p in fuzzy:\n",
    "            if len(urls) >= max_results:\n",
    "                break\n",
    "            if p not in [u.split(\"/\")[-2] for u in urls]:\n",
    "                urls.append(CRAN_SHORT_URL.format(pkg=p))\n",
    "\n",
    "    return urls[:max_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_URL = \"https://www.googleapis.com/customsearch/v1\"\n",
    "\n",
    "API_KEYS = [\n",
    "    os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    os.getenv(\"GOOGLE_API_KEY1\"),\n",
    "    os.getenv(\"GOOGLE_API_KEY2\"),\n",
    "    os.getenv(\"GOOGLE_API_KEY3\")\n",
    "]\n",
    "\n",
    "API_KEYS = [k for k in API_KEYS if k]\n",
    "\n",
    "CSE_ID = os.environ[\"GOOGLE_CSE_ID\"]\n",
    "EXCLUDE_SITES = [\n",
    "    \"github.com\",\n",
    "    \"pypi.org\",\n",
    "    \"cran.r-project.org\",\n",
    "    \"youtube.com\",\n",
    "    \"youtu.be\",\n",
    "    \"medium.com\",\n",
    "    \"stackoverflow.com\",\n",
    "    \"reddit.com\",\n",
    "    \"twitter.com\",\n",
    "    \"facebook.com\",\n",
    "    \"linkedin.com\",\n",
    "    \"geeksforgeeks.org\",\n",
    "    \"w3schools.com\",\n",
    "    \"tutorialspoint.com\"\n",
    "]\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def fetch_google_urls(name: str, num_results: int = 5) -> List[str]:\n",
    "    exclude_query = \" \".join(f\"-site:{d}\" for d in EXCLUDE_SITES)\n",
    "    query = f\"{name} {exclude_query}\"\n",
    "    key_idx = 0\n",
    "    urls = []\n",
    "    page_size = 10\n",
    "    nkeys = len(API_KEYS)\n",
    "    # loop over pages of results\n",
    "    for start in range(1, num_results + 1, page_size):\n",
    "        params = {\n",
    "            \"key\":   API_KEYS[key_idx],\n",
    "            \"cx\":    CSE_ID,\n",
    "            \"q\":     query,\n",
    "            \"start\": start,\n",
    "            \"num\":   min(page_size, num_results - len(urls)),\n",
    "        }\n",
    "\n",
    "        # try each key up to nkeys times\n",
    "        for attempt in range(nkeys):\n",
    "            resp = requests.get(GOOGLE_API_URL, params=params, timeout=5)\n",
    "\n",
    "            # rate-limited? rotate key and retry\n",
    "            if resp.status_code == 429:\n",
    "                retry_after = int(resp.headers.get(\"Retry-After\", 0))\n",
    "                wait = retry_after if retry_after > 0 else 2 ** attempt\n",
    "                print(f\"[{name!r}] key#{key_idx} 429 → sleeping {wait}s…\")\n",
    "                time.sleep(wait)\n",
    "                key_idx = (key_idx + 1) % nkeys\n",
    "                params[\"key\"] = API_KEYS[key_idx]\n",
    "                continue\n",
    "\n",
    "            # other errors raise\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            # success!\n",
    "            data = resp.json()\n",
    "            for item in data.get(\"items\", []):\n",
    "                urls.append(item[\"link\"])\n",
    "                print(f\"[{name!r}] key#{key_idx} → {item['link']}\")\n",
    "                if len(urls) >= num_results:\n",
    "                    break\n",
    "            break  # out of retry-loop\n",
    "\n",
    "        if len(urls) >= num_results:\n",
    "            break  # out of paging-loop\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_candidate_urls(name: str) -> set[str]:\n",
    "    \"\"\"\n",
    "    For each software name, fetch candidate URLs in this order:\n",
    "      1. GitHub\n",
    "      2. PyPI\n",
    "      3. CRAN\n",
    "      4. General Google search (excluding above domains)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # GitHub\n",
    "    try:\n",
    "        results += fetch_github_urls(name)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] GitHub fetch failed for '{name}': {e}\")\n",
    "\n",
    "    # PyPI\n",
    "    try:\n",
    "        results += fetch_pypi_urls(name)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] PyPI fetch failed for '{name}': {e}\")\n",
    "\n",
    "    # CRAN\n",
    "    try:\n",
    "        results += fetch_cran_urls(name)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] CRAN check failed for '{name}': {e}\")\n",
    "\n",
    "    # Google\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        results += fetch_google_urls(name)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Google search failed for '{name}': {e}\")\n",
    "\n",
    "    # dedupe, preserve order\n",
    "    return set(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'names = [\"TensOrflow\",\\'tidyr\\',\\'reQuests\\']\\nfor name in names:\\n    print(f\"Candidate URLs for \\'{name}\\':\")\\n    urls = fetch_candidate_urls(name)\\n    for url in urls:\\n        print(f\"  - {url}\")\\n    print()'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"names = [\"TensOrflow\",'tidyr','reQuests']\n",
    "for name in names:\n",
    "    print(f\"Candidate URLs for '{name}':\")\n",
    "    urls = fetch_candidate_urls(name)\n",
    "    for url in urls:\n",
    "        print(f\"  - {url}\")\n",
    "    print()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_candidates(path: str) -> Dict[str, Set[str]]:\n",
    "    \"\"\"Load a JSON cache of {name: [urls…]}, return {name: set(urls)…}.\"\"\"\n",
    "    if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"⚠️ Warning: corrupt JSON cache; starting fresh.\")\n",
    "                data = {}\n",
    "    else:\n",
    "        data = {}\n",
    "\n",
    "    # convert lists→sets\n",
    "    return {name: set(urls) for name, urls in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_candidates(candidates: Dict[str, Set[str]], path: str):\n",
    "    \"\"\"Convert sets→lists and write out a pretty JSON file.\"\"\"\n",
    "    serializable = {name: sorted(list(urls)) for name, urls in candidates.items()}\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(serializable, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_candidate_cache(\n",
    "    corpus: pd.DataFrame,\n",
    "    fetcher,                # your fetch_candidate_urls(name) function\n",
    "    cache_path: str\n",
    ") -> Dict[str, Set[str]]:\n",
    "    # 1) load existing\n",
    "    candidates = load_candidates(cache_path)\n",
    "\n",
    "    # 2) iterate unique names\n",
    "    for name in corpus['name'].unique():\n",
    "        # initialize if needed\n",
    "        if name not in candidates:\n",
    "            candidates[name] = set()\n",
    "\n",
    "        # 3) add any pre-existing URLs from your dataframe\n",
    "        urls_cell = corpus.loc[corpus['name'] == name, 'candidate_urls'].dropna().astype(str)\n",
    "        for cell in urls_cell:\n",
    "            for u in cell.split(','):\n",
    "                u = u.strip()\n",
    "                if u:\n",
    "                    candidates[name].add(u)\n",
    "\n",
    "        # 4) fetch & add new ones\n",
    "        new = set(fetcher(name))\n",
    "        # only do the network hit if there’s something new to add\n",
    "        if not new.issubset(candidates[name]):\n",
    "            candidates[name].update(new)\n",
    "\n",
    "    # 5) persist back to JSON\n",
    "    save_candidates(candidates, cache_path)\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'names = [\"TensOrflow\",\\'tidyr\\',\\'reQuests\\']\\nfor name in names:\\n    print(f\"Candidate URLs for \\'{name}\\':\")\\n    urls = fetch_candidate_urls(name)\\n    for url in urls:\\n        print(f\"  - {url}\")\\n    print()'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"names = [\"TensOrflow\",'tidyr','reQuests']\n",
    "for name in names:\n",
    "    print(f\"Candidate URLs for '{name}':\")\n",
    "    urls = fetch_candidate_urls(name)\n",
    "    for url in urls:\n",
    "        print(f\"  - {url}\")\n",
    "    print()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus = pd.read_excel(\"../corpus_v2.xlsx\")\\ncache_file = \"../candidate_urls.json\"\\n\\ncandidates = update_candidate_cache(corpus, fetch_candidate_urls, cache_file)\\nprint(f\"Cached URLs for {len(candidates)} names.\")'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"corpus = pd.read_excel(\"../corpus_v2.xlsx\")\n",
    "cache_file = \"../candidate_urls.json\"\n",
    "\n",
    "candidates = update_candidate_cache(corpus, fetch_candidate_urls, cache_file)\n",
    "print(f\"Cached URLs for {len(candidates)} names.\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate URLs for 'mnnpy':\n",
      "['mnnpy'] key#0 429 → sleeping 1s…\n",
      "{'https://cran.r-project.org/package=mpoly', 'https://github.com/aysalama/mnnpython', 'https://cran.r-project.org/package=knnp', 'https://github.com/chriscainx/mnnpy', 'https://github.com/granatumx/gbox-mnnpy', 'https://cran.r-project.org/package=mnonr', 'https://pypi.org/project/mnnpy/', 'https://cran.r-project.org/package=nplyr', 'https://cran.r-project.org/package=nn2poly'}\n",
      "\n",
      "Candidate URLs for 'pDraw':\n",
      "['pDraw'] key#0 429 → sleeping 1s…\n",
      "{'https://cran.r-project.org/package=paws', 'https://cran.r-project.org/package=raw', 'https://github.com/contriteobserver/AnimatedWebPDrawable', 'https://github.com/ShowMeThe/AnimatedWebPDrawable', 'https://cran.r-project.org/package=draw', 'https://cran.r-project.org/package=trawl', 'https://cran.r-project.org/package=pgdraw', 'https://github.com/Akaaba/pdraw', 'https://github.com/Phylliida/P2PDraw', 'https://github.com/Parrot-Developers/pdraw'}\n",
      "\n",
      "Candidate URLs for 'pymagnitude':\n",
      "['pymagnitude'] key#0 429 → sleeping 1s…\n",
      "{'https://github.com/shr3yans/pymagnitude-tutorial', 'https://github.com/zfang/benchmark_pymagnitude', 'https://github.com/kamujun/compare_pymagnitude_with_gensim_and_tfhub', 'https://github.com/Sankar16kds/pymagnitude', 'https://github.com/lambwang/pymagnitude', 'https://pypi.org/project/pymagnitude/', 'https://cran.r-project.org/package=pargasite'}\n",
      "\n",
      "Candidate URLs for 'pvr':\n",
      "['pvr'] key#0 429 → sleeping 1s…\n",
      "{'https://github.com/mrossini-ethz/camera-calibration-pvr', 'https://cran.r-project.org/package=gwrpvr', 'https://github.com/kodi-pvr/pvr.iptvsimple', 'https://github.com/pvrbook/pvr', 'https://cran.r-project.org/package=ypr', 'https://cran.r-project.org/package=pvar', 'https://github.com/fxgames/pvrtextool_wrapper', 'https://cran.r-project.org/package=PVR', 'https://cran.r-project.org/package=pcvr', 'https://github.com/kodi-pvr/pvr.hts'}\n",
      "\n",
      "Candidate URLs for 'pyAFQ':\n",
      "['pyAFQ'] key#0 429 → sleeping 1s…\n",
      "{'https://github.com/Anneef/pyAFQ', 'https://github.com/yeatmanlab/pyAFQ', 'https://pypi.org/project/pyAFQ/', 'https://github.com/brainlife/app-pyafq-segment', 'https://github.com/tractometry/pyAFQ', 'https://github.com/scitran-apps/pyafq'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidates = load_candidates(\"../candidate_urls.json\")\n",
    "names = [\"mnnpy\",\"pDraw\",\"pymagnitude\",\"pvr\",\"pyAFQ\"]\n",
    "for name in names:\n",
    "    print(f\"Candidate URLs for '{name}':\")\n",
    "    candidates[name] = fetch_candidate_urls(name)\n",
    "    print(candidates[name])\n",
    "    print()\n",
    "save_candidates(candidates, \"../candidate_urls.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
