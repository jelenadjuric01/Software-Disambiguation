{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching URL candidate from GitHub, PyPI, CRAN and then finally from Google, excluding results from the previous three.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmlrpc.client\n",
    "from time import sleep\n",
    "from typing import List, Dict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googlesearch-python in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from googlesearch-python) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jelena\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install googlesearch-python beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "GITHUB_API_URL = \"https://api.github.com/search/repositories\"\n",
    "CRAN_BASE_URL = \"https://cran.r-project.org/web/packages/{name}/index.html\"\n",
    "PYPI_JSON_API = \"https://pypi.org/pypi/{name}/json\"\n",
    "PYPI_PROJECT_URL = \"https://pypi.org/project/{name}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"Please set the GITHUB_TOKEN environment variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_github_urls(name: str, per_page: int = 5) -> List[str]:\n",
    "    \"\"\"Return up to `per_page` GitHub repo URLs matching `name`, using token if provided.\"\"\"\n",
    "    params = {\"q\": name, \"sort\": \"stars\", \"order\": \"desc\", \"per_page\": per_page}\n",
    "    headers = {}\n",
    "    if GITHUB_TOKEN:\n",
    "        headers[\"Authorization\"] = f\"token {GITHUB_TOKEN}\"\n",
    "\n",
    "    resp = requests.get(GITHUB_API_URL, params=params, headers=headers, timeout=10)\n",
    "    if resp.status_code == 401:\n",
    "        raise RuntimeError(\"GitHub API Unauthorized (401). Check your GITHUB_TOKEN environment variable.\")\n",
    "    resp.raise_for_status()\n",
    "    items = resp.json().get(\"items\", [])\n",
    "    return [item[\"html_url\"] for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pypi_urls(name: str, max_results: int = 5) -> List[str]:\n",
    "    \"\"\"Return up to `max_results` PyPI project URLs matching `name`, combining exact lookup and fuzzy search.\"\"\"\n",
    "    urls: List[str] = []\n",
    "    # 1) Exact lookup via JSON API\n",
    "    api_url = PYPI_JSON_API.format(name=name)\n",
    "    try:\n",
    "        resp = requests.get(api_url, timeout=10)\n",
    "        if resp.status_code == 200:\n",
    "            urls.append(PYPI_PROJECT_URL.format(name=name))\n",
    "    except requests.RequestException:\n",
    "        pass\n",
    "\n",
    "    # 2) Fuzzy search via Google for site-specific PyPI project pages\n",
    "    query = f\"site:pypi.org/project {name}\"\n",
    "    try:\n",
    "        results = list(search(query, num_results=max_results, sleep_interval=2.0))\n",
    "        for url in results:\n",
    "            if url.startswith(\"https://pypi.org/project/\"):\n",
    "                urls.append(url)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Deduplicate and return\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for u in urls:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            deduped.append(u)\n",
    "    return deduped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_cran_urls(name: str, max_results: int = 5) -> List[str]:\n",
    "    \"\"\"Return up to `max_results` CRAN package URLs matching `name`, combining exact lookup and fuzzy search.\"\"\"\n",
    "    urls: List[str] = []\n",
    "    # 1) Exact lookup via GET\n",
    "    exact_url = CRAN_BASE_URL.format(name=name)\n",
    "    try:\n",
    "        resp = requests.get(exact_url, allow_redirects=True, timeout=10)\n",
    "        if resp.status_code == 200:\n",
    "            urls.append(exact_url)\n",
    "    except requests.RequestException:\n",
    "        pass\n",
    "\n",
    "    # 2) Fuzzy search via Google for site-specific CRAN package pages\n",
    "    query = f\"site:cran.r-project.org/web/packages {name}\"\n",
    "    try:\n",
    "        results = list(search(query, num_results=max_results, sleep_interval=2.0))\n",
    "        for url in results:\n",
    "            if url.startswith(\"https://cran.r-project.org/web/packages/\"):\n",
    "                urls.append(url)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Deduplicate and return\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for u in urls:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            deduped.append(u)\n",
    "    return deduped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domains to exclude in general web search\n",
    "EXCLUDE_SITES = [\n",
    "    \"github.com\",\n",
    "    \"pypi.org\",\n",
    "    \"cran.r-project.org\",\n",
    "    \"youtube.com\",\n",
    "    \"youtu.be\",\n",
    "    \"medium.com\",\n",
    "    \"stackoverflow.com\",\n",
    "    \"reddit.com\",\n",
    "    \"twitter.com\",\n",
    "    \"facebook.com\",\n",
    "    \"linkedin.com\",\n",
    "    \"geeksforgeeks.org\",\n",
    "    \"w3schools.com\",\n",
    "    \"tutorialspoint.com\"\n",
    "]\n",
    "def fetch_google_urls(name: str, num_results: int = 5, sleep_interval: float = 2.0) -> List[str]:\n",
    "    \"\"\"\n",
    "    Use googlesearch to find other URLs for `name`, excluding known non-software domains.\n",
    "\n",
    "    `EXCLUDE_SITES` contains domains to omit (e.g., YouTube, social media, Q&A sites).\n",
    "    \"\"\"\n",
    "    # Build exclude portion of query\n",
    "    exclude_queries = \" \".join(f\"-site:{domain}\" for domain in EXCLUDE_SITES)\n",
    "    query = f\"{name} {exclude_queries}\"\n",
    "    # `num_results` and `sleep_interval` are supported parameters\n",
    "    return list(search(query, num_results=num_results, sleep_interval=sleep_interval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_candidate_urls(software_names: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    For each software name, fetch candidate URLs in this order:\n",
    "      1. GitHub\n",
    "      2. PyPI\n",
    "      3. CRAN\n",
    "      4. General Google search (excluding above domains)\n",
    "    \"\"\"\n",
    "    all_results: Dict[str, List[str]] = {}\n",
    "    for name in software_names:\n",
    "        results = []\n",
    "\n",
    "        # GitHub\n",
    "        try:\n",
    "            results += fetch_github_urls(name)\n",
    "        except Exception as e:\n",
    "            print(f\"[!] GitHub fetch failed for '{name}': {e}\")\n",
    "\n",
    "        # PyPI\n",
    "        try:\n",
    "            results += fetch_pypi_urls(name)\n",
    "        except Exception as e:\n",
    "            print(f\"[!] PyPI fetch failed for '{name}': {e}\")\n",
    "\n",
    "        # CRAN\n",
    "        try:\n",
    "            results += fetch_cran_urls(name)\n",
    "        except Exception as e:\n",
    "            print(f\"[!] CRAN check failed for '{name}': {e}\")\n",
    "\n",
    "        # Google\n",
    "        try:\n",
    "            sleep(1)\n",
    "            results += fetch_google_urls(name)\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Google search failed for '{name}': {e}\")\n",
    "\n",
    "        # dedupe, preserve order\n",
    "        seen = set()\n",
    "        deduped = []\n",
    "        for url in results:\n",
    "            if url not in seen:\n",
    "                seen.add(url)\n",
    "                deduped.append(url)\n",
    "        all_results[name] = deduped\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensorflow:\n",
      "  - https://github.com/tensorflow/tensorflow\n",
      "  - https://github.com/huggingface/transformers\n",
      "  - https://github.com/tensorflow/models\n",
      "  - https://github.com/fighting41love/funNLP\n",
      "  - https://github.com/keras-team/keras\n",
      "  - https://pypi.org/project/tensorflow/\n",
      "  - https://pypi.org/project/tensorflow-tpu/\n",
      "  - https://pypi.org/project/tensorflow/2.4.1/\n",
      "  - https://pypi.org/project/types-tensorflow/\n",
      "  - https://pypi.org/project/tensorflow/2.5.0/\n",
      "  - https://cran.r-project.org/web/packages/tensorflow/index.html\n",
      "  - https://cran.r-project.org/web/packages/tfhub/vignettes/hub-with-keras.html\n",
      "  - https://cran.r-project.org/web/packages/tfestimators/vignettes/tensorflow_layers.html\n",
      "  - https://cran.r-project.org/web/packages/tfdeploy/vignettes/introduction.html\n",
      "  - https://cran.r-project.org/web/packages/tensorflow/tensorflow.pdf\n",
      "  - https://cran.r-project.org/web/packages/tfaddons/readme/README.html\n",
      "  - https://www.tensorflow.org/\n",
      "  - https://en.wikipedia.org/wiki/TensorFlow\n",
      "  - https://developers.google.com/learn/pathways/tensorflow\n",
      "  - https://ai.google.dev/edge/litert\n",
      "  - https://developer.apple.com/metal/tensorflow-plugin/\n",
      "\n",
      "dplyr:\n",
      "  - https://github.com/tidyverse/dplyr\n",
      "  - https://github.com/machow/siuba\n",
      "  - https://github.com/sparklyr/sparklyr\n",
      "  - https://github.com/kieferk/dfply\n",
      "  - https://github.com/business-science/tidyquant\n",
      "  - https://pypi.org/project/dfply/\n",
      "  - https://pypi.org/project/dppd/\n",
      "  - https://pypi.org/project/pandas-ply/\n",
      "  - https://pypi.org/project/plydata/\n",
      "  - https://pypi.org/project/tabeline/0.3.0/\n",
      "  - https://cran.r-project.org/web/packages/dplyr/index.html\n",
      "  - https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html\n",
      "  - https://cran.r-project.org/web/packages/dplyr/dplyr.pdf\n",
      "  - https://cran.r-project.org/web/packages/dplyr/vignettes/base.html\n",
      "  - https://cran.r-project.org/web/packages/dplyr/vignettes/programming.html\n",
      "  - https://cran.r-project.org/web/packages/dplyr/readme/README.html\n",
      "  - https://dplyr.tidyverse.org/\n",
      "  - https://www.google.com/search?num=7\n",
      "  - https://en.wikipedia.org/wiki/Dplyr\n",
      "  - https://datacarpentry.github.io/R-genomics/04-dplyr\n",
      "  - https://www.kaggle.com/code/jessemostipak/dive-into-dplyr-tutorial-1\n",
      "\n",
      "requests:\n",
      "  - https://github.com/git/git\n",
      "  - https://github.com/psf/requests\n",
      "  - https://github.com/request/request\n",
      "  - https://github.com/guzzle/guzzle\n",
      "  - https://github.com/yeongpin/cursor-free-vip\n",
      "  - https://pypi.org/project/requests/\n",
      "  - https://pypi.org/project/requests/2.7.0/\n",
      "  - https://pypi.org/project/requests/2.11.1/\n",
      "  - https://pypi.org/project/requests/2.3.0/\n",
      "  - https://pypi.org/project/requests/0.6.0/\n",
      "  - https://cran.r-project.org/web/packages/crul/vignettes/choosing-a-client.html\n",
      "  - https://cran.r-project.org/web/packages/request/readme/README.html\n",
      "  - https://cran.r-project.org/web/packages/reqres/reqres.pdf\n",
      "  - https://cran.r-project.org/web/packages/rsconnect/vignettes/custom-http.html\n",
      "  - https://cran.r-project.org/web/packages/crul/vignettes/best-practices-api-packages.html\n",
      "  - https://requests.readthedocs.io/\n",
      "  - https://realpython.com/python-requests/\n",
      "  - https://en.wikipedia.org/wiki/Requests_(software)\n",
      "  - https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Methods\n",
      "  - https://laravel.com/docs/12.x/requests\n"
     ]
    }
   ],
   "source": [
    "software_list = [\"tensorflow\", \"dplyr\", \"requests\"]\n",
    "candidates = fetch_candidate_urls(software_list)\n",
    "for name, urls in candidates.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for u in urls:\n",
    "        print(\"  -\", u)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
