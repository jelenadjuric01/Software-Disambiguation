ID;software_mention;package_url;homepage_url;github_repo;exact_match;doi;paragraph
SM102065;pytorch;https://pypi.org/project/pytorch;;;FALSE;10.3390/s21093265;As for Tan [11], Kokkinos [13], Cui [14], and our proposed method, all of them are implemented by the CNN approach, and thus we accelerated them using pytorch on CUDA.
SM102065;pytorch;https://pypi.org/project/pytorch;;;FALSE;10.3390/s19030667;The initialization of other layers was done in accordance with the method officially recommended in version 0.4 of pytorch
SM102065;pytorch;https://pypi.org/project/pytorch;;;FALSE;10.3389/frobt.2019.00022;Part of the appeal of using DNNs is the fact that many off-the-shelf frameworks for deep learning (Tensorflow, MXNet, pytorch) all support automatic differentiation
SM102065;pytorch;https://pypi.org/project/pytorch;;;FALSE;10.1038/s41598-019-52737-x;Training was performed on the NIH Biowulf cluster using 2-GPU nodes (2xNVIDIA K80, for a total of 4 logical GPUs with 12 GB each) with a batch size of 4 (pytorch nn.DataParallel)
SM102065;pytorch;https://pypi.org/project/pytorch;;;FALSE;10.3390/s21093106;Based on the pytorch deep learning frame, the image datasets were trained using the SSD algorithm to generate identification models
SM102065;pytorch;https://pypi.org/project/pytorch;;;FALSE;10.3390/s21010229;We implemented MDPO on pytorch DNN framework, which is an actively developed open-source deep learning library
SM102065;pytorch;https://pypi.org/project/pytorch;;;FALSE;10.3389/fncom.2021.626259;For comparison, we used the pre-trained ResNet-18 from pytorch 1.2.0 as the normal DCNN, referred to as the ResNet in this study for brevity.
SM102065;pytorch;https://pypi.org/project/pytorch;;;FALSE;10.3390/s20216264;Our experiments were performed on a computer with Intel Core I7 and NVIDA GeForce GTX 1080Ti, and we operated our algorithm by c++ and pytorch.
SM102065;pytorch;https://pypi.org/project/pytorch;;;FALSE;10.1186/s12864-018-5283-8;We have trained our models running on a single NVidia Quadro P6000 with stochastic gradient descent with momentum in pytorch
SM102065;pytorch;https://pypi.org/project/pytorch;;;FALSE;10.15252/msb.202010179;Inference was performed in pytorch (preprint: Paszke et al, 2019) using the Adam optimizer (preprint: Kingma & Ba, 2017)
SM102065;pytorch;https://pypi.org/project/torch/,https://pytorch.org/,https://github.com/pytorch/pytorch;;;TRUE;10.3390/s21093265;As for Tan [11], Kokkinos [13], Cui [14], and our proposed method, all of them are implemented by the CNN approach, and thus we accelerated them using pytorch on CUDA.
SM102065;pytorch;https://pypi.org/project/torch/,https://pytorch.org/,https://github.com/pytorch/pytorch;;;TRUE;10.3390/s19030667;The initialization of other layers was done in accordance with the method officially recommended in version 0.4 of pytorch
SM102065;pytorch;https://pypi.org/project/torch/,https://pytorch.org/,https://github.com/pytorch/pytorch;;;TRUE;10.3389/frobt.2019.00022;Part of the appeal of using DNNs is the fact that many off-the-shelf frameworks for deep learning (Tensorflow, MXNet, pytorch) all support automatic differentiation
SM102065;pytorch;https://pypi.org/project/torch/,https://pytorch.org/,https://github.com/pytorch/pytorch;;;TRUE;10.1038/s41598-019-52737-x;Training was performed on the NIH Biowulf cluster using 2-GPU nodes (2xNVIDIA K80, for a total of 4 logical GPUs with 12 GB each) with a batch size of 4 (pytorch nn.DataParallel)
SM102065;pytorch;https://pypi.org/project/torch/,https://pytorch.org/,https://github.com/pytorch/pytorch;;;TRUE;10.3390/s21093106;Based on the pytorch deep learning frame, the image datasets were trained using the SSD algorithm to generate identification models
SM102065;pytorch;https://pypi.org/project/torch/,https://pytorch.org/,https://github.com/pytorch/pytorch;;;TRUE;10.3390/s21010229;We implemented MDPO on pytorch DNN framework, which is an actively developed open-source deep learning library
SM102065;pytorch;https://pypi.org/project/torch/,https://pytorch.org/,https://github.com/pytorch/pytorch;;;TRUE;10.3389/fncom.2021.626259;For comparison, we used the pre-trained ResNet-18 from pytorch 1.2.0 as the normal DCNN, referred to as the ResNet in this study for brevity.
SM102065;pytorch;https://pypi.org/project/torch/,https://pytorch.org/,https://github.com/pytorch/pytorch;;;TRUE;10.3390/s20216264;Our experiments were performed on a computer with Intel Core I7 and NVIDA GeForce GTX 1080Ti, and we operated our algorithm by c++ and pytorch.
SM102065;pytorch;https://pypi.org/project/torch/,https://pytorch.org/,https://github.com/pytorch/pytorch;;;TRUE;10.1186/s12864-018-5283-8;We have trained our models running on a single NVidia Quadro P6000 with stochastic gradient descent with momentum in pytorch
SM102065;pytorch;https://pypi.org/project/torch/,https://pytorch.org/,https://github.com/pytorch/pytorch;;;TRUE;10.15252/msb.202010179;Inference was performed in pytorch (preprint: Paszke et al, 2019) using the Adam optimizer (preprint: Kingma & Ba, 2017)
SM107490;requests;https://pypi.org/project/requests,https://requests.readthedocs.io/en/latest/,https://github.com/psf/requests;;;TRUE;10.1186/s12859-018-2367-z;Other requirements: At least Python 3.4, the packages numpy, requests and msgpack must be installed
SM107490;requests;https://pypi.org/project/requests,https://requests.readthedocs.io/en/latest/,https://github.com/psf/requests;;;TRUE;10.1186/1471-2105-15-44;The HTS barcode checker is written in python and uses the biopython[10], beautiful-soup[11] and requests[12] packages to handle FASTA sequences and communicate with the various APIs and web services used, such as NCBI GenBank and the PhyloTastic TNRS service
SM107490;requests;https://pypi.org/project/requests,https://requests.readthedocs.io/en/latest/,https://github.com/psf/requests;;;TRUE;10.1186/s12911-020-1046-y;Project name: Prikbord Project home page:http://prikbord.science.ru.nl/Operating system: Linux Programming language: Python, javascript Other requirements: Django 1.5.11 or higher, MongoDB 2.6.10, pymongo 2.7.2 or higher, requests 2.13.0 or higher License: GNU GPL Any restrictions to use by non-academics: licence needed
SM107490;requests;https://pypi.org/project/requests,https://requests.readthedocs.io/en/latest/,https://github.com/psf/requests;;;TRUE;10.7717/peerj.4395;Other requirements include GoogleAppEnginePipeline 1.9.22.1, pyqi 0.3.1, requests 2.10.0, requests-toolbelt 0.6.2, mailjet-rest 1.2.2, biom-format 1.1.2, ete3 3.0.0 (for tree generation–see below for details), webapp2 2.5.2, numpy 1.6.1, matplotlib 1.2.0, jinja2 2.6, ssl 2.7.11
SM107490;requests;https://pypi.org/project/requests,https://requests.readthedocs.io/en/latest/,https://github.com/psf/requests;;;TRUE;10.1016/j.dib.2021.107360;We used the json and requests packages [35,36]to collect data and the scikit-learn package [37] to impute missing values
SM107490;requests;https://pypi.org/project/requests,https://requests.readthedocs.io/en/latest/,https://github.com/psf/requests;;;TRUE;10.3389/fvets.2021.674730;Using requests and BeautifulSoup packages in Python, all the PDF files with the titles containing “MRK” by avoiding cases sensitivity of uppercase or lowercase for each letter (e.g., “mRK,” “MrK,” “mrk,” etc.) from the years 2018, 2019, and 2020 were automatically collected and saved in a separate folder for further steps
SM107490;requests;https://pypi.org/project/requests,https://requests.readthedocs.io/en/latest/,https://github.com/psf/requests;;;TRUE;10.3390/ijerph17155596;For data collection, Python Beautiful Soap and requests libraries have been used to automatically collect weather information from the Italian website ”IlMeteo.it” (https://www.ilmeteo.it/portale/archivio-meteo/) and to store the data in a single CSV file
SM3075;scikit-learn;https://pypi.org/project/scikit-learn,https://github.com/scikit-learn/scikit-learn,https://scikit-learn.org/stable/;;;TRUE;10.1186/s12885-019-5646-9;We used the python module scikit-learn to perform all the above modelling process using default parameters
SM3075;scikit-learn;https://pypi.org/project/scikit-learn,https://github.com/scikit-learn/scikit-learn,https://scikit-learn.org/stable/;;;TRUE;10.1186/s13059-021-02480-2;To build the machine learning models, we use the scikit-learn (version: 0.23.2) package
SM3075;scikit-learn;https://pypi.org/project/scikit-learn,https://github.com/scikit-learn/scikit-learn,https://scikit-learn.org/stable/;;;TRUE;10.1109/TBME.2018.2890167;Note that the SVM-based classifier we utilized is based on an open-source tool called scikit-learn (http://scikit-learn.org), with default settings.
SM3075;scikit-learn;https://pypi.org/project/scikit-learn,https://github.com/scikit-learn/scikit-learn,https://scikit-learn.org/stable/;;;TRUE;10.3389/fgene.2021.648329;The optimal number of clusters was determined based on two metrics: Silhouette index (Rousseeuw, 1987) and Calinski–Harabasz index (Calinski and Harabasz, 1974), using scikit-learn package (Pedregosa et al., 2011).
SM3075;scikit-learn;https://pypi.org/project/scikit-learn,https://github.com/scikit-learn/scikit-learn,https://scikit-learn.org/stable/;;;TRUE;10.3390/jpm11080748;All machine learning algorithms were implemented using scikit-learn package, version 0.24.1 in Python 3.8.5
SM3075;scikit-learn;https://pypi.org/project/scikit-learn,https://github.com/scikit-learn/scikit-learn,https://scikit-learn.org/stable/;;;TRUE;10.1186/s12920-017-0264-3;We used Python implementation of random forest from scikit-learn version 0.14.1
SM3075;scikit-learn;https://pypi.org/project/scikit-learn,https://github.com/scikit-learn/scikit-learn,https://scikit-learn.org/stable/;;;TRUE;10.7554/eLife.61277;We used scikit-learn (Pedregosa, 2011) PCA (sklearn.decomposition.PCA) to identify the dominant axes of gene expression variation across the entire AHBA dataset, as well as for brain-specific genes
SM3075;scikit-learn;https://pypi.org/project/scikit-learn,https://github.com/scikit-learn/scikit-learn,https://scikit-learn.org/stable/;;;TRUE;10.1186/s13148-021-01029-1;Box plots of the cohorts by BCI values and the receiver operating characteristic (ROC) curve for the BCI classification (Fig. 2) were generated in python 3.7.2 using numpy 1.16.2 and pandas 0.24.2 packages for data processing, custom code to generate the true-positive rates and false-positive rates for the ROC curve sliding threshold, scikit-learn 0.20.3 to calculate the ROC area under the curve (AUC), and seaborn 0.9.0 with matplotlib 3.0.2 for visualization.
SM3075;scikit-learn;https://pypi.org/project/scikit-learn,https://github.com/scikit-learn/scikit-learn,https://scikit-learn.org/stable/;;;TRUE;10.1371/journal.pone.0236962;Using the python scikit-learn library v0.20.2, we trained Random Forest [28] and Logistic Regression [29] machine learning algorithms on the VarTrain missense variants, which includes 10,070 deleterious variants as the positive set and 10,070 benign variants as the negative set
SM3075;scikit-learn;https://pypi.org/project/scikit-learn,https://github.com/scikit-learn/scikit-learn,https://scikit-learn.org/stable/;;;TRUE;10.3389/fgene.2019.01292;Text-mining and SVM computing were processed using the python scikit-learn library (Pedregosa et al., 2011)
SM3077;matplotlib;https://pypi.org/project/matplotlib,https://github.com/matplotlib/matplotlib,https://matplotlib.org/;;;TRUE;10.1038/s41598-019-43005-z;Gromacs’s trajectory analysis tools, MDTraj along with in-house bash and python scripts were used for data analysis and matplotlib for plotting
SM3077;matplotlib;https://pypi.org/project/matplotlib,https://github.com/matplotlib/matplotlib,https://matplotlib.org/;;;TRUE;10.3390/antibiotics9110834;Heatmaps illustrating AMR patterns in calf isolates from each herd were generated in Python (www.python.org) using the matplotlib, pandas and seaborn packages
SM3077;matplotlib;https://pypi.org/project/matplotlib,https://github.com/matplotlib/matplotlib,https://matplotlib.org/;;;TRUE;10.1038/s41598-020-80308-y;Image and data pre- and post-processing was carried out using the Python libraries scikit-image, scikit-learn, OpenCV, numpy, scipy, matplotlib, seaborn, statsmodels and pandas
SM3077;matplotlib;https://pypi.org/project/matplotlib,https://github.com/matplotlib/matplotlib,https://matplotlib.org/;;;TRUE;10.1016/j.dcn.2019.100630;Brain plots were created using the following Python packages: matplotlib (Hunter, 2007), nibabel (Brett et al., 2016), and nilearn (Abraham et al., 2014)
SM3077;matplotlib;https://pypi.org/project/matplotlib,https://github.com/matplotlib/matplotlib,https://matplotlib.org/;;;TRUE;10.1038/s41598-018-22083-5;All statistical learning, linear modelling and corresponding figures were performed in Python packages scikit-learn, xgboost, statsmodels, matplotlib, seaborn, and pandas.
SM3077;matplotlib;https://pypi.org/project/matplotlib,https://github.com/matplotlib/matplotlib,https://matplotlib.org/;;;TRUE;10.7717/peerj.8783;The 10 most important features were graphed for each environment based on relative importance utilizing pandas (McKinney, 2010) and matplotlib (Hunter, 2007)
SM3077;matplotlib;https://pypi.org/project/matplotlib,https://github.com/matplotlib/matplotlib,https://matplotlib.org/;;;TRUE;10.7717/peerj.1029;GraPhlAn uses the matplotlib library (Hunter, 2007)
SM3077;matplotlib;https://pypi.org/project/matplotlib,https://github.com/matplotlib/matplotlib,https://matplotlib.org/;;;TRUE;10.1038/sdata.2018.280;The notebooks import the following modules: pandas (version 0.22.0), numpy (version 1.14.0), matplotlib (version 2.1.2), and csv (version 1.0).
SM3077;matplotlib;https://pypi.org/project/matplotlib,https://github.com/matplotlib/matplotlib,https://matplotlib.org/;;;TRUE;10.1038/s41398-018-0334-0;We used the publicly available Python eSig package (version 0.6.31) to calculate signatures of streams of data, Python pandas package (version 0.20.1) for statistical analysis, data manipulations and processing, Python scikit-learn package (version 0.18.1) for machine learning tasks and matplotlib for plotting and graphics (version 2.0.1).
SM3077;matplotlib;https://pypi.org/project/matplotlib,https://github.com/matplotlib/matplotlib,https://matplotlib.org/;;;TRUE;10.1038/s41598-017-16050-9;Subsequent analyses were performed using Python in Jupyter Notebook, using the numpy, sklearn, pandas, and SciPy packages, as well as matplotlib and Seaborn visualization libraries
SM3411;numpy;https://pypi.org/project/numpy,https://numpy.org/,https://github.com/numpy/numpy;;;TRUE;10.1098/rstb.2013.0067;2.7.3 (http://www.python.org) with its numpy, scipy and pylab extension modules
SM3411;numpy;https://pypi.org/project/numpy,https://numpy.org/,https://github.com/numpy/numpy;;;TRUE;10.1371/journal.pcbi.1007113;All statistical analyses and data visualization were performed using Jupyter Notebook [64] and Python 2.7, and the following packages: scipy.stats, numpy [65], pandas [66], matplotlib [67], mpl_toolkits, matplotlib_venn, seaborn, statsmodels.stats.multitest.fdrcorrection, mygene [68,69], sklearn.decomposition.PCA [59], sklearn.preprocessing.StandardScaler.
SM3411;numpy;https://pypi.org/project/numpy,https://numpy.org/,https://github.com/numpy/numpy;;;TRUE;10.1038/s41598-017-13828-9;To use the code, one requires a Python 3 interpreter, as well as the numpy, scipy and pandas Python modules
SM3411;numpy;https://pypi.org/project/numpy,https://numpy.org/,https://github.com/numpy/numpy;;;TRUE;10.1371/journal.pone.0206193;All scripts were coded in Matlab and Python using statistics libraries (numpy, pylab, scilab and matplotlib)
SM3411;numpy;https://pypi.org/project/numpy,https://numpy.org/,https://github.com/numpy/numpy;;;TRUE;10.7717/peerj.4925;AMPtk is written in Python and relies on several modules: edlib (Šošic & Šikic, 2017), biopython (Cock et al., 2009), biom-format (McDonald et al., 2012), pandas (McKinney, 2010), numpy (van der Walt, Colbert & Varoquaux, 2011), and matplotlib modules (Hunter, 2007)
SM3411;numpy;https://pypi.org/project/numpy,https://numpy.org/,https://github.com/numpy/numpy;;;TRUE;10.1038/s41598-019-44272-6;We simulated one single admixture event from Southern Ural/West Siberian to both Indo-European and Hungarian Sekler populations occurring 30 generations ago, with same unknown amount of admixture [using numpy.random.random_sample] and then randomly drifted for 30 generations
SM3411;numpy;https://pypi.org/project/numpy,https://numpy.org/,https://github.com/numpy/numpy;;;TRUE;10.1371/journal.pgen.1009774;Bioinformatic analyses were done using Python 3.6 and the following packages: numpy 1.18.1, pandas 1.0.3 and matplotlib 3.1.3
SM3411;numpy;https://pypi.org/project/numpy,https://numpy.org/,https://github.com/numpy/numpy;;;TRUE;10.1186/s12864-019-6436-0;0.22.0), numpy (v
SM3411;numpy;https://pypi.org/project/numpy,https://numpy.org/,https://github.com/numpy/numpy;;;TRUE;10.1128/mBio.03221-19;The pipeline was written in Python3 with packages pandas, numpy, Biopython, pybedtools, pyBigWig, and pysam
SM3411;numpy;https://pypi.org/project/numpy,https://numpy.org/,https://github.com/numpy/numpy;;;TRUE;10.1371/journal.pone.0218966;Our algorithm is presented in Algorithm 1, which was developed in Python 3.6.1 with the packages pandas 0.20.3 and numpy 1.13.1.
SM3412;pandas;https://pypi.org/project/pandas,https://pandas.pydata.org/,https://github.com/pandas-dev/pandas;;;TRUE;10.1186/1476-072X-13-11;Statistics were computed using the open-source Python statsmodels and pandas packages.
SM3412;pandas;https://pypi.org/project/pandas,https://pandas.pydata.org/,https://github.com/pandas-dev/pandas;;;TRUE;10.1371/journal.pone.0230416;The processing pipeline, including DAS classification, as well as the descriptive part below were all developed in Python [32], mainly relying on the following libraries or tools: scipy [33], scikit-learn [34], pandas [35], numpy [36], nltk [37], matplotlib [38], seaborn [39], gensim [40], beautifulsoup (https://www.crummy.com/software/BeautifulSoup), TextBlob (https://github.com/sloria/textblob) and pymongo (MongoDB, https://www.mongodb.com).
SM3412;pandas;https://pypi.org/project/pandas,https://pandas.pydata.org/,https://github.com/pandas-dev/pandas;;;TRUE;10.3390/ma14164507;First, all data is converted to the form of DataFrame from pandas library [52]
SM3412;pandas;https://pypi.org/project/pandas,https://pandas.pydata.org/,https://github.com/pandas-dev/pandas;;;TRUE;10.1186/s12911-019-0805-0;All analyses were performed using Python version 2.7 and relevant open-source libraries: scikit-learn, scipy, pandas and numpy.
SM3412;pandas;https://pypi.org/project/pandas,https://pandas.pydata.org/,https://github.com/pandas-dev/pandas;;;TRUE;10.1186/s12864-019-5924-6;I used this binary table to calculate the Hamming distance [11] between all strains in Python using pandas and the scipy spatial packages.
SM3412;pandas;https://pypi.org/project/pandas,https://pandas.pydata.org/,https://github.com/pandas-dev/pandas;;;TRUE;10.1038/s41598-019-44839-3;Data processing was performed using Python, and third party libraries such as NumPy, pandas, scikit-image and scikit-learn.
SM3412;pandas;https://pypi.org/project/pandas,https://pandas.pydata.org/,https://github.com/pandas-dev/pandas;;;TRUE;10.1186/s12859-015-0608-y;○ external dependences: python 2.7, numpy (> = 1.7), pandas
SM3412;pandas;https://pypi.org/project/pandas,https://pandas.pydata.org/,https://github.com/pandas-dev/pandas;;;TRUE;10.2196/13962;Dropout and step count data were preprocessed using Python (mainly the pandas and matplotlib packages) to generate graphs and format the data into a suitable format for R
SM3412;pandas;https://pypi.org/project/pandas,https://pandas.pydata.org/,https://github.com/pandas-dev/pandas;;;TRUE;10.3390/cancers13061315;Data preprocessing was done in python (version 3.8.3) using the numpy (version 1.18.5) and pandas (version 1.0.5) packages
SM3412;pandas;https://pypi.org/project/pandas,https://pandas.pydata.org/,https://github.com/pandas-dev/pandas;;;TRUE;10.1038/s41598-020-72174-5;Additional packages used are pandas 0.23.4 for handling and analysis of data and PyBioMed-1.0 package for accessing the AAindex data
SM3415;tensorflow;https://pypi.org/project/tensorflow,https://www.tensorflow.org/,https://github.com/tensorflow/tensorflow;;;TRUE;10.12688/f1000research.51117.1;"Most commonly, there was a description of machine-learning toolkits (Mallet, N = 12; Weka, N = 6; tensorflow, N = 5; scikit-learn, N = 3)"
SM3415;tensorflow;https://pypi.org/project/tensorflow,https://www.tensorflow.org/,https://github.com/tensorflow/tensorflow;;;TRUE;10.1001/jamanetworkopen.2021.6096;Pydicom version 1.4.2, tensorflow, and opencv version 4.1.0 were used for image processing.
SM3415;tensorflow;https://pypi.org/project/tensorflow,https://www.tensorflow.org/,https://github.com/tensorflow/tensorflow;;;TRUE;10.1021/acsomega.1c00991;Machine learning procedures were used/implemented with sklearn, tensorflow, and (for the GNN) pytorch-geometric, while the fingerprints were generated with Dscribe(54) and our own implementation of the PDDF
SM3415;tensorflow;https://pypi.org/project/tensorflow,https://www.tensorflow.org/,https://github.com/tensorflow/tensorflow;;;TRUE;10.1002/mp.14415;For all training and inference tasks, tensorflow and a DGX workstation with a 16‐GB NVIDIA V100 graphics processing unit was used.
SM3415;tensorflow;https://pypi.org/project/tensorflow,https://www.tensorflow.org/,https://github.com/tensorflow/tensorflow;;;TRUE;10.1186/s40537-020-00387-6;With tensorflow version 1.14.0 and python version 3.7.4
SM3415;tensorflow;https://pypi.org/project/tensorflow,https://www.tensorflow.org/,https://github.com/tensorflow/tensorflow;;;TRUE;10.1002/advs.202004958;The Keras version 2.3, a highly useful neural networks API, and the tensorflow‐gpu 1.15 version were adopted for a rapid parallel computing.
SM3415;tensorflow;https://pypi.org/project/tensorflow,https://www.tensorflow.org/,https://github.com/tensorflow/tensorflow;;;TRUE;10.1371/journal.pcbi.1008918;(iii) The packages keras and tensorflow were applied to develop deep learning-based regression model.
SM3415;tensorflow;https://pypi.org/project/tensorflow,https://www.tensorflow.org/,https://github.com/tensorflow/tensorflow;;;TRUE;10.3390/s21051734;As regards the choice of the DL framework, there are numerous open-source frameworks [98,99], such as keras [100], tensorflow [101], and pytorch [102]
SM3415;tensorflow;https://pypi.org/project/tensorflow,https://www.tensorflow.org/,https://github.com/tensorflow/tensorflow;;;TRUE;10.1186/s12911-019-0991-9;Data were analysed in R version 3.4.4 using the packages GLMnet, e1071, randomforest, pROC, ROCR, ggplot, and the neural network was run in python 2.7.12 with tensorflow 1.10.1 (Additional file 3)
SM3415;tensorflow;https://cran.r-project.org/package=tensorflow,https://github.com/rstudio/tensorflow,https://tensorflow.rstudio.com/;;;TRUE;10.3390/ijms21186652;R (v.4.0.1) with RStudio (v.1.3.959, RStudio, Boston, MA, USA) and the R-packages Keras (v.2.3.0.0), generics (v.0.0.2) reticulate (v.1.16-9000), tfruns (v.1.4), magrittr (v.1.5), zeallot (v.0.1.0), R6 (v.2.4.1), tensorflow (v.2.2.0), config (v.0.3), jsonlite (v.1.6.1), processx (v.3.4.2), yaml (v.2.2.1), rstudioapi (v.0.11), caret (v.6.0-86), and e1071 (v.1.7-3)
SM3415;tensorflow;https://pypi.org/project/tensorflow,https://www.tensorflow.org/,https://github.com/tensorflow/tensorflow;;;FALSE;10.3390/ijms21186652;R (v.4.0.1) with RStudio (v.1.3.959, RStudio, Boston, MA, USA) and the R-packages Keras (v.2.3.0.0), generics (v.0.0.2) reticulate (v.1.16-9000), tfruns (v.1.4), magrittr (v.1.5), zeallot (v.0.1.0), R6 (v.2.4.1), tensorflow (v.2.2.0), config (v.0.3), jsonlite (v.1.6.1), processx (v.3.4.2), yaml (v.2.2.1), rstudioapi (v.0.11), caret (v.6.0-86), and e1071 (v.1.7-3)
SM975261;beautifulsoup4;https://pypi.org/project/beautifulsoup4,https://www.crummy.com/software/BeautifulSoup/,https://code.launchpad.net/beautifulsoup,https://github.com/wention/BeautifulSoup4;;;TRUE;10.3390/s20010190;Our API for the acquisition of weather data operates by scraping the web page of the automatic Brazilian weather stations [5] using the libraries requests [67] and beautifulsoup4 [68], as well the frameworks Django [69] and Django rest [70]
SM10267;Django;https://pypi.org/project/Django,https://www.djangoproject.com/,https://github.com/django/django;;;TRUE;10.1186/s12911-019-0750-y;The main tools and the development environments that we utilized in developing VIADS include Django, Python, JavaScript, Vis.js, Graph.js, JQuery, Plotly, Chart.js, Unittest, R, and MySQL
SM10267;Django;https://pypi.org/project/Django,https://www.djangoproject.com/,https://github.com/django/django;;;TRUE;10.1093/database/bax073;Django is a free and open-source framework, written in Python, which encourages rapid software development and clean, pragmatic design, facilitating the tasks of creating complex, database-driven Web applications
SM10267;Django;https://pypi.org/project/Django,https://www.djangoproject.com/,https://github.com/django/django;;;TRUE;10.3390/metabo5030431;The Django Python package makes building web requests that integrated diverse data stores straightforward [19] The API defined in Django allows access to both of these resources.
SM10267;Django;https://pypi.org/project/Django,https://www.djangoproject.com/,https://github.com/django/django;;;TRUE;10.3389/fimmu.2019.02820;This web tool was built using the Django web framework (24) and makes use of several Python libraries including scikit-learn (23), SciPy (25), Pyteomics (26), Altair (27), NumPy (28), Pandas (29), and Dask (30)
SM10267;Django;https://pypi.org/project/Django,https://www.djangoproject.com/,https://github.com/django/django;;;TRUE;10.1016/j.heliyon.2019.e02622;The auto-assessment tool is built on Windows Server 2016 and contains Django Web framework for user interface, PTC Creo Parametric 3.0 M110 to open and run CAD models, Python (version 3.5.4) wrapper for Creo's Visual Basic API for commanding Creo, and Celery for queue tasks
SM10267;Django;https://pypi.org/project/Django,https://www.djangoproject.com/,https://github.com/django/django;;;TRUE;10.3390/s20102962;However, the cloud platform is being re-developed from scratch to create a highly scalable cloud, using Angular [43] for web development, and Django [44] as the web framework, with OAuth 2.0 authentication [45]
SM10267;Django;https://pypi.org/project/Django,https://www.djangoproject.com/,https://github.com/django/django;;;TRUE;10.1186/s12859-018-2082-9;DecoFungi is implemented in Python using several open-source libraries: Django (as the Web application framework), OpenCV (library for image processing and computer vision), the Keras framework with a Tensorflow back-end (provides the deep learning techniques), and the scikit-learn library (library for machine learning).
SM10267;Django;https://pypi.org/project/Django,https://www.djangoproject.com/,https://github.com/django/django;;;TRUE;10.4269/ajtmh.2011.10-0528;The RapidSMS™ platform is based on the Django web framework and written in Python programming language
SM10267;Django;https://pypi.org/project/Django,https://www.djangoproject.com/,https://github.com/django/django;;;TRUE;10.1186/1471-2164-15-371;The web interface is powered by a Python application built on Django (an open source web framework), HTML/CSS and Javascript
SM10267;Django;https://pypi.org/project/Django,https://www.djangoproject.com/,https://github.com/django/django;;;TRUE;10.3390/vaccines8040709;The ML models with the best performance were implemented on a web server using the Python and Django framework
SM48699;BeautifulSoup;https://pypi.org/project/beautifulsoup4,https://www.crummy.com/software/BeautifulSoup/,https://code.launchpad.net/beautifulsoup,https://github.com/wention/BeautifulSoup4;;;TRUE;10.2196/10986;To extract the content of a Web page from the HTML source we tested: BeautifulSoup, Naive [68], which just naively removes HTML tags and Boilerpipe, Boi [78] and Justext, Jst [79], which eliminates boilerplate text together with HTML tags
SM48699;BeautifulSoup;https://pypi.org/project/beautifulsoup4,https://www.crummy.com/software/BeautifulSoup/,https://code.launchpad.net/beautifulsoup,https://github.com/wention/BeautifulSoup5;;;TRUE;10.1038/s41597-020-00682-0;Wikipedia is accessed through the MediaWiki API using Python, and the BeautifulSoup, json, and requests packages
SM48699;BeautifulSoup;https://pypi.org/project/beautifulsoup4,https://www.crummy.com/software/BeautifulSoup/,https://code.launchpad.net/beautifulsoup,https://github.com/wention/BeautifulSoup6;;;TRUE;10.1371/journal.pone.0018657;Data collection scripts were coded in Python version 2.5.2 (many libraries were used, including EUtils, BeautifulSoup, pyparsing and nltk [47]) and SQLite version 3.4
SM48699;BeautifulSoup;https://pypi.org/project/beautifulsoup4,https://www.crummy.com/software/BeautifulSoup/,https://code.launchpad.net/beautifulsoup,https://github.com/wention/BeautifulSoup7;;;TRUE;10.1007/s12061-021-09386-3;Our web-scraping tool based on Python and BeautifulSoup retrieved the following information: name, telephone, address, postcode, latitude, longitude, opening and closing times for each day of the week, and a list of services including Finance services
SM48699;BeautifulSoup;https://pypi.org/project/beautifulsoup4,https://www.crummy.com/software/BeautifulSoup/,https://code.launchpad.net/beautifulsoup,https://github.com/wention/BeautifulSoup8;;;TRUE;10.1186/s12859-020-3540-8;For the prior knowledge from UniProt KB, we use Bioservices, urllib, BeautifulSoup tool does finish a series of processes
SM48699;BeautifulSoup;https://pypi.org/project/beautifulsoup4,https://www.crummy.com/software/BeautifulSoup/,https://code.launchpad.net/beautifulsoup,https://github.com/wention/BeautifulSoup9;;;TRUE;10.3390/ani8020025;Data were extracted using the BeautifulSoup module in Python Version 3.2.1 (Python Software Foundation, Wilmington, DE, USA) to parse the ads, then cleaned and analyzed using R (The R Foundation, Vienna, Austria) and Pivot tables in Microsoft Excel® 2013 (Microsoft, Redmond, WA, USA)
SM48699;BeautifulSoup;https://pypi.org/project/beautifulsoup4,https://www.crummy.com/software/BeautifulSoup/,https://code.launchpad.net/beautifulsoup,https://github.com/wention/BeautifulSoup10;;;TRUE;10.3389/fvets.2021.674730;"The HTML and XML document parser, BeautifulSoup, from Python module BS4 was implemented to parse the source page; multimodal texts of Web page sources were extracted as BeatifulSoup object"
SM48699;BeautifulSoup;https://pypi.org/project/beautifulsoup4,https://www.crummy.com/software/BeautifulSoup/,https://code.launchpad.net/beautifulsoup,https://github.com/wention/BeautifulSoup11;;;TRUE;10.3390/s21165496;As for their experimental setup, the authors used: (1) the NLTK library to compute Morphological Features, (2) the Linguistic Inquiry and Word Count for Psychological Features, (3) Textstat for Readability Features, as well as (4) BeautifulSoup and Newspaper for web-markup features
SM48699;BeautifulSoup;https://pypi.org/project/beautifulsoup4,https://www.crummy.com/software/BeautifulSoup/,https://code.launchpad.net/beautifulsoup,https://github.com/wention/BeautifulSoup12;;;TRUE;10.1186/1751-0473-8-9;While we believe that the approach taken by twill and the robust parsing provided by BeautifulSoup will prevent many upstream changes breaking these wrappers, inevitably breakage will occur
SM48699;BeautifulSoup;https://pypi.org/project/beautifulsoup4,https://www.crummy.com/software/BeautifulSoup/,https://code.launchpad.net/beautifulsoup,https://github.com/wention/BeautifulSoup13;;;TRUE;10.3389/fmolb.2021.620475;BeautifulSoup is used for extracting the text between paragraphs (<p>) and tables (<table>) tags
SM8369;Flask;https://pypi.org/project/Flask,https://flask.palletsprojects.com/en/stable/,https://github.com/pallets/flask/,https://palletsprojects.com/projects/flask;;;TRUE;10.1093/database/baz075;The database was developed by using Flask framework with Python 3.4 and MySQL
SM8369;Flask;https://pypi.org/project/Flask,https://flask.palletsprojects.com/en/stable/,https://github.com/pallets/flask/,https://palletsprojects.com/projects/flask;;;TRUE;10.1038/s41598-021-87134-w;We developed a user-friendly web-server to implement the BChemRF-CPPred, which was coded using Flask, HTML, CSS, and JavaScript programming languages
SM8369;Flask;https://pypi.org/project/Flask,https://flask.palletsprojects.com/en/stable/,https://github.com/pallets/flask/,https://palletsprojects.com/projects/flask;;;TRUE;10.1093/database/baaa106;Python 3 using Flask was used as the framework programming language to develop a common interface (Figure 1).
SM8369;Flask;https://pypi.org/project/Flask,https://flask.palletsprojects.com/en/stable/,https://github.com/pallets/flask/,https://palletsprojects.com/projects/flask;;;TRUE;10.7717/peerj-cs.144;The core Flask code defines, via @-prefixed decorators, what actions should be performed when a user accesses specific URLs
SM8369;Flask;https://pypi.org/project/Flask,https://flask.palletsprojects.com/en/stable/,https://github.com/pallets/flask/,https://palletsprojects.com/projects/flask;;;TRUE;10.3390/s18020474;Flask, an MVC model, is used for better project organization and the need to bypass any middleware technologies
SM8369;Flask;https://pypi.org/project/Flask,https://flask.palletsprojects.com/en/stable/,https://github.com/pallets/flask/,https://palletsprojects.com/projects/flask;;;TRUE;10.1093/gigascience/giz060;The web application uses Flask and Nginx on a virtual machine, as well as Gunicorn as a Web Server Gateway Interface
SM8369;Flask;https://pypi.org/project/Flask,https://flask.palletsprojects.com/en/stable/,https://github.com/pallets/flask/,https://palletsprojects.com/projects/flask;;;TRUE;10.1093/bioinformatics/bty999;GBE extends the ExAC browser (Karczewski ) which is built in Python, utilizes Flask framework and uses d3 and plot.ly for plot rendering
SM8369;Flask;https://pypi.org/project/Flask,https://flask.palletsprojects.com/en/stable/,https://github.com/pallets/flask/,https://palletsprojects.com/projects/flask;;;TRUE;10.1093/database/baw052;The frontend HTML is designed using Flask (version 0.10.1), a web framework for Python (version 3.4.1)
SM8369;Flask;https://pypi.org/project/Flask,https://flask.palletsprojects.com/en/stable/,https://github.com/pallets/flask/,https://palletsprojects.com/projects/flask;;;TRUE;10.3390/genes12081117;The web server uses the Python programming language with the Flask library
SM8369;Flask;https://pypi.org/project/Flask,https://flask.palletsprojects.com/en/stable/,https://github.com/pallets/flask/,https://palletsprojects.com/projects/flask;;;TRUE;10.3390/ijerph17145243;The Python/Flask API template was then fleshed out with code logic to process API input parameters and form appropriate SQL queries to return requested exposures data.
