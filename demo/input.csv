id;name;doi;paragraph
1;(STAR;10.1038/s41598-021-86032-5;Then, the high-quality reads were aligned to the reference genome (hg19) using Spliced Transcripts Alignment to a Reference (STAR) with the parameter –quantMode GeneCounts for gene-level quantification and otherwise default parameters
2;(STAR;10.3389/fphys.2020.623190;Next, the two reads per sample (R1 and R2) were mapped to the mouse reference genome (build GRCm38/mm10) using Spliced Transcripts Alignment to a Reference (STAR version 2.6.1) (Dobin et al., 2013)
3;ABBYY;10.1109/JTEHM.2019.2935451;A 4-sample test for equality of proportions showed that the overall successful search rate differed significantly between the OCR engines (, df = 3, p < 0.001), with ML Kit and Rekognition having a significantly higher overall success rate than Azure OCR and ABBYY RTR (p < 0.001 for all multiple pairwise comparison with Bonferroni correction)
4;ABBYY;10.1109/JTEHM.2019.2935451;Four different OCR Engines that can be implemented via an API were evaluated to determine the accuracy and usability of each engine: Google Machine Learning Kit (ML Kit) [26], Microsoft Azure’s Cognitive Services (Azure OCR) [27], ABBYY’s Real Time Recognition SDK (ABBY RTR) [28], and Amazon’s Rekognition SDK (Rekognition) [29]
5;ABBYY;10.1109/JTEHM.2019.2935451;Successful search rate at zoom level differed significantly between the 4 OCR engines (, df = 3, p < 0.001), with ML Kit being the most successful in detecting keywords without needing to zoom in, followed by Rekognition, Azure OCR, and ABBYY RTR
6;ABBYY;10.1186/s13637-017-0057-1;Hand-writing recognition is a well-established problem and we have experimented with different resources including Omnipage Capture SDK [18], Captricity [19], and ABBYY [20], which to the best of our knowledge are among the best tools in the market for recognizing hand-written letters and have been widely used in recognizing and transforming documents into usable digital forms [21]
7;ABBYY;10.12688/f1000research.7329.3;We analyzed different freely available and commercial OCR systems and libraries including Aspose, PUMA, Microsoft OCR, Tesseract, LEADTOOLS, Nicomsoft OCR, MeOCR OCR, OmniPage, ABBYY, Bytescout claiming to be able to extract embedded text from figures
8;BCEA;10.1371/journal.pntd.0008521;We used the BCEA package in the software R 3.6.1 for the analysis [23].
9;BeautifulSoup;10.1007/s12061-021-09386-3;Our web-scraping tool based on Python and BeautifulSoup retrieved the following information: name, telephone, address, postcode, latitude, longitude, opening and closing times for each day of the week, and a list of services including Finance services
10;BeautifulSoup;10.1038/s41597-020-00682-0;Wikipedia is accessed through the MediaWiki API using Python, and the BeautifulSoup, json, and requests packages
11;BeautifulSoup;10.1186/1751-0473-8-9;While we believe that the approach taken by twill and the robust parsing provided by BeautifulSoup will prevent many upstream changes breaking these wrappers, inevitably breakage will occur
12;BeautifulSoup;10.1186/s12859-020-3540-8;For the prior knowledge from UniProt KB, we use Bioservices, urllib, BeautifulSoup tool does finish a series of processes
13;BeautifulSoup;10.1371/journal.pone.0018657;Data collection scripts were coded in Python version 2.5.2 (many libraries were used, including EUtils, BeautifulSoup, pyparsing and nltk [47]) and SQLite version 3.4
14;BeautifulSoup;10.2196/10986;To extract the content of a Web page from the HTML source we tested: BeautifulSoup, Naive [68], which just naively removes HTML tags and Boilerpipe, Boi [78] and Justext, Jst [79], which eliminates boilerplate text together with HTML tags
15;BeautifulSoup;10.3389/fmolb.2021.620475;BeautifulSoup is used for extracting the text between paragraphs (<p>) and tables (<table>) tags
16;BeautifulSoup;10.3389/fvets.2021.674730;"The HTML and XML document parser, BeautifulSoup, from Python module BS4 was implemented to parse the source page; multimodal texts of Web page sources were extracted as BeatifulSoup object"
17;BeautifulSoup;10.3390/ani8020025;Data were extracted using the BeautifulSoup module in Python Version 3.2.1 (Python Software Foundation, Wilmington, DE, USA) to parse the ads, then cleaned and analyzed using R (The R Foundation, Vienna, Austria) and Pivot tables in Microsoft Excel® 2013 (Microsoft, Redmond, WA, USA)
18;BeautifulSoup;10.3390/s21165496;As for their experimental setup, the authors used: (1) the NLTK library to compute Morphological Features, (2) the Linguistic Inquiry and Word Count for Psychological Features, (3) Textstat for Readability Features, as well as (4) BeautifulSoup and Newspaper for web-markup features
19;CoAPthon;10.3390/s19030716;For CoAP, CoAPthon [21], an open-source implementation, was employed to simulate communications and messages were collected from the communication channel
20;CoAPthon;10.3390/s19245467;Hence, a variety of IoT libraries, such as CoAPthon [27], and frameworks [28], such as IDeA, FRASAD, D-LITe, IoTLink, WebRTC based IoT application Framework, Datatweet, IoTSuite and RapIoT, have been developed to manage those complexities.
21;DiagrammeRsvg;10.1038/s41598-021-91205-3;"All statistical analyses were performed using R version 3.6.1 including the following packages: esc; effectsize; meta; metafor; dmetar: DiagrammeR; DiagrammeRsvg; and ggplot2"
22;Django;10.1016/j.heliyon.2019.e02622;The auto-assessment tool is built on Windows Server 2016 and contains Django Web framework for user interface, PTC Creo Parametric 3.0 M110 to open and run CAD models, Python (version 3.5.4) wrapper for Creo's Visual Basic API for commanding Creo, and Celery for queue tasks
23;Django;10.1093/database/bax073;Django is a free and open-source framework, written in Python, which encourages rapid software development and clean, pragmatic design, facilitating the tasks of creating complex, database-driven Web applications
24;Django;10.1186/1471-2164-15-371;The web interface is powered by a Python application built on Django (an open source web framework), HTML/CSS and Javascript
25;Django;10.1186/s12859-018-2082-9;DecoFungi is implemented in Python using several open-source libraries: Django (as the Web application framework), OpenCV (library for image processing and computer vision), the Keras framework with a Tensorflow back-end (provides the deep learning techniques), and the scikit-learn library (library for machine learning).
26;Django;10.1186/s12911-019-0750-y;The main tools and the development environments that we utilized in developing VIADS include Django, Python, JavaScript, Vis.js, Graph.js, JQuery, Plotly, Chart.js, Unittest, R, and MySQL
27;Django;10.3389/fimmu.2019.02820;This web tool was built using the Django web framework (24) and makes use of several Python libraries including scikit-learn (23), SciPy (25), Pyteomics (26), Altair (27), NumPy (28), Pandas (29), and Dask (30)
28;Django;10.3390/metabo5030431;The Django Python package makes building web requests that integrated diverse data stores straightforward [19] The API defined in Django allows access to both of these resources.
29;Django;10.3390/s20102962;However, the cloud platform is being re-developed from scratch to create a highly scalable cloud, using Angular [43] for web development, and Django [44] as the web framework, with OAuth 2.0 authentication [45]
30;Django;10.3390/vaccines8040709;The ML models with the best performance were implemented on a web server using the Python and Django framework
31;Django;10.4269/ajtmh.2011.10-0528;The RapidSMS™ platform is based on the Django web framework and written in Python programming language
32;FinCal;10.3390/insects12040298;To calculate the profitability criteria, the FinCal package [37] was used
33;Flask;10.1038/s41598-021-87134-w;We developed a user-friendly web-server to implement the BChemRF-CPPred, which was coded using Flask, HTML, CSS, and JavaScript programming languages
34;Flask;10.1093/bioinformatics/bty999;GBE extends the ExAC browser (Karczewski ) which is built in Python, utilizes Flask framework and uses d3 and plot.ly for plot rendering
35;Flask;10.1093/database/baaa106;Python 3 using Flask was used as the framework programming language to develop a common interface (Figure 1).
36;Flask;10.1093/database/baw052;The frontend HTML is designed using Flask (version 0.10.1), a web framework for Python (version 3.4.1)
37;Flask;10.1093/database/baz075;The database was developed by using Flask framework with Python 3.4 and MySQL
38;Flask;10.1093/gigascience/giz060;The web application uses Flask and Nginx on a virtual machine, as well as Gunicorn as a Web Server Gateway Interface
39;Flask;10.3390/genes12081117;The web server uses the Python programming language with the Flask library
40;Flask;10.3390/ijerph17145243;The Python/Flask API template was then fleshed out with code logic to process API input parameters and form appropriate SQL queries to return requested exposures data.
41;Flask;10.3390/s18020474;Flask, an MVC model, is used for better project organization and the need to bypass any middleware technologies
42;Flask;10.7717/peerj-cs.144;The core Flask code defines, via @-prefixed decorators, what actions should be performed when a user accesses specific URLs
43;GillespieSSA;10.1038/srep36301;Simulation were undertaken with the ssa() function of the GillespieSSA package14 using R software ( http://www.R-project.org/).
44;GillespieSSA;10.1186/s12859-019-2843-0;On the other hand, individual libraries targeting epidemiological modelling, such as Epipy - a visualisation data tool for epidemiology written in Python [44], or GillespieSSA - an R package for generating stochastic simulation using Gillespie’s algorithms [45], cover only specific epidemiological needs.
45;GillespieSSA;10.1371/journal.pbio.1001793;Because the simulation function depends on the R-package GillespieSSA, install this package first by executing ‘install.packages(“GillespieSSA”)’ in R
46;GillespieSSA;10.1371/journal.pone.0079345;In an extensive search for available stochastic simulators, CAIN, COPASI, Facile-EasyStoch, GillespieSSA, and StochKit2 were identified as those with the closest functionality to StochPy
47;GillespieSSA;10.3390/v7031189;To simulate our system stochastically, we used the Gillespie algorithm [59], implemented with the GillespieSSA package in R (www.r-project.org)
48;HIV.LifeTables;10.1371/journal.pone.0096447;We have released an R package, HIV.LifeTables [35], that implements the model described in this paper, and we will continue to develop and improve that package
49;HydeNet;10.1111/risa.13580;"We use the R program HydeNet and the JAGS modeling framework to build a decision network that uses the collective administrative data to generate distributions for simulating posteriors under different surveillance scenarios (Nutter, n.d.; Plummer, 2004)"
50;MG-RAST;10.1371/journal.pone.0052069;These low values contrast with the Amazon River metagenome reads classified using BLASTX against NCBI-nr in MG-RAST, 49% of 1.1 million pyrosequencing reads [46]
51;MG-RAST;10.3389/fmicb.2015.00053;"The assembled metagenomic dataset is publicly available on MG-RAST (http://metagenomics.anl.gov; project ID: 4529136.3)"
52;MG-RAST;10.3389/fmicb.2017.00308;Diversity (at class level) within metagenomic assemblies, as determined by MG-RAST ( Other represents classes with relative abundance <1% in any sample
53;MG-RAST;10.3389/fmicb.2019.02741;The Taihu 2013 metatranscriptomes are publicly available on the MG-RAST (Meyer et al., 2008) server (ID: mgm4768721, mgm4768720, mgm4768724, mgm4768726, mgm4768728, mgm47687130) and the Lake Erie 2014 diel metatranscriptomes are available from the NCBI SRA database (SRP128942, SRP128954, SRP128945, SRP117911, SRP117914, SRA117922, SRA117915)
54;MG-RAST;10.6026/97320630014031;MG-RAST server is an automated analysis platform for meta-genomes to present the quantitative understandings into microbial populations generated from sequencing data
55;MGDrivE;10.1186/s12915-020-0759-9;Applying the MGDrivE modeling framework to our research questions, we incorporate the inheritance patterns of reciprocal chromosomal translocations and UDMEL into the inheritance module of the model (Fig. 1a, b, Additional file 2: Fig
56;MGDrivE;10.1371/journal.pcbi.1009030;In this paper, we describe the key developments implemented in MGDrivE 2
57;MGDrivE;10.1371/journal.pgen.1009385;This approach is similar to that of MGDrivE [86], however our model is strictly deterministic, not species-specific, considers space differently (we assume large, 1–3 panmictic populations compared to MGDrivE’s more spatially explicit dynamics), we track haplotypes rather than just genotypes, and can adjust recombination distance between loci
58;MGDrivE;10.3390/insects11010052;Various gene drive model platforms exist, including SkeeterBuster, SLiM 2/3, and MGDrivE, although they vary in flexibility [85,86,87,88].
59;MGDrivE;10.7554/eLife.65939;"To determine the number of releases required to introgress effector genes into 95% of the female portion of a population, and ascertain how long that introgression could be effective (up to 4 years; Figure 4), we performed simulations using the full version of MGDrivE (Sanchez et al., 2019), implementing overlapping generations and density-dependent growth effects on aquatic stages"
60;MultiPhen;10.1038/srep34323;We used extensive simulation studies as well as real data application to compare the performance of AFC with TATES, Tippett, FC, MANOVA, MultiPhen, and SUMSCORE
61;MultiPhen;10.1038/srep38837;While the methods generally perform as expected under the null, there is mild inflation for min-P and TATES under high phenotypic correlations for ≤ 8 phenotypes and for MultiPhen for 48 phenotypes, and strong deflation for min-P, TATES and SHom for 48 phenotypes
62;MultiPhen;10.1093/bioinformatics/btu783;MultiPhen, canonical correlation analysis (CCA), i.e
63;MultiPhen;10.1371/journal.pone.0149206;Results are from joint testing of all four VSS variables using MultiPhen [26], with adjustment for age, sex, %TBSA burned, number of operations, and population stratification.
64;MultiPhen;10.3389/fgene.2020.00431;The statistical power in these discovery efforts can be boosted considerably by multivariate tests, which have become more practical through recent implementations that require only univariate summary statistics, such as MultiPhen (O’Reilly et al., 2012), TATES (van der Sluis et al., 2013), CONFIT (Gai and Eskin, 2018), MTAG (Turley et al., 2018), MTAR (Guo and Wu, 2019), and metaCCA (Cichonska et al., 2016)
65;NLME;10.1007/s10928-014-9353-5;For NLME, especially when used in simulations, the stochastic components of the model are crucial but the development procedure can be laborious
66;NLME;10.1007/s10928-017-9526-0;The aim of this study is to develop a new method to quantify and model pulsatile data for the development of a pharmacodynamic model that is able to quantify highly variable pulsatile secretion patterns over time by combining deconvolution analysis techniques and NLME modeling
67;NLME;10.1186/s12918-015-0203-x;Using all available experimental data, the estimated parameters from STS and NLME are roughly the same and they both describe the data well as can be seen in Fig
68;NLME;10.1371/journal.pone.0253047;The models were adjusted by the restricted maximum likelihood method using the LME function of the NLME package
69;NLME;10.3389/fphar.2020.00552;The population pharmacokinetics model of teicoplanin was developed using NLME program
70;PANDA;10.1186/1471-2105-9-182;Of these 2,464,046 sequences, 1,528,382 (62%) are marked as redundant, with 1,353,885 (55%) being marked as redundant by existing cd-hit representatives in the CAMERA clusters and the remaining being marked as redundant by other PANDA sequences
71;PANDA;10.1186/1471-2105-9-182;The PANDA sequences that are labeled as predicted proteins (i.e
72;PANDA;10.1186/s13195-018-0413-8;We carried out probabilistic fiber tractography on these seeds using the DTI analysis software PANDA (Additional file 1: Figure S2)
73;PANDA;10.14336/AD.2019.0929;"Deterministic tractography was performed using the FACT (Fiber Assignment by Continuous Tracking) algorithm in PANDA (www.nitrc.org/projects/panda; version 1.3.1) [33] and ended at voxels with FA < 0.2 or a tract turning angle of < 45˚ [34]"
74;PANDA;10.18632/aging.102023;The preprocessing of diffusion-weighted images was performed using the functional magnetic resonance imaging of the brain (FMRIB) Software Library in Version 5.0 (FSL, [65]), diffusion toolkit [66], and PANDA [67].
75;PANDA;10.18632/aging.102901;ucl.ac.uk/spm/software/spm12) running on MATLAB version 2016b, the PANDA toolbox (https://www.nitrc
76;PANDA (Pipeline for Analyzing braiN Diffusion imAges);10.3389/fncom.2021.659838;"In this study, PANDA (Pipeline for Analyzing braiN Diffusion imAges) (Cui et al., 2013), a toolkit based on MATLAB (R2009b; MathWorks) and FSL that integrates several processing steps, was used to process the data and construct the network"
77;POAP;10.1038/s41598-021-92622-0;We have obtained the SMILES notations of these compounds, and generated their 3D models (in mol2 format) through the POAP Ligand Preparation pipeline
78;POAP;10.31557/APJCP.2019.20.11.3399;"POAP calculates the ligand binding energy and scoring based on the AutoDock Lamarckian Genetic Algorithm and free energy empirical scoring (Morris et al., 2009; Samdani and Vetrivel, 2018)"
79;POAP;10.31557/APJCP.2019.20.11.3399;POAP integrates the tools such as Open Babel, AutoDock, AutoDock Vina and AutoDockZN in an easily configurable Bash shell-based text interface
80;POAP;10.31557/APJCP.2019.20.11.3399;The energies obtained from POAP were compared with the ligand score identified from Schrodinger
81;POAP;10.3389/fmed.2021.672629;POAP implements dynamic file handling methods for efficient memory usage and data organization, ligand minimization (5,000 steps), MMFF94 force-field was employed with the addition of hydrogens
82;Pandas;10.1002/advs.202001314;Statistical analyses were performed using Python libraries Numpy, Scipy, and Pandas.
83;Pandas;10.1007/s00701-021-04837-9;The large datasets were analyzed using Python 3.8.2 in conjunction with Pandas 1.2.1 and NumPy 1.20.0 before statistical analysis.
84;Pandas;10.1021/acs.analchem.0c04373;The package was written with Python (v.3.6.6) and relied heavily on Pandas (v
85;Pandas;10.3390/ani10060939;The Python [15] language was used with critical libraries including SciPy [16,17], NumPy [18,19], Pandas [20], Scikit-Posthocs [21], Lifelines [22], Matplotlib/Seaborn [23,24], and FBProphet [25].
86;Pandas;10.3390/ani11030865;To obtain the bacterial composition and function profiles of sable intestinal microbiome, the proportion of microbes at different taxonomic levels and functional gene benchmark were calculated in the NumPy and Pandas packages in Python
87;Pandas;10.3390/antibiotics10040385;The clean data were then merged with metadata (i.e., DOI, article title, year published, journal, PMC ID) from the paired JSON files for each article, and a combination of Pandas, a popular python library with integrated data processing functions, data frames and NLTK processing were used to calculate the term counts per article
88;PresenceAbsence;10.1038/s41598-018-35920-4;The best cut-off value or optimal classification threshold for each model was defined as that which provides the maximum percentage of correct predictions (accuracy, ACC), and was calculated using the PresenceAbsence package (version 1.1.9)
89;PresenceAbsence;10.1038/s41598-020-74682-w;Since these methods need background data points we generated a randomly drawn sample of 10,000 background points (e.g., pseudo-absence points) from the extent of the study area using the PresenceAbsence package
90;PresenceAbsence;10.1186/1476-072X-11-39;To evaluate the predictive power of the four models, the area under the curve (AUC), from a receiver operating characteristics (ROC) analysis, was calculated (“PresenceAbsence” package in R) [52,53]
91;PresenceAbsence;10.1371/journal.pntd.0005722;GLMMs, VIFs and the accuracy measures were conducted using the lme4 [95], car [96] and PresenceAbsence package [97] in the R software [98], respectively.
92;PresenceAbsence;10.1371/journal.pone.0134043;The ROC curve and kappa statistical analyses were implemented with the PresenceAbsence package for R.3.0.0 [59].
93;PySiology;10.3390/bs10010011;For the analysis of the physiological signals and the computation of the physiological synchrony, we used custom scripts based on pyphysio [33,34], physynch [35], and PySiology [36].
94;Pygments;10.1186/s12864-016-3278-x;Code syntax was highlighted using Pygments version 2.2 [33].
95;Python package sklearn;10.1186/s12916-020-01748-x;We tested gene signatures using the test set, and ROC curves were generated to compute performance metrics using the Python package sklearn.metrics.roc_curve [29]⁠
96;Python package sklearn;10.1186/s13059-020-02189-8;With the values for the above sequence and epigenetic features and the category labels, we performed random forest analysis using the RandomForestRegressor function in the Python package sklearn [57] to evaluate the importance of each feature to the DNA methylation heterogeneity.
97;Python package sklearn;10.3390/ijms21030713;Each ML method was applied with its default settings using Python package sklearn [40], both with and without data trimming, separately for each dataset
98;Python sklearn;10.3389/fnins.2021.629892;"The task consists in training a 256-500-500-10 network to solve a synthetic classification problem with 16×16-pixel images and 10 classes; the data to classify is generated automatically with the Python sklearn library (Pedregosa et al., 2011) (see section 4)"
99;Python sklearn;10.3389/frobt.2019.00049;Accuracy, precision, recall and F1 score were calculated to assess the performance of the classifier (following recommendations in Sorower (2010) and using the weighted implementations of the metrics available in the Python sklearn toolkit)
100;Python sklearn;10.3389/frobt.2019.00049;"We compared the performances of four of classifier (random forest classifier, extra-tree classifier, multi-layer perceptron classifier and a k-Nearest Neighbor classifier, using implementations from the Python sklearn toolkit; hyper-parameters were optimized using a grid search where applicable), and eventually selected a k-Nearest Neighbor (with k = 3) classifier as providing the best overall classification performance."
101;Python sklearn;10.3390/e22111274;Having produced clustering of the dataset by several approximating graphs (see previous steps) one can use the well-established and efficiently implemented in Python sklearn library scores for clustering comparison
102;Python sklearn;10.3390/e22111274;They are efficiently implemented in Python sklearn library
103;Python sklearn;10.3390/pharmaceutics13071026;Drug molecular features were standardised by removing their mean and scaling to unit variance (performed with the StandardScaler tool in the Python sklearn.preprocessing library)
104;Python sklearn library;10.1155/2020/1384749;The SVM and RF algorithms were implemented using the Python sklearn library [49]
105;Python sklearn library;10.2196/19133;We computed bag-of-words models using the TfIdfVectorizer() function in the Python sklearn library [43].
106;Python sklearn library;10.3389/fonc.2021.652063;Thus, to further characterize them we used several non-deep ML methods implemented in the Python sklearn library (56).
107;Python sklearn library;10.3390/e22111274;It is implemented using Python sklearn library which provides highly standardised and efficient set of tools for calculation clustering quality and clustering comparison scores
108;QCluster;10.1186/s13015-014-0029-x;All these measures are implemented in a software called QCluster
109;QCluster;10.1186/s13015-014-0029-x;Both these approximations are implemented within the software QCluster and tests are presented in the Experimental Results section.
110;QCluster;10.1186/s13015-014-0029-x;Starting from this software we developed QCluster by incorporating the computation of the -type statistics described above using both AWP and AQP prior probability estimators and the redistribution of quality values.
111;QCluster;10.1186/s13015-014-0029-x;These statistics are implemented in a software called QCluster (http://www.dei.unipd.it/~ciompin/main/qcluster.html).
112;QCluster;10.1186/s13015-014-0029-x;We apply QCluster using different distances, to the whole set of reads and then we measure the quality of the clusters produced by evaluating the extent to which the partitioning agrees with the natural splitting of the sequences
113;R package STAR;10.1016/j.cub.2019.09.005;Reads from the RNA-sequencing experiments provided as raw fastq data were quality controlled and mapped to the M. truncatula reference genome version 4.0 (Mt4.0v1) [70] using R package STAR [61]
114;R2DT;10.3390/ijms22094686;Both predicted structures were generated by R2DT using the lysine riboswitch template provided by Rfam [39,40].
115;RAINBOWR;10.1371/journal.pcbi.1007663;A stable version of RAINBOWR is available from the CRAN (Comprehensive R Archive Network), https://cran.r-project.org/web/packages/RAINBOWR/index.html
116;RAINBOWR;10.1371/journal.pcbi.1007663;Our R package, RAINBOWR, was implemented using the R packages Rcpp version 1.0.2 [33–35] and RcppEigen version 0.3.3.5.0 [36] to reduce the computational time required for solving the multi-kernel mixed-effects model described below
117;RAINBOWR;10.1371/journal.pcbi.1007663;Source codes for the R package RAINBOWR are deposited in S1 File
118;RAINBOWR;10.1371/journal.pcbi.1007663;The latest version of RAINBOWR is also available from the “KosukeHamazaki/RAINBOWR” repository in the GitHub, https://github.com/KosukeHamazaki/RAINBOWR
119;RAINBOWR;10.1534/g3.120.401582;This model was implemented using the package RAINBOWR (Hamazaki and Iwata 2020) in R (R Core Team 2019).
120;Rdistance;10.7717/peerj.4746;"We conducted density analyses with the “Rdistance” package in R-statistical software (R Development Core Team, 2012; McDonald, Nielson & Carlisle, 2015)"
121;Rtrack;10.1186/s40478-021-01190-x;Analysis of search strategies with Rtrack revealed no significant differences one training day one, however rmTBI mice used significantly different search strategies by training days two (χ2 = 6.2, df = 2, p = 0.05) and three (χ2 = 33.9, df = 2, p < 0.0001) with rmTBI mice utilizing less allocentric strategies than shams by the end of training
122;SCINA;10.1016/j.ebiom.2021.103390;B cells (CD19, MS4A1, CD79A), T cells (CD3D, CD3E, CD3G), CD4 T cells (CCR7, CD4, IL7R, FOXP3, IL2RA), CD8 T cells (CD8A, CD8B), Natural killer cells (KLRF1), Macs Monos DCs (TYROBP, FCER1G), Epithelial (SFTPA1, SFTPB, AGER, AQP4, SFTPC, SCGB3A2, KRT5, CYP2F1, CCDC153, TPPP3) cells were identified using relevant gene markers using SCINA algorithm [23]
123;SCINA;10.1186/s13059-019-1795-z;For the two PBMC datasets (Zheng 68K and Zheng sorted), the prior-knowledge classifiers Garnett, Moana, DigitalCellSorter, and SCINA could be evaluated and benchmarked with the rest of the classifiers
124;SCINA;10.1186/s13059-020-02116-x;For SCINA, we first performed quantile normalization on the log-transformed expression matrix and annotated the dataset with the LM22 gene signature
125;SCINA;10.1186/s13059-021-02281-7;This could be the reason that SCINA, a method that merely relies on the expression of marker genes, performed much worse than scSorter and Garnett
126;SCINA;10.3390/genes10070531;With more reference single cell datasets, such as those from the Tabula Muris project, supervised analyses of single cells, such as SCINA, will quickly become more feasible, useful and relevant.
127;SKLEARN;10.1155/2020/8858588;Li [4] proposed an SKLEARN system based on automatic machine learning
128;SKLEARN;10.1155/2020/8858588;Through the automatic solution algorithm of the SKLEARN system and the optimization of hyperparameters, the accuracy of radar signal recognition is improved and the stability is more reliable
129;SKLEARN;10.3389/fmicb.2019.01084;As SKLEARN represents a machine learning approach, the additional flexibility provided was to assign taxonomy after training the algorithm with extracted reference sequences from the SILVA and UNITE databases using the aforementioned PCR primers and trimmed to a length equal to the maximum length of the reads after quality filtering
130;SKLEARN;10.3389/fmicb.2019.01084;In relaxed terms, during the training process SKLEARN splits each reference sequence into a series of overlapping heptamers and assigns a level of taxonomy to a given collection of heptamers
131;SKLEARN;10.3389/fmicb.2019.01084;Later on, during the classification process SKLEARN splits each sequence once again into a collection of overlapping heptamers, and tries to assign a level of taxonomy by taking into consideration the collections of heptamers from the reference sequences
132;SKLEARN;10.3389/fmicb.2019.01084;The training process of SKLEARN is based on k-mers where the value 7 was used as it is the default balanced QIIME 2 parameter
133;SPAtest;10.7554/eLife.25060;(A) The results of running PheWAS using SPAtest
134;STAR;10.1063/5.0054639;Analysis was conducted using STAR, R, DeSeq2, and python software.
135;STAR;10.1107/S2059798321001856;Starting with RELION version 3.1, information about optics groups has been added as a second data table to STAR files
136;STAR;10.1177/1759091415593066;Reads were aligned with the STAR (Dobin et al., 2013) aligner with the additional options (–outFilterMatchNmin 30 –outFilterMismatchNmax 5 –outFilterMismatchNoverLmax 0.05 –outFilterMultimapNmax 50)
137;STAR;10.1177/1759091419843393;All RNA reads were first mapped to the mouse (GRCm38) reference genome using STAR v2.5.2b release with default parameters (Dobin et al., 2013)
138;STAR;10.1186/s13568-020-01170-9;Sequences were aligned using STAR (Dobin et al
139;STAR RNA-seq aligner package;10.1371/journal.pone.0239938;taurus UMD 3.1.1) [35] obtained from Ensembl [36] using the STAR RNA-seq aligner package (version 2.5.3a) [37]
140;STAR package;10.1186/s12920-021-01041-7;Read quantification was conducted and aligned to the GENCODE v28 (HG38) reference genome using STAR package [16]
141;STAR package;10.1186/s13059-021-02364-5;In brief, STARsolo from STAR package v2.7.5c [14] and Cell Ranger v2.1.1 and scSNV were limited to at most 24 threads
142;STAR);10.1002/advs.202000593;Spliced trans alignment to a reference (STAR) (v2.4.2a) software was used to perform sequence alignment on the clean reads of each sample
143;STAR);10.1093/gigascience/giy092;As a proof-of-concept example, we create a hot-start container for the spliced transcripts alignment to a reference (STAR) aligner and deploy this container to align RNA sequencing data
144;STAR) RNA-Seq aligner;10.3390/cells10020273;A custom up-to-date version of the Mus Musculus genome was generated using the spliced transcripts alignment to a reference (STAR) RNA-Seq aligner [23] (version 2.7.6a) using the “—twopassMode Basic” parameter to increase the mapping change of the reads
145;STAR) aligner;10.1093/gigascience/giy092;We demonstrate that hot-starting from a saved container checkpoint can significantly reduce the execution time using the spliced transcripts alignment to a reference (STAR) aligner [14, 15] for RNA-seq data analyses
146;SentimentAnalysis;10.1371/journal.pone.0209323;The package SentimentAnalysis is available for download via CRAN: https://cran.r-project.org/package=SentimentAnalysis.
147;SentimentAnalysis;10.2196/jopm.9745;The SentimentAnalysis package [40] in R yielded the best results at 0.81 (average system-rater agreement measured using Cohen kappa) reaching good interrater agreement
148;SentimentAnalysis;10.34172/apb.2021.023;To analyze comments, the library “SentimentAnalysis” has been used
149;SeuratObject;10.1093/gigascience/giaa102;Another issue was the proliferation of the many different and quickly evolving R-based file formats for processing and storing the data, such as SingleCellExperiment as used by the Scater suite, SCSeq from RaceID, and SeuratObject from Seurat [10,11]
150;SeuratObject;10.3389/fonc.2021.557477;After taking the intersection of the genes of the two datasets, “SeuratObject” was constructed, respectively
151;SparseDC;10.3389/fgene.2019.00629;SparseDC solves a unified optimization problem so that it can carry out three tasks simultaneously, e.g., identifying cell types, tracing expression changes across conditions, and identifying marker genes for these changes (Barron et al., 2018)
152;Spliced Transcripts Alignment to Reference;10.7717/peerj.5759;Sequences were aligned to the equine genome with Spliced Transcripts Alignment to Reference software
153;Spliced Transcripts Alignment to a Reference (STAR);10.1186/s12915-016-0279-9;Reads were subsequently mapped with Spliced Transcripts Alignment to a Reference (STAR) [92], and gene expression analysis was carried out using HTSeq [93] and DESeq2 [94] with conditional quantile normalization [95]
154;Spliced Transcripts Alignment to a Reference (STAR);10.1186/s40478-016-0338-z;For this analysis, we directly aligned all reads from these datasets to an alignment index containing the human genome plus a library of 740 virus genomes (including sequences from all known human viruses documented by NCBI) using the Spliced Transcripts Alignment to a Reference (STAR) aligner
155;Spliced Transcripts Alignment to a Reference (STAR);10.1186/s40478-018-0519-z;Sequencing raw reads were mapped to the Ensemble RN5 genome with Spliced Transcripts Alignment to a Reference (STAR) software (version 2.3.0e_r291) [24] with parameter alignIntronMax 1 to prohibit splicing and allow genomic mapping
156;Spliced Transcripts Alignment to a Reference (STAR);10.18632/aging.102840;Reads were aligned to the Human reference genome (GRCh37) using the Spliced Transcripts Alignment to a Reference (STAR) software v2.5 [47], then summarized as gene-level counts using featureCounts 1.4.4 [48]
157;Spliced Transcripts Alignment to a Reference (STAR);10.18632/aging.102968;The reads were aligned to the UCSC reference human genome 19 (hg19) using Spliced Transcripts Alignment to a Reference (STAR) software v2.3.1 (DNASTAR, Inc
158;Spliced Transcripts Alignment to a Reference (STAR);10.3390/ani11041131;The clean reads were mapped to the unigenes from full-length transcriptome by Spliced Transcripts Alignment to a Reference (STAR) [26].
159;Star;10.1002/advs.201903583;"After sequencing, the raw data were filtered using Fastp software to remove low‐quality sequences, undetected sequences, and linker sequences, and named” Clean Data.” The Clean Data sequences were compared with the genomic sequences using the Hisat2, MapSplice, Star, and Tophat2 algorithms, and a bam file of the genomic alignment was obtained; the chromosome distribution and genomic alignment rate data were obtained from the “Sample.bam” file"
160;Star;10.1186/1751-0147-53-57;The chromatogram was analysed by means of the program Star 5.5 (Varian), with the internal standard as reference peak
161;Star;10.1186/s40478-016-0364-x;Reads were aligned using the Star 2.3.1 l aligner [51] to the ensemble reference, in which two mismatches were allowed
162;Statistix;10.1371/journal.pone.0000677;After logarithmic transformation, most of obtained data were normally distributed as confirmed by Shapiro-Wilk normality test (using Statistix 7.0®, Analytical software, Tallahassee, FL, USA)
163;TreeToReads;10.1099/mgen.0.000261;Considering that TreeToReads does not simulate SVs, this result indicates that real, complex variants between the reference and root sequence used in the first benchmark surely caused many false-positive SNP calls and enabled realistic evaluation of the accuracy.
164;TreeToReads;10.1186/s12859-017-1592-1;Anchoring an observed genome to a tip in a tree using TreeToReads allows us to test choices about selection of reference genomes in a way that is directly comparable to empirical data
165;TreeToReads;10.1186/s12859-017-1592-1;If ART is invoked in TreeToReads the program will output a folder labeled ‘fastq’ containing directories labeled with the names of each tip from the simulation tree, in which the simulated reads are deposited in.fastq.gz formats
166;TreeToReads;10.1186/s12859-017-1592-1;TreeToReads allows researchers to test the joint effects of multiple parameter values on the ability of any analysis pipeline to recover the signal and infer the correct tree
167;TreeToReads;10.7717/peerj.3893;The simulated dataset was created using the TreeToReads v 0.0.5 (McTavish et al., 2017), which takes as input a tree file (true phylogeny), an anchor genome, and a set of user-defined parameter values
168;TunePareto;10.1016/j.ebiom.2016.08.037;All experiments were performed with help of the TunePareto software (Mussel et al., 2012).
169;TunePareto;10.1038/s41598-017-05116-3;All classification experiments were conducted with help of the TunePareto Software.
170;TunePareto;10.1098/rsif.2019.0612;All experiments were performed with help of the TunePareto software [44].
171;TunePareto;10.18632/oncotarget.22601;All experiments have been conducted in the TunePareto framework [49].
172;TunePareto;10.3390/biom8040158;Experiments are performed with the R-package (www.r-project.org) TunePareto [68].
173;Velvet;10.1128/genomeA.00342-16;The data generated was assembled using Velvet (v1.2.10.) (8), resulting in 81 contigs, a total of 4,513,937 bp, and a N50 contig size of 233,632 bp
174;Velvet;10.1186/1471-2164-13-3;The variable paired end sequences for each common SE were extracted using the filtered sequence set and compiled for the LongRead® contig construction, using a modified version of the Velvet sequence assembler (v
175;Velvet;10.1371/journal.pone.0143472; De novo assembly was performed on the unmapped reads following assembly with bowtie2, as well as the entire read set, using Velvet [33] in conjunction with the VelvetOptimiser (http://bioinformatics.net.au/software.velvetoptimiser.shtml)
176;Velvet;10.1371/journal.pone.0216679;"Velvet ([77]; https://www.ebi.ac.uk/~zerbino/velvet/)"
177;Velvet;10.3389/fmicb.2017.02619;For genomic assembler it is used the Velvet algorithm package
178;WeightedROC;10.2196/16153;"R version 3.6.1 and variable functions from its packages were used: decision tree, “rpart” in the caret package (using down-sampling and cross-validation) [19,20]; dummy-coded variables, “dummy.code” in the psych package [21]; cross-validation for Lasso, “cv.glmnet” in the glmnet package [22]; survey-weighted multiple logistic regression, “svyglm” in the survey package [23]; weighted ROC curve and AUC, “WeightedROC” and “WeightedAUC,” respectively, in the WeightedROC package [24]; confidence interval of AUC"
179;WeightedROC;10.2196/27344;Receiver operating characteristic (ROC) curves were generated using the ggplot2 package [26], and the weighted AUC was calculated by using the “WeightedAUC” function with the sample weight (WeightedROC package) [19].
180;activity;10.1038/s41598-021-99341-6;We plotted the activity patterns of each species using a von Mises kernel using the package activity in R version 4.0.2
181;activity;10.1186/s12870-015-0666-3;Results This report demonstrates that a respiration activity-monitoring system (RAMOS) can identify compounds with defense priming-inducing activity in parsley cell suspension in culture
182;activity;10.1371/journal.pone.0256876;"We used the function compareCkern in package activity [52] to test whether activity patterns were significantly different for the same species across different sites; for different species at the same site; and between males and females of the same species at the same site (for lion and leopard only)."
183;activity;10.3390/ani11020562;For each pairwise comparison between species, an additional bootstrap analysis was performed to estimate the probability that two sets of circular observations belonged to the same distribution, using the package activity [17] in R 3.5.1
184;activity;10.3390/ani9121071;Thus, we fitted non-parametric circular kernel density models using the packages “activity” (Version 1.1) and “overlap” (Version 0.3.2) [72,73], in the statistical software R (version 3.6.1) [74]
185;ads;10.1002/ece3.4592;The L 12 (r)?function was calculated between 1 m and 10 m for distance r by using the package ads (Pélissier & Goreaud, 2015) for free statistical software R (R Core Team, 2017)
186;ampir;10.7717/peerj.10555;(2020) presented ampir, which does support detection of precursor sequences
187;anocva;10.3389/fnins.2017.00016;ANOCVA is implemented in R and is freely available at the R project website (package “anocva”).
188;assignPOP;10.1002/ece3.6304;Then, both data sets were used as input files to the assign.X function from assignPOP v1.1.4 (Chen et al., 2018), which uses the baseline data (i.e., the basin of origin data set) to build a predictive model which is subsequently used to make predictions on the origin of the test data (i.e., the genotypes of the Corsican lobsters)
189;assignPOP;10.1038/s41598-020-72369-w;The package assignPOP was also used to standardize the data with and without the optional software to remove low variance loci across the dataset
190;assignPOP;10.1111/eva.12849;assignPOP uses a cross‐validation procedure followed by PCA to evaluate assignment accuracy and membership probabilities
191;assignPOP;10.1111/eva.12962;For cross‐validation in assignPOP, using the top half loci with highest F ST values versus
192;assignPOP;10.3390/ani10091584;In order to overcome the problem of unbalanced population sizes, a machine-learning approach (assignPOP package, r software) was applied to study the mean and variance of assignments (Figure 3)
193;beautifulsoup4;10.3390/s20010190;Our API for the acquisition of weather data operates by scraping the web page of the automatic Brazilian weather stations [5] using the libraries requests [67] and beautifulsoup4 [68], as well the frameworks Django [69] and Django rest [70]
194;beepr;10.1371/journal.pone.0188955;It incorporates existing packages calibration, roc, gbm.predict.grids (from [31]’s appendix, bundled into gbm.utils by Dedman), beepr, dismo, gbm, mapplots, mgcv, raster, rgdal and vegan, and functions also built by the authors for this task: gbm.map, gbm.basemap, gbm.rsb, gbm.cons, gbm.valuemap, gbm.bfcheck and gbm.loop
195;beepr;10.3389/fimmu.2021.632333;The raw RNA-seq data was analyzed using the statistical computing environment R, the Bioconductor suite of packages for R and RStudio (tidyverse, reshape2, tximport, biomaRt, RColorBrewer, genefilter, edgeR, matrixStats, hrbrthemes, gplots, limma, DT, gt, plotly, beepr, skimr, cowplot, data.table, sva).
196;biopy;10.1093/molbev/msx126;For all simulations, the version of biopy (Heled, unpublished data) used was 0.1.9
197;biopy;10.1093/molbev/msx126;Simulated trees were generated using biopy, and trees sampled from a prior distribution were generated using StarBEAST2 with all new features enabled
198;biopy;10.1093/sysbio/syv118;Sequence alignments were also simulated using biopy for experiments 1 and 2, and Seq-Gen (Rambaut and Grass 1997) was used to simulate nucleotide alignments for experiment 3.
199;biopy;10.1186/1471-2148-12-23;Tree files were summarized in biopy v0.1.2 [104], the posterior was resampled, and the variance among 100 random resampled species trees was visualized in DensiTree [105]
200;biopy;10.1186/1471-2148-13-221;The methods are implemented in biopy [9], and integrated with DensiTree, making it easy to examine the summary tree in the context of the full posterior.
201;biosignalEMG;10.3758/s13428-020-01435-y;In addition, a baseline correction using the mean level of activation during the baseline measurement was applied to the EMG channels using the dcbiasremoval function from the biosignalEMG package (Guerrero & Macias-Diaz, 2018)
202;biosignalEMG;10.3758/s13428-020-01435-y;In line with the recommendations of Fridlund and Cacioppo (1986), we also applied a low-pass filter at 250 Hz using the lowpass function from the biosignalEMG package (Guerrero & Macias-Diaz, 2018)
203;bootLR;10.1016/j.pmedr.2021.101447;All analyses were performed in RStudio (version 1.2.1335) using the packages rms, cutpointr, bootLR, ROCR, and ggplot2 (R Core Team, 2019, Thiele, 2020, Marill, 2017, Sing et al., 2005, Wickham, 2016, Harrell, 2019).
204;bootLR;10.1371/journal.pone.0224825;Statistical analysis was performed using the program Calc from LibreOffice version 5.3.7.2 (x64), The Document Foundation, Berlin, Germany, and the statistics program R version 3.4.2 (2017), The R Foundation, Vienna, Austria, using packages IswR 2.0–7, bootLR 1.0, epiR 0.9–93 and epibasix 1.3
205;cRegulome;10.1186/s13104-018-3160-9;We developed an R package called cRegulome (unpublished) that interfaces the miRCancerdb programmatically from R environment
206;cRegulome;10.3390/cancers10080273;To explore the last possibility, we queried the cRegulome database to identify transcription factors and microRNAs that target the RKIP/PEBP1 gene and one or more of EMT and autophagy genes in prostate cancer (preparing for publication)
207;cRegulome;10.7717/peerj.6509;Although we didn’t show in this article nor provided a mechanism in cRegulome for integrating transcription factor and microRNA data, it should be possible at least in principle simply by combining the output of two separate queries or their graphs
208;cRegulome;10.7717/peerj.6509;Using the cRegulome package, we identified two transcription factors and three microRNAs that commonly regulate PEBP1 and one or more of the eight gene products of interest.
209;catmap;10.1186/1471-2105-9-130;Results I introduce the package catmap for the R statistical computing environment that implements fixed- and random-effects pooled estimates for case-control and transmission disequilibrium methods, allowing for the use of genetic association data across study types
210;catmap;10.1186/1471-2105-9-130;catmap allows for the meta-analysis of all case-control or all family-based studies and for the combination of both family-based and case-control studies
211;catmap;10.1186/1471-2105-9-130;catmap calculates fixed- and random-effects estimates, a χ2 test for heterogeneity and associated p-values and must be used to create a catmap object for use in downstream functions
212;catmap;10.1186/1471-2350-12-107;For all analyses performed here, the statistical package Stata 10 (Stata Corporation, College Station, Texas, USA) and the catmap package developed using the R language (http://www.r-project.org) were used
213;catmap;10.1371/journal.pone.0136282;R Software version 2.13.0 with packages genetics, catmap and meta [14] were used for analyses.
214;cellranger;10.1016/j.celrep.2020.03.063;The cellranger count output was fed into the cellranger aggr pipeline to normalize sequencing depth between samples
215;cellranger;10.1093/jmcb/mjy089;Both ‘cellranger mkfastq’ and ‘cellranger count’ were run with default command line options
216;cellranger;10.1186/s13059-021-02293-3;First, cellranger count used STAR to align cDNA reads to the hg19 human reference transcriptome, which accompanied the Cell Ranger Single Cell Software Suite [44]
217;cellranger;10.1371/journal.pone.0251194;Loaded via a namespace (and not attached): cellranger 1.1.0, cli 2.4.0, codetools 0.2-18, colorspace 2.0-0, compiler 4.0.3, crayon 1.4.1, crul 1.1.0, curl 4.3, digest 0.6.27, DT 0.18, ellipsis 0.3.1, evaluate 0.14, fansi 0.4.2, farver 2.1.0, fastmap 1.1.0, generics 0.0.2, glue 1.4.2, grid 4.0.3, gtable 0.3.0, hms 0.5.3, htmltools 0.5.1.1, htmlwidgets 1.5.3, httpcode 0.3.0, httpuv 1.5.5, httr 1.4.2, jsonlite 1.7.2, labeling 0.4.2, later 1.1.0.1, lifecycle 1.0.0, magrittr 2.0.1, mime 0.10, miniUI 0.1.1.1, mun
218;cellranger;10.21203/rs.3.rs-664507/v1;We used R package Seurat 3 for downstream analysis of count matrixes that we got as output from cellranger count
219;cgraph;10.1186/s12859-019-2615-x;The software uses cgraph (the C library behind Graphviz) for making metabolic network layouts, and the Allegro 5.2 gaming library for graphics
220;chemometrics;10.1038/s41598-018-34190-4;Multivariate analysis was performed with the R environment (version: 3.3.2), PLS-DA and PLS regression were performed with using the package “MixOmics”, double cross-validation was executed with package “chemometrics”
221;chemometrics;10.1038/s41598-020-73338-z;The functions PCAgrid and pcaDiagplot used for this analysis belonged to the pcaPP and chemometrics packages, respectively
222;chemometrics;10.1371/journal.pone.0089280;Mass spectral data from the XCMS Online analyses were used to construct two sets of principle components analysis (PCA) and linear discriminant analysis (LDA) classification models using the chemometrics statistical package in “R” [53]
223;chemometrics;10.1371/journal.pone.0129740;The ions identified as significantly different in each of the four within-group comparisons, evaluated pre- and post-challenge, were used in principle components analysis (PCA) and linear discriminant analysis (LDA) classification models using the “chemometrics” statistical package in “R” [27]
224;chemometrics;10.3389/fmicb.2018.00938;"Prior to any statistical testing, we added a constant of 10-10 to all NSAF values and then applied a centered log-ratio (clr) transform (Aitchison, 1982; Fernandes et al., 2014) using the clr function from the chemometrics package in R (Kumar et al., 2014)"
225;cobs;10.15698/mic2021.07.754;To this end, we fitted a constrained smoothing spline to the CFU data using the cobs package in R [71] ()
226;cumSeg;10.1371/journal.pone.0078143;Both SomatiCA and cumSeg used model selection to effectively reduce the false positive rate with some compromise on sensitivity
227;cumSeg;10.1371/journal.pone.0078143;In Figure S3 and S4, we show the segmentation from SomatiCA and its comparison with CBS and cumSeg using chromosomes 7 and 10 respectively
228;cumSeg;10.1371/journal.pone.0078143;Overall, CBS and SomatiCA outperformed cumSeg in sensitivity at detecting SCNAs larger than 1 Mb (Figure 2)
229;cumSeg;10.1371/journal.pone.0078143;SomatiCA detected 83% simulated SCNAs whereas cumSeg only captured 10%
230;cumSeg;10.7554/eLife.46259;Data were plotted and the step-function fit (grey dashed line) was calculated using the mean GC value for each locus via the cumSeg package for breakpoint estimation in genomic sequences
231;cytoflow;10.1038/s41598-021-86135-z;The data from the flow cytometry was imported and analysed in Python 3.5 using the cytoflow library
232;dae;10.1038/s41598-017-01211-7;The main plot design was generated using Digger and the subplot randomization was done using dae, packages for the R statistical computing environment The experimental layout used is shown in Fig. 1.
233;dae;10.1371/journal.pone.0239673;The experimental design was obtained using CycDesigN [41] and randomized using the dae package in R [42, 43] (Fig 1).
234;dae;10.3389/fpls.2017.02194;Such model-based approaches can be implemented in various software packages, e.g., dae (Brien, 2017), DiGGer (Coombes, 2009), or OD (Butler, 2013) in R or the OPTEX procedure in SAS (SAS Institute Inc., 2014)
235;dae;10.34133/2019/5893953;The design was randomised using dae [34], a package for the R statistical computing environment [35]
236;data.table;10.1093/eurheartj/ehaa756;All data management and analyses were performed using R version 4.0.0 using the data.table package version 1.12.8, the magrittr package version 1.5, and the lme4 package version 1.1-21
237;data.table;10.1186/s12864-015-2140-x;This package requires R > = 3.0.1 and depends on several R/Bioconductor packages including Rsamtools, GenomicRanges, data.table, stringr, rtracklayer, ggplot2, gridExtra, igraph and Gviz.
238;data.table;10.1186/s12911-019-0837-5;The ETL processes were built using R (version 3.4.4) and the data.table package (version 1.11.6) [21]
239;data.table;10.1186/s13071-020-04319-4;Data analysis was done with the tidyverse and multiple helper functions within the following packages: data.table [21], magrittr [22], here [23], Hmisc [24], ggplot2 [25], gridExtra [26], ggpubr [27] and extrafont [28]
240;data.table;10.1186/s40575-021-00100-7;Following internal validity checking and data cleaning in Excel (Microsoft Office Excel 2013, Microsoft Corp.), data were cleaned in Rstudio™ using the following packages: plyr, dplyr, data.table, tidyR, and stringr [81–84]
241;data.table;10.1371/journal.pbio.3001296;Other R packages used in the analyses and data visualisation were the following: data.table [52], dplyr [53], gridExtra [54], mapdata [55], mcmcplots [56], MCMCvis [57], plyr [58], RColorBrewer [59], rgdal [60], readxl [61], tidyverse [62], viridis [63], and writexl [64].
242;data.table;10.1371/journal.pone.0132783;To facilitate effortless analysis and visualization of the amplicon data, we build the R package ampvis v.1.9.2 (https://github.com/MadsAlbertsen/ampvis) which builds on the R packages phyloseq [36], ggplot2 [37], reshape2 [38], dplyr [39], vegan [40], knitr [41], Biostrings [42], data.table [43], DESeq2 [44], ggdendro [45] and stringr [46].
243;data.table;10.1371/journal.pone.0239197;In writing these scripts, we used the following packages: readr [76], dplyr [77], ggplot2 [78], tidyr [79], reshape2 [80], ggrepel [81], cowplot [82], data.table [83], UpSetR [84], BSgenome.Hsapiens.UCSC.hg38 [85, 86], and Rtsne [87].
244;data.table;10.1371/journal.pone.0246663;The script is written in R programming language with the shiny [6], data.table [7], and ggplot2 [8] packages with customizations added in HTML, JavaScript and CSS languages
245;data.table;10.3390/microorganisms7110547;Other R packages used for data management and visualization included BiocManager [61], biomformat [62], dplyr [63], data.table [64], extrafont [65], gdata [66], ggplot2 [67], plyr [68], prodlim [69] and vegan [70].
246;deepredeff;10.1186/s12859-021-04293-3;Accurate plant pathogen effector protein classification ab initio with deepredeff: an ensemble of convolutional neural networks
247;django-blastplus;10.1186/s12920-018-0430-2;The BLAST server was constructed by django-blastplus (version 0.4.0) and NCBI BLAST+ (version 2.3.0) [43]
248;dplyr;10.1002/ece3.3242;All data analyses were performed in R using code written by GMM and the packages vegan (Oksanen et al., 2015), rioja (Juggins, 2015), ggplot2 (Wickham, Chang, & Wickham, 2013), dplyr (Wickham & Francois, 2016), maptools (Bivand & Lewin‐Koh, 2016), and rgdal (Bivand, Keitt, & Rowlingson, 2016).
249;dplyr;10.1038/s41597-020-00660-6;We used various C++  based R packages (dplyr and sf ) for data handling and geospatial analysis
250;dplyr;10.1186/s40168-017-0387-y;Descriptive statistics were performed using the dplyr [42], tidyr [43], and ggplot2 [44] packages of R [38].
251;dplyr;10.18632/aging.202739;Data representation was performed with softwares R v3.6 and Rstudio v1.2.1335 using tidyverse, dplyr, ggplot2, ggpubr, complexheatmap and corrplot packages.
252;dplyr;10.3389/fgene.2020.564301;The DE matrix was input to ggplot (Wickham, 2006), ggrepel (Slowikowski, 2017), and dplyr (Wickham et al., 2017) to generate volcano plots to visualize DE patterns
253;dplyr;10.3389/fneur.2017.00178;All statistical calculations were performed using Rstudio (Rstudio, Boston, MA, USA) applying standard packages and packages “dplyr,” “BlandAltmanLeh,” “Stats,” and “ICC.”
254;dplyr;10.3389/fphys.2020.00862;The analysis was performed using R version 3.5.3 and tidyr (0.8.3), dplyr (0.8.1) and igraph (1.2.4.1).
255;dplyr;10.3390/ijerph18168514;"Statistical analysis was performed with R software (version 3.6.0, 2018, R Core Team, Vienna; Austria) within R Studio interface using rio, tidyverse, dplyr, questionr, ggplot2, and ggpubr packages."
256;dplyr;10.3390/jpm11060512;R-package ‘dplyr’ was used for data manipulation [28], and R-package ‘summarytools’ was used for frequencies tables, cross-tabulation, and other descriptive statistics [29]
257;dplyr;10.3390/molecules26123564;The general handling of the imported data was using packages from the tidyverse set of packages [65], specifically the packages tidyr [66], dplyr [67], purrr [68], and tibble [69].
258;dplyr;10.3390/pathogens9100841;Data manipulation and analysis was done in R [44] using the packages dplyr [45], tidyverse [46] and forcats [47] for manipulation, epiR [48] for analysis and packages ggplot2 [49], rgdal [50], gridExtra [51], png [52], ggsn [53], maps [54] and maptools packages [55] for graphs and maps
259;eQTLseq;10.1093/bioinformatics/btx355; eQTLseq requires appropriately transformed expression data as input, when a Normal model is selected
260;eQTLseq;10.1093/bioinformatics/btx355;All methods are implemented in the freely available Python software eQTLseq (https://github.com/dvav/eQTLseq).
261;eQTLseq;10.1093/bioinformatics/btx355;Availability and implementation All methods are implemented in the free software eQTLseq : https://github.com/dvav/eQTLseq Supplementary information Supplementary data are available at Bioinformatics online.
262;eQTLseq;10.1093/bioinformatics/btx355;eQTLseq (Fig
263;eQTLseq;10.1093/bioinformatics/btx355;eQTLseq takes matrices X and Z (or Y, a transformed version of Z) as input and employs Gibbs sampling (Andrieu ) to estimate an M?×?K matrix of regression coefficients B
264;earth;10.1038/s41598-018-21530-7;The EARTH analysis was undertaken using the earth package in R
265;earth;10.1038/s41598-020-75476-w;In this study, the ‘earth’ package, as well as the ‘sdm’ package, was used to spatial modeling of snow avalanche debris by the MARS model.
266;earth;10.1155/2018/3719703;Modeling was carried out with the use of computational intelligence tools available as packages in the Open Source statistical environment R [20], namely, fscaret [21], monmlp (Monotonic Artificial Neural Networks) [22], Cubist [23], randomForest (RF, Random Forest) [24], earth (MARS, Multivariate Adaptive Regression Splines) [25], rgp (Genetic Programming and Symbolic Regression) [26], and nloptr [27]
267;earth;10.1186/1471-2156-9-71;We found that the computational burden of RFs using the randomForest package in R is greater than that of MARS, using the earth package which is also implemented in R
268;earth;10.3390/healthcare9060710;We used R package “stats”, “e1071”, “earth”, “rpart”, “XGBoost” to construct the prediction models LR, CART, SVR, MARS, and XGBoost, respectively.
269;edd-utils;10.3389/fbioe.2021.612893;The edd-utils package uses EDD's REST API to provide a DataFrame inside of your Jupyter notebook to visualize and manipulate as desired
270;edd-utils;10.3389/fbioe.2021.612893;The users can run the export_study() function from the edd-utils package (see code availability) to download a study from a particular EDD instance
271;enetLTS;10.1186/s12859-020-03653-9;Conversely, regardless of the proportion of outliers, enetLTS had the highest outlier detection accuracy among the three methods.
272;enetLTS;10.1186/s12859-020-03653-9;Other genes selected by enetLTS, FOXA1 [25], GATA3 [25], SPDEF [26], FOXC1 [27], EN1 [28], HORMAD1 [29], KRT16 [30], and CT83 [31], have been reported to be related to TNBC
273;enetLTS;10.1186/s12859-020-03653-9;Overall, the outlier detection accuracy of Ensemble was worse than that of enetLTS
274;enetLTS;10.1186/s12859-020-03653-9;Results The accuracy of variable selection, outlier identification, and prediction of three methods (Ensemble, enetLTS, Rlogreg) were compared for simulated and an RNA-seq dataset
275;enetLTS;10.1186/s12859-020-03653-9;Variable selection accuracy of Rlogreg, enetLTS, and Ensemble when n = 500
276;fastDummies;10.1016/j.prevetmed.2021.105395;There were 57 categorical variables which were coded as 105 dummy variables for the elastic net models (Kassambara, 2018) using fastDummies (Kaplan, 2020)
277;fullfact;10.1002/ece3.1943;The fullfact package more carefully examines the parameter space and can conduct other helpful analyses, including the assay of statistical power (both a priori using simulated data based on the literature and post hoc) for full factorial mating designs
278;fullfact;10.1002/ece3.1943;The fullfact package will ultimately facilitate and enhance the analyses of full factorial mating designs in R.
279;fullfact;10.1002/ece3.1943;We also developed the fullfact package to incorporate fixed effects and nonnormal models, which are also underutilized for these types of analyses
280;fullfact;10.1002/ece3.1943;We introduce fullfact – an R package that addresses these issues and facilitates the analysis of full factorial mating designs with mixed‐effects models
281;fullfact;10.1002/ece3.1943;fullfact : an R package for the analysis of genetic and maternal variance components from full factorial mating designs
282;funFEM;10.1371/journal.pone.0242197;Once again, funFEM and funHDDC produced a hard partition with posterior probabilities equal to either 0 or 1, thus highlighting their tendency to discover hard partitions.
283;funFEM;10.1371/journal.pone.0242197;Specifically, funFEM and funHDDC assigned the patients to the clusters with posterior probabilities essentially equal to 1 even if, as already observed, some functionals shared the features of both the clusters and therefore more uncertainty in the cluster assignment would be desirable
284;furniture;10.1186/s12957-021-02391-3;We used the R software environment for statistical analysis version 4.0.2 (2020-06-22) and the packages dplyr, furniture, and epiR
285;furniture;10.2196/27977;The data set was analyzed using RStudio 1.2.1335 [20] with additional packages (furniture [21], lme4 [22], ROCR [23], and randomForest [24]), and visualizations were created in Tableau 2020.3.2 [25]
286;fwdpy;10.1371/journal.pgen.1006573;The model was implemented using the Python package fwdpy version 0.0.4, which uses fwdpp[87] version 0.5.1 as a C++ back-end
287;gdsCAD;10.3390/cells7080094;"The mask was manufactured by Delta Mask, Toppan, Netherlands, using a GDSII layout file that was created using the Python platform and gdsCAD 0.4.5 package containing the pattern designs; i.e., circle patterns with a diameter 50 ?m"
288;geobr;10.1371/journal.pntd.0009700;We produced the maps using R software, geobr package [31, 32], (MIT license https://ipeagit.github.io/geobr/).
289;geobr;10.1371/journal.pone.0257235;Maps data obtained using geobr, an open-source respository of public domain, official spatial data sets of Brazil [24].
290;geobr;10.3390/pathogens10080988;Geographical maps and general plots were generated using R v3.6.1 [68], and the ggplot2 v3.3.2 [69], geobr v.1.4 [70], and sf v0.9.8 [71] packages
291;ggnewscale;10.1186/s13040-021-00267-6;eQTpLot was developed in R version 4.0.0 and depends on a number of packages for various aspects of its implementation (biomaRt, dplyr, GenomicRanges, ggnewscale, ggplot2, ggplotfy, ggpubr, gridExtra, Gviz, LDheatmap, patchwork) [12–21]
292;ggnewscale;10.3389/fimmu.2020.620716;Output files of the simulation were analyzed in R (Core Team, 2019) version 3.5.3 using various libraries: forcats (0.5.0), purr (0.3.4), tidyr (1.0.3), tibble (3.0.1), ggplot(2_3.3.0), tidyverse (1.3.0), viridis (0.5.1), viridisLite (0.3.0), ggnewscale (0.4.1), readr (1.3.1), dplyr (0.8.5)
293;ggnewscale;10.3389/fmicb.2021.679671;"R packages “factoextra,” “ggplot2,” “ggnewscale” were used for PCA calculation and processing (Wickham, 2016; Kassambara and Mundt, 2020; Campitelli, 2021)"
294;ggnewscale;10.3389/fmicb.2021.685263;Data visualization was further performed using the packages ggplot2 (Wickham, 2016), ggtree (Yu et al., 2017), ggnewscale (Campitelli, 2020), and cowplot (Wilke, 2020).
295;ggnewscale;10.7554/eLife.43966;"Plots were created using the ggplot2 package and extensions ggrepel, ggforce, ggseqlogo, ggnewscale, ggrastr, RColorBrewer, viridis, and cowplot (Campitelli, 2019; Garnier, 2018; Neuwirth, 2014; Pedersen, 2016; Petukhov, 2018; Wagih, 2017; Wickham, 2016; Wilke, 2018)"
296;ggplot2;10.1007/s00436-020-06666-8;For all statistical analyses, we used the R statistical software (RCore 2015) with the packages ggplot2 (Wickham 2016), glmmTMB (Brooks et al
297;ggplot2;10.1038/s41598-018-28116-3;aureus that produced or did not produce TSST-1) were compared in between-class analysis (BCA) with the ADE4 and ggplot2 packages for R
298;ggplot2;10.1155/2020/8841859;The R package used for this operation includes “clusterProfiler” [11], “org.Hs.eg.db,” “enrichplot,” “DOSE,” “ggplot2,” “stringi,” “colorspace,” “digest,” and “GOplot.”
299;ggplot2;10.1186/s12864-020-6653-6;"Further analysis, visualizations and exploratory data analysis were carried out in R v3.6.0 [38] with the following packages: phangorn v2.5.4 [39]; ape v5.3 [40]; ggplot2 v3.1.1 [41]; ggtree v1.17.1 [42]; gplots v3.0.1.1 [43]; stats v3.6.0."
300;ggplot2;10.1186/s13068-021-01948-4;Enriched categories were plotted using ggplot2 [47]
301;ggplot2;10.1245/s10434-020-09200-3;Data analysis was performed using R Foundation Statistical software (R 3.2.2) with TableOne, ggplot2, Hmisc, Matchit and survival packages (The R Foundation for Statistical Computing, Vienna, Austria), as previously reported.
302;ggplot2;10.3389/fchem.2019.00311;The standard curve was plotted using the ggplot2 library in CRAN-R
303;ggplot2;10.3389/fmicb.2020.00494;These analyses were performed using the “phyloseq” R package and graphical outputs were generated using the “ggplot2” R package
304;ggplot2;10.3390/cancers13051079;Dot plot (generated using R’s ggplot2 package) shows the correlation of NES values generated from GSEA between four indicated comparisons, where the color represents the Spearman correlation and size presents the –log10(p-value) of the correlation using the cor.test function
305;ggplot2;10.7717/peerj.10237;"To estimate the microbial community coverage, OTU tables and rarefaction data files were analyzed using the entropart (v 1.5-3; Marcon, 2018), ggplot2 (v 3.0.0; Wickham, 2016) and vegan (v 2.5.2; Oksanen et al., 2018) packages of the R statistical software (R Core Team, 2017)"
306;ggpmisc;10.1016/j.molcel.2019.12.001;Linear regression lines were plotted using stat_poly_eq function from the ggpmisc (v0.3.1) R package, together with the model’s R2 coefficient of determination
307;ggpmisc;10.1038/s41598-018-25201-5;The Pearson’s correlation coefficient of determination R2 and the p-value were calculated and added to the plots using two functions (stat_poly_eq() and stat_fit_glance()) from the ggpmisc package
308;ggpmisc;10.1186/s12870-021-03103-5;Peak SNP’s were then pinpointed for each selection scan using the ggpmisc package (Aphalo, 2020) in R
309;ggpmisc;10.3389/fpls.2019.01645;The ggpmisc package was used to calculate and add p-values and R2 values to the graph using the functions stat_poly_eq and stat_fit_glance
310;ggpmisc;10.3390/genes12020312;These figures were prepared using R [38] with the library scales included in the basic R and further with packages ggplot2, forcats, and ggpmisc, all belonging to tidyverse [39]
311;ggsci;10.1007/s12031-020-01787-2;Calculations were performed using the ggplot2, ggfortify, ggsci, rlang, viridis, and vegan packages in R software v4.0.3 (R Core Team 2020).
312;ggsci;10.1073/pnas.2100542118;We used the ggplot2 (72), ggpubr, grid, gridExtra, ggsci, scales, png, ComplexHeatmap (73), and ggrepel R libraries for visualization
313;ggsci;10.3389/fmicb.2021.640408;Figures were constructed using R v.4.0.3 (R Foundation for Statistical Computing, Austria) and the data visualization R packages ggplot2 v.3.2.1 and ggsci v.2.9.
314;ggsci;10.3390/cells10092367;MS data were subjected to statistical analysis with R 3.6.1 using the packages plyr [30], reshape2 [31], xlsx [32], ggsci [33], circlize [34], calibrate [35], ggplot2 [36], readxl [37], qpcR [38], splitstackshape [39], tidyr [40], and Tmisc [41]
315;ggsci;10.7554/eLife.59654;Data visualized and statistics calculated using packages ggpubr, ggplot2, ggsci, and dplyr.
316;gitlab;10.1371/journal.pcbi.1008969;I can confirm that I could access the code shared on gitlab
317;gitlab;10.1371/journal.pcbi.1009301;In addition, the authors should guarantee that re-running their code from the gitlab repository is possible.
318;gitlab;10.1371/journal.pone.0252775;"These scripts could be made on dedicated platforms such as the Open Science Framework, github, gitlab, ResearchBox, or the PsychArchives; or maybe also Supplementary Material to this paper"
319;gitlab;10.5334/jors.289;The software may be forked via gitlab which implements issue trackers to enable bug reporting and feature requests.
320;hab;10.1002/ece3.7190;We also assessed the predictive ability of each final model with k?fold cross?validation using the hab package (Basille, 2015)
321;hab;10.1371/journal.pone.0195480;We evaluated models with k-fold cross validation [67] using the hab package [68]
322;httplib2;10.3389/fninf.2014.00052;Dependencies are pydicom, httplib2, and DCMTK
323;insol;10.1002/ece3.2001;"To identify the importance of seasonality in photoperiod for the distribution of F. distichus, we compiled two global rasters with the R packages ‘raster’ (Hijmans 2015) and ‘insol’ (Corripio 2014): (1) Summer solstice, representing the hours of daylight at midsummer (21 Jun); and (2) Winter solstice, representing the hours of daylight at the shortest day of the year (21 Dec)."
324;insol;10.1002/ece3.4189;The azimuth (a) and zenith (z) of the sun were extracted using the “insol” package (Corripio, 2015)
325;insol;10.1038/s41598-020-63005-8;Data on river discharge, water temperature and diel period (calculated with the insol package) was assembled for each hourly increment over the duration of the study.
326;insol;10.1038/sdata.2018.177;The soil microclimate temperatures were divided into daytime and nighttime records by calculating the time of sunrise and sunset each day over the time period, per site, using the ‘insol’ package in R (ver
327;intePareto;10.1186/s12864-020-07205-6;Conclusion intePareto facilitates comprehensive understanding of high dimensional transcriptomic and epigenomic data
328;intePareto;10.1186/s12864-020-07205-6;a The general pipeline of intePareto
329;intePareto;10.1186/s12864-020-07205-6;intePareto determines the Z scores for each gene g and each histone modification type h, defined as: 
330;intePareto;10.1186/s12864-020-07205-6;intePareto is implemented as an R package that provides an easy-to-use workflow to quantitatively integrate RNA-Seq and ChIP-Seq data of one or more different histone modifications
331;intePareto;10.1186/s12864-020-07256-9;[9] introduced an R package, intePareto, for the integrative analysis of RNA-seq and ChIP-seq data for different histone modifications
332;interep;10.3390/genes10121002;We developed a package, (interep https://cran.r-project.org/package=interep) that incorporates our recently developed penalization procedures to conduct interaction analysis for high-dimensional lipidomics data with repeated measurements [21].
333;intsvy;10.3389/fpsyg.2020.577410;"However, these kinds of procedures are rare with traditional statistics programs, meaning representative analyses need either add-ons such as the IDB Analyzer or specifically developed packages for R (e.g., survey; BIFIEsurvey, or intsvy; see Heine and Reiss, 2019)."
334;knitr;10.1007/s00277-020-04362-2;"Survival times calculations were done using GraphPad Prism software (release 5.2; San Diego, CA, USA), IBM SPPS Statistics (release 24.0; Armonk, NY, USA), and the statistical software environment R (release 3.3.2; Vienna, Austria), together with the R packages ‘maxstat’ (release 0.7-25), ‘knitr’ (release 1.20), ‘survplot’ (release 0.0.7), ‘rms’ (release 5.1-2), ‘cmprsk’ (release 2.2–7), and ‘survival’ (release 2.42-6)"
335;knitr;10.1037/xge0000739;The Method and Results sections of this article were generated from R code using the literate programming tool knitr
336;knitr;10.1038/s41598-018-30701-5;report generation: pacman v0.4.6, knitr v1.20, here v0.1.
337;knitr;10.1038/s41598-020-76843-3;For convenience of the workflow we further used RStudio 1.1.463 and the following R-packages: dplyr 0.8.0.1, forcats 0.3.0, ggplot2 3.1.0, purrr 0.3.1, readr 1.3.1, stringr 1.4.0, tibble 2.0.1, tidyr 0.8.3, skimr 1.0.5, rstudioapi 0.7, rmarkdown 1.10, knitr 1.20 and hues 0.1.
338;knitr;10.1080/17453674.2019.1624339;5.1-3) for survival modelling, knitr (v
339;knitr;10.1136/bmjopen-2019-031251;All analyses were performed using the R statistical language (R Foundation for Statistical Computing, Vienna, Austria, V.3.5.3 with packages knitr, Gmisc, ggplot2, tidyverse, intubate).
340;knitr;10.1186/s12859-016-1125-3;At the end of the pipeline, a full report of all results could be produced using “literate programming” tools such as knitr [41] or sweave [42].
341;knitr;10.1186/s12859-021-03970-7;[32], zoo version 1.8-7 [33], sm version 2.2-5.6 [34], cowplot version 1.0.0 [35], ggplot2 version 3.2.1 [36], knitr version 1.27 [37], microbenchmark version 1.4-7 [38], and RcolorBrewer version 1.1-2 [39]
342;knitr;10.12688/f1000research.13049.2;The framework outputs a file collection that includes a dynamically generated PDF report using R, knitr, and LaTeX, as well as publication-ready table and figure files
343;knitr;10.3390/jintelligence6030041;All analyses were conducted in R [71] using RStudio [72] and the packages psych [73], lm.beta [74], knitr [75], apaTables [76], caret [77], tidyverse [78], and readr [79]
344;kontroller;10.7554/eLife.27670;Using kontroller to automate the process, we performed a direct search on hardware to find the best distribution of control signals that was closest to the desired Gaussian distribution
345;kontroller;10.7554/eLife.27670;We wrote a general-purpose acquisition and control system called kontroller (available at https://github.com/emonetlab/kontroller) in MATLAB (Mathworks, Inc.) to control MFCs, valves and LEDs and to collect data from electrophysiology and the stimulus measurement.
346;libmesh;10.1007/s00466-017-1381-8;For the implementation we used the C++ library libmesh [33], and the multi-frontal direct solver mumps [2] to solve the resulting linear systems
347;logspline;10.1038/s41387-021-00154-3;For calculating tail probabilities on permutation tests, the logspline-library was used
348;logspline;10.1186/s12870-019-1791-1;All the statistical analyses and plots were performed using both Past multivariate statistics software package v3.0 [63] and the computing environment R 3.5.1 (R Development Core Team, 2005) with the following packages: ‘dplyr’, ‘fitdistrplus’, ‘ggplot2’, ‘logspline’, ‘multcomp’, ‘nsRFA’, and ‘Rmisc’.
349;logspline;10.3390/ijerph18189589;For statistical inference, the Savage-Dickey Bayes Factors (BF10) were computed [73,74] by using the package logspline 2.1.15 [75]
350;lubridate;10.1007/s00228-020-03052-2;R-studio (version 1.2.1335) with the packages dplyr, readr, lubridate, stringr, openxlsx, comorbidity, table 1, geepack, ggplot2, grid, and gridExtra was used for the data scripts, analysis, and visualization.
351;lubridate;10.1038/s41598-021-87837-0;"""All the analyses described above were performed using R and its packages, including the following: """"tidyverse"""" (for data wrangling and visualization), """"igraph"""" (for network analysis), """"lubridate"""" (for handling date), """"ggraph"""" (for visualization), """"slam"""" (for data wrangling), and """"poweRlaw"""" (for analyzing power law distribution)."""
352;lubridate;10.1186/s12889-018-5897-4;Several packages were loaded into R for use with the analyses, including “psych”, “effsize”, “lubridate”, “rms”, and “pscl”
353;lubridate;10.12688/gatesopenres.12929.1;"The following open source R packages were used to perform data evaluation from initially accessing data to producing the final manuscript: bookdown v0.9, DBI v1.0.0 , dbplyr v1.2.2, here v0.1, hms v0.4.2, lubridate v1.7.4, networkD3 v0.4, purrr v0.3.0, RMySQL v0.10.15, rstudioapi v0.8, snakecase v0.9.2, stringr v1.4.0, tidyverse v1.2.1 ( Allaire ; Allaire ; Grosser, 2018; Grolemund & Wickham, 2011; Henry & Wickham, 2019; Müller, 2017; Müller, 2018; Ooms ; R Special Interest Group on Databases (R-SIG-DB) ; W"
354;lubridate;10.1371/journal.pone.0247002;They have dependencies on the shiny, shinythemes, plotly, ggplot2, epicontacts, tibble, dplyr, shinycssloaders, lubridate, data.table, magrittr, igraph, DT, network, GGally, sna, intergraph and htmlwidgets packages [1, 3–19]
355;lubridate;10.3389/fmicb.2020.594928;All the statistical analysis and plots were generated in R (3.6.1) statistical programming language using ggplot2, dplyr, reshape2, lubridate, ggsci, and ggpubr package available from CRAN and Bioconductor repository.
356;lubridate;10.3389/fneur.2021.651869;The lubridate package in R was used to parse information on time, enabling analysis of weekday vs
357;lubridate;10.3390/ijerph17093208;The R packages tidyverse [37] and lubridate [38] were used for data management, and patchwork [39] was used for some plots
358;lubridate;10.3390/s20247188;Subsequent analyses were done in R (version 3.5.0) [35] and RStudio (version 1.1.463) [36], with several packages for data handling (tidyr, eeptools, reshape, dplyr, lubridate, phyloseq, VIM, margrittr, chron, kableExtra, knitr, and qwraps2) and plotting (corrplot, ggplot2, lattice, ggfortify, sjPlot, and cowplot) [37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54]
359;lubridate;10.7554/eLife.70086;We modified the wayback code using the downloader v.0.4 (Chang, 2015), httr v.1.4.2 Wickham, 2018, jsonlite v.1.7.0 (Ooms, 2014), lubridate v.1.7.9 (Grolemund and Wickham, 2011), and tibble v.3.0.3 packages (Müller and Wickham, 2019).
360;lunar;10.1002/ece3.7006;Visitation was used as the binary response variable, and the covariates included were the month of the visit, the size of the lick in m2, the lick type (face present or not present), elevation in m, slope in degrees, distance the closest river or stream in m, distance from the closest hunting camp in m (a proxy for hunting pressure, see Griffiths, 2020), and the brightness of the moon calculated using the lunar.illumination function in the lunar package (Lazaridis, 2014) in R
361;lunar;10.1371/journal.pone.0246564;Lunar stage was included as a categorical variable of either ‘new’, ‘waxing’, ‘full’ or ‘waning’ and was obtained using the ‘lunar’ package [63] in the R programming environment [64]
362;lunar;10.1371/journal.pone.0252092;Lunar illumination was calculated through the ’lunar’ package in R [43] and represents the proportion of lunar illumination for any specific date and time
363;margins;10.1371/journal.pgen.1009703;This latter approach can be implemented using the margins() package in R
364;margins;10.1371/journal.pone.0248977;"For each predictor variable, we calculated the average marginal effect (AME; that are more informative and intuitive than the classic odd-ratios [49]) in the “margins” package [50] with robust standard errors [51] using heteroscedasticity and autocorrelation consistent estimators implemented in the “sandwich” package [52]"
365;margins;10.3390/ijerph17239050;All statistical analyses were performed in SPSS [23] and/or R [24] using packages margins [25] and pscl [26].
366;matplotlib;10.1016/j.dcn.2019.100630;Brain plots were created using the following Python packages: matplotlib (Hunter, 2007), nibabel (Brett et al., 2016), and nilearn (Abraham et al., 2014)
367;matplotlib;10.1038/s41398-018-0334-0;We used the publicly available Python eSig package (version 0.6.31) to calculate signatures of streams of data, Python pandas package (version 0.20.1) for statistical analysis, data manipulations and processing, Python scikit-learn package (version 0.18.1) for machine learning tasks and matplotlib for plotting and graphics (version 2.0.1).
368;matplotlib;10.1038/s41598-017-16050-9;Subsequent analyses were performed using Python in Jupyter Notebook, using the numpy, sklearn, pandas, and SciPy packages, as well as matplotlib and Seaborn visualization libraries
369;matplotlib;10.1038/s41598-018-22083-5;All statistical learning, linear modelling and corresponding figures were performed in Python packages scikit-learn, xgboost, statsmodels, matplotlib, seaborn, and pandas.
370;matplotlib;10.1038/s41598-019-43005-z;Gromacs’s trajectory analysis tools, MDTraj along with in-house bash and python scripts were used for data analysis and matplotlib for plotting
371;matplotlib;10.1038/s41598-020-80308-y;Image and data pre- and post-processing was carried out using the Python libraries scikit-image, scikit-learn, OpenCV, numpy, scipy, matplotlib, seaborn, statsmodels and pandas
372;matplotlib;10.1038/sdata.2018.280;The notebooks import the following modules: pandas (version 0.22.0), numpy (version 1.14.0), matplotlib (version 2.1.2), and csv (version 1.0).
373;matplotlib;10.3390/antibiotics9110834;Heatmaps illustrating AMR patterns in calf isolates from each herd were generated in Python (www.python.org) using the matplotlib, pandas and seaborn packages
374;matplotlib;10.7717/peerj.1029;GraPhlAn uses the matplotlib library (Hunter, 2007)
375;matplotlib;10.7717/peerj.8783;The 10 most important features were graphed for each environment based on relative importance utilizing pandas (McKinney, 2010) and matplotlib (Hunter, 2007)
376;methylcheck;10.1186/s13072-019-0321-6;Data obtained via methylcheck python package
377;mirt;10.1007/s10519-021-10076-6;"regardless of genotyping status, see Table 1) by Item-Response Theory (IRT; Embretson and Reise 2000) and calculated with the Generalized Partial Credit Model (GPCM) in R (R Core Team 2017), with the mirt package (Chalmers 2012)"
378;mirt;10.1007/s11136-015-1199-9;Models were estimated using the mirt library [34, 35] in R [36].
379;mirt;10.2196/jmir.9380;The 3PL model was fit to data for each set of questions using the open source software R packages mirt and ltm [48,49]
380;mirt;10.3389/fpsyg.2016.00109;The increase in computing power of personal computers and the development of advanced psychometric software—for example, the “mirt” package in R (R Core Team, 2015), Mplus (Muthén and Muthén, 1998–2012), and flexMIRT (Cai, 2013)—now make it possible to calibrate complex IRT models on relatively long tests with large samples
381;mirt;10.3390/ijerph18030879;The EFA and DIF were performed with the R packages “psych” and “mirt”, respectively [35,36]
382;mixKernel;10.3389/fmicb.2021.609048;(2018) combined metagenomic data and environmental measures of the TARA ocean expedition using unsupervised MKL with the mixKernel package
383;mixKernel;10.3389/fmicb.2021.609048;Here we addressed all these questions, while also providing some examples of how previous kernel-based tools like MiRKAT and mixKernel can fit into our framework.
384;mixKernel;10.3389/fmicb.2021.609048;Some examples are Link-HD (Zingaretti et al., 2020), mixKernel (Mariette et al., 2018), and MOFA (Argelaguet et al., 2018), all focused in the unsupervised learning setting
385;mixKernel;10.3389/fonc.2020.01030;mixKernel (45) is a R package compatible with mixOmics, which allows integration of multiple datasets by representing each dataset through a kernel that provides pairwise information between samples
386;mixKernel;10.3390/metabo11060407;The mixKernel [35] (https://CRAN.R-project.org/package=mixKernel, v0.3, accessed on 11 May 2021) package was used to compute the kernel matrix from the original data
387;mnnpy;10.1016/j.cell.2020.08.013;The resulting peripheral retina, foveal retina, and developed organoid transcriptomes were integrated using the mnnpy implementation of MNN correction (Haghverdi et al., 2018)
388;mnnpy;10.1186/s13073-020-00799-2;Mutual nearest neighbor (MNN) correction [31] (mnnpy [32] version 0.1.9.5) was used to combine data across the eight donors for clustering and cell state identification
389;mnnpy;10.1242/dev.174557;Batch effects between the two datasets were corrected by matching mutual nearest neighbours in the implementation of mnnpy (v0.1.9.3) (parameters: svd_mode=‘irlb’) (Lun et al., 2016)
390;modes;10.1098/rsos.191511;Trip duration distribution of these pooled trips was assessed for bimodality with calculation of the bimodality coefficient [61] and mode(s) estimation(s) by mixture distribution implemented in the modes package [62]
391;modes;10.1371/journal.pone.0223490;0.75–7, and the bimodality coefficient after Ellison [55] was calculated with the R-package ‘modes’ v
392;mokken;10.1007/s10862-018-9686-2;All analyses were performed in the statistical program R 3.1.3, package mokken
393;mokken;10.1080/21642850.2018.1472602;"Among the NIRT models, Mokken Scaling Analysis (MSA) represents the ordinal-level version of Rasch and Rating Scale analyses (Schuur, 2003), is available in R in the mokken package (Ark, 2007), and has been increasingly applied to health research (Stochl et al., 2012; Watson et al., 2012)"
394;mokken;10.1186/s12998-018-0176-0;The aisp function in the mokken package [24] was used to identify potential Mokken or unidimensional scales
395;mokken;10.1371/journal.pone.0149973;Analyses were conducted using SPSS 22 [46], the ‘mokken’ package [47,48] for the R Statistical computing environment [49] and RUMM2030 [50].
396;mokken;10.3390/ijerph111212223;The review of scales, fitting of GRM models, estimation of latent variables, and further analyses were carried out using software written in the R language [54], namely the mokken [43], ltm [55], and psych [45] packages, our own R code, and IRT Pro [56].
397;mord;10.1038/s41598-021-86538-y;For that, we use the ordinal logistic regression model available in the software mord developed by Pedregosa-Izquierdo.
398;mpi4py;10.1186/s12859-020-03562-x;HPC-REDItools are again written in Python (to increase portability and for continuity with the previous version) and makes use of mpi4py library [17] (version 2.0.0) that is the binding of the Message Passing Interface (MPI) standard library for the Python programming language
399;mpi4py;10.1371/journal.pcbi.1005930;We chose an MPI distributed-memory parallelization implemented with the Python library mpi4py [78] whereby simulation of every spot size is mapped to one MPI process
400;mpi4py;10.1371/journal.pone.0146581;from mpi4py import MPI
401;mpi4py;10.3389/fnhum.2013.00869;Simulations and analyses were performed in Python, using NumPy 1.7.1, SciPy 0.12.0, mpi4py 1.3, and OpenMPI 1.6.1 on commercially available x86_64 hardware
402;mpi4py;10.3389/fninf.2018.00092;The simulated results and analysis presented here were made possible using Python 2.7.11 with the Intel(R) MPI Library v5.1.3, NEURON v7.5 (1472:078b74551227), Cython v0.23.4, LFPy (github.com/LFPy/LFPy, SHA:0d1509), mpi4py v2.0.0, numpy v1.10.4, scipy v0.17.0, h5py v2.6.0, parameters (github.com/NeuralEnsemble/parameters, SHA:v0aaeb), csa (github.com/INCF/csa, SHA:452a35) and matplotlib v2.1.0 running in parallel using 120-4800 cores on the JURECA cluster in Jülich, Germany, composed of two 2.5 GHz Intel X
403;mrMLM;10.1038/s41598-019-49737-2;Moreover, a large number of associated SNPs were detected in trait HT, DBH, RW and RYC using mrMLM, FASTmrMLM, FASTmrEMMA and ISIS EM-BLASSO
404;mrMLM;10.1038/s41598-020-80391-1;QTNs associated with the KMC and KDR were detected using five methods in the package mrMLM (mrMLM, FASTmrMLM, FASTmrEMMA, pLARmEB, and ISIS EM-BLASSO)
405;mrMLM;10.1186/s12870-020-02676-x;Therefore, many statistical models have been developed to improve power for identifying genotype-phenotype associations when using the GWAS approach, such as the single-locus mixed linear model (MLM) method [18, 19] and the multilocus methods mrMLM [20], ISIS EM-BLASSO [21], pLARmEB [22], FASTmrEMMA [23], pKWmEB [24], and FASTmrMLM [25]
406;mrMLM;10.3389/fpls.2019.00964;In particular, six recently developed ML-GWAS models including mrMLM (Wang et al., 2016), ISIS EM-BLASSO (Tamba et al., 2017), FASTmrEMMA (Wen et al., 2018), pLARmEB (Zhang et al., 2017), FASTmrMLM (Tamba and Zhang, 2018), and pKWmEB (Ren et al., 2018), have been proved to have more advantages for QTL detection than the single-locus methods
407;mrMLM;10.3390/ijms22137188;GWAS analysis was performed using six multi locus GWAS methods within mrMLM [74], FASTmrMLM [75], FASTmrEMMA [76], pKWmEB [77], pLARmEB [78] and ISIS EMBLASSO [79], which were included in the R package mrMLM v3.1 [80]
408;ndl;10.1111/cogs.12910;(1), (2), (3) implemented in R (R Core Team, 2019) using the edl package (van Rij & Hoppe, 2020) and the ndl package (Arppe et al., 2018)
409;ndl;10.1371/journal.pone.0075734;We then estimated the weights of the model using the ‘ndl’ package in R (version 0.2.10) which implements the Danks equations [23] introduced above
410;ndl;10.1371/journal.pone.0218802;We therefore use the implementation of the equilibrium equations for the Rescorla-Wagner model [30] in version 0.2.18 of the ndl package for the statistical software r to estimate the connection strength (V) of cue (C) to outcome (O): where Pr(C|C) is the conditional probability of cue C given cue C, Pr(O|C) is the conditional probability of outcome O given cue C and n + 1 is the number of different cues
411;ngCGH;10.1002/cam4.3370;For copy number analysis, We used version 0.4.4 of the ngCGH python package to generate aCGH?like data from the WES data
412;ngCGH;10.1371/journal.pcbi.1004873;Tools have been developed for copy number analysis of these datasets, as well, including CNVer [6], ExomeCNV [7], exomeCopy [8], CONTRA [9], CoNIFER [10], ExomeDepth [11], VarScan 2 [12], XHMM [13], ngCGH [14], EXCAVATOR [15], CANOES [16], PatternCNV [17], CODEX [18], and recent versions of Control-FREEC [19] and cn.MOPS [20]
413;ngCGH;10.18632/oncotarget.11922;For copy number analysis from the exome sequencing data, ngCGH and RankSegmentation statistical algorithm in NEXUS software v7.5 were used
414;ngCGH;10.18632/oncotarget.15932;"We also evaluated the ability of the CNA calling tools (CODEX, ngCGH, and falcon) that subcategorize gain and loss events (amplification, +2; duplication, +1; deletion, -1; homozygous deletion, -2) (Supplementary Figure 3) to estimate amplifications and homozygous deletions"
415;ngCGH;10.18632/oncotarget.23975;The ngCGH python package was used to generate aCGH-like data from whole exome sequencing (WES)
416;nlsic;10.1371/journal.pcbi.1007799;The objective function h is minimized using the nlsic optimization algorithm [10] (with 50 iterations)
417;nlsic;10.3390/metabo10040150;The objective function (defined as the sum of squared weighted differences between measured and simulated data) was minimized using the nlsic optimization algorithm [63]
418;nolds;10.1038/s41598-018-24318-x;SampE and DFA were computed using publicly available software “Nonlinear measures for dynamical systems” or nolds, version 0.3.2, which can be downloaded from (https://pypi.python.org/pypi/nolds).
419;nolds;10.3390/e20120962;We performed self-similarity analysis of epileptic EEG datasets by extracting their Hurst exponent through the standard rescaled range approach [38] ( function of Python’s nolds library)
420;nolds;10.3390/e20120962;We simulated fBm signals using the Cholesky decomposition method [37] ( function of Python’s nolds library) at a Hurst exponent of H = 0.75 and five scaling coefficients D = 0.001, 0.01, 1, 10, and 100, where the value of D = 1 leads to the original form of fBm
421;numpy;10.1038/s41598-017-13828-9;To use the code, one requires a Python 3 interpreter, as well as the numpy, scipy and pandas Python modules
422;numpy;10.1038/s41598-019-44272-6;We simulated one single admixture event from Southern Ural/West Siberian to both Indo-European and Hungarian Sekler populations occurring 30 generations ago, with same unknown amount of admixture [using numpy.random.random_sample] and then randomly drifted for 30 generations
423;numpy;10.1098/rstb.2013.0067;2.7.3 (http://www.python.org) with its numpy, scipy and pylab extension modules
424;numpy;10.1128/mBio.03221-19;The pipeline was written in Python3 with packages pandas, numpy, Biopython, pybedtools, pyBigWig, and pysam
425;numpy;10.1186/s12864-019-6436-0;0.22.0), numpy (v
426;numpy;10.1371/journal.pcbi.1007113;All statistical analyses and data visualization were performed using Jupyter Notebook [64] and Python 2.7, and the following packages: scipy.stats, numpy [65], pandas [66], matplotlib [67], mpl_toolkits, matplotlib_venn, seaborn, statsmodels.stats.multitest.fdrcorrection, mygene [68,69], sklearn.decomposition.PCA [59], sklearn.preprocessing.StandardScaler.
427;numpy;10.1371/journal.pgen.1009774;Bioinformatic analyses were done using Python 3.6 and the following packages: numpy 1.18.1, pandas 1.0.3 and matplotlib 3.1.3
428;numpy;10.1371/journal.pone.0206193;All scripts were coded in Matlab and Python using statistics libraries (numpy, pylab, scilab and matplotlib)
429;numpy;10.1371/journal.pone.0218966;Our algorithm is presented in Algorithm 1, which was developed in Python 3.6.1 with the packages pandas 0.20.3 and numpy 1.13.1.
430;numpy;10.7717/peerj.4925;AMPtk is written in Python and relies on several modules: edlib (Šošic & Šikic, 2017), biopython (Cock et al., 2009), biom-format (McDonald et al., 2012), pandas (McKinney, 2010), numpy (van der Walt, Colbert & Varoquaux, 2011), and matplotlib modules (Hunter, 2007)
431;onnx;10.1007/s42452-021-04588-3;After that, we used the onnx tool, onnx-TensorRT tool, to convert onnx to TensorRT
432;openrouteservice;10.3389/fped.2020.00395;"Due to the limit in travel data requests per time when using openrouteservice, we calculated travel time and distance from a random sample of about 28% of the random points of each country (Germany: 100,000 points; Ireland: 20,000 points; UK: 70,000 points) to the nearest provider for each type of pediatric service"
433;openrouteservice;10.3389/fped.2020.00395;Then, using openrouteservice, we calculated travel time and travel distance by car from each random point to the nearest provider of each service
434;optrees;10.1371/journal.pone.0188972;To establish the strain comparison order, we constructed a minimum spanning tree (MST) based on the donor region size and location with a custom R-script following the previously published pseudocode [19] supplemented with the R software package optrees [20] and igraph
435;osmdata;10.1038/s41598-021-82145-z;R package: osmdata
436;osmdata;10.3390/ijerph17062099;The “osmdata” queries containing keys and values for each mode of transport are listed in annex B
437;panda;10.1186/s13073-021-00904-z;"Other requirements: PLINK v1.9; R packages: shiny, shinyjs, DT, RSQLite, ggplot2, heatmaply, plotly, future; Python packages: panda, scipy, joblib, tqdm, sqlite"
438;panda;10.1371/journal.pone.0228422;1: Load the data using read_csv() function of panda library
439;panda;10.3389/fcvm.2021.619386;We used Python version 3.7 as the basic language of the whole model and call numpy, panda, sklearn, xgboost, and Matplotlib libraries to process and model the data
440;panda;10.3389/fnbeh.2021.709775;"The following data analysis and visualization was performed with R (R Core Team, 2020; version 3.4.2) and Python (version 3.8), using the panda package (version 1.2.3)."
441;panda;10.3389/fphys.2017.00354;Bio-analysis was performed under Ubuntu 16.04 LTS Docker image (Hung et al., 2016) within a custom Python (Python Software Foundation, 2015) scripts using the packages biopython, pyexcel, csv, collection, panda, and matplotlib and R v3.3.1 (R Core Team, 2016) scripts within the ggplot2 and plotrix libraries
442;pandas;10.1021/acsomega.1c00991;"All calculations were done in a Python environment using the numpy, pandas, and ase packages for basic data manipulation and structure file handling; plotting was done with matplotlib"
443;pandas;10.1038/s41598-019-44839-3;Data processing was performed using Python, and third party libraries such as NumPy, pandas, scikit-image and scikit-learn.
444;pandas;10.1038/s41598-020-72174-5;Additional packages used are pandas 0.23.4 for handling and analysis of data and PyBioMed-1.0 package for accessing the AAindex data
445;pandas;10.1099/acmi.0.000129;Analysis was performed in Python, utilizing the Biopython [7]⁠ and pandas [8]⁠ libraries.
446;pandas;10.1107/S2053273319011446;The pairing of acentric anomalous reflections was performed by an algorithm developed in the Python programming language with the libraries cctbx, pandas, NumPy and SciPy
447;pandas;10.1107/S2053273319011446;The pandas library was used to store and search among the large number of paired and unpaired reflection data obtained from the different crystals and data collection methods.
448;pandas;10.1107/S2059798317007707;The pandas software library (http://pandas.pydata.org/) was used for statistical analyses and the mmLib library for the superimpositions of dinucleotides (Painter & Merritt, 2004 ▸)
449;pandas;10.1186/1476-072X-13-11;Statistics were computed using the open-source Python statsmodels and pandas packages.
450;pandas;10.1186/s12859-015-0608-y;○ external dependences: python 2.7, numpy (> = 1.7), pandas
451;pandas;10.1186/s12864-019-5924-6;I used this binary table to calculate the Hamming distance [11] between all strains in Python using pandas and the scipy spatial packages.
452;pandas;10.1186/s12911-019-0805-0;All analyses were performed using Python version 2.7 and relevant open-source libraries: scikit-learn, scipy, pandas and numpy.
453;pandas;10.1186/s40478-018-0574-5;The network was generated using the python programming language (Python Software Foundation), and the networkx, numpy, and pandas python packages.
454;pandas;10.1371/journal.pone.0230416;The processing pipeline, including DAS classification, as well as the descriptive part below were all developed in Python [32], mainly relying on the following libraries or tools: scipy [33], scikit-learn [34], pandas [35], numpy [36], nltk [37], matplotlib [38], seaborn [39], gensim [40], beautifulsoup (https://www.crummy.com/software/BeautifulSoup), TextBlob (https://github.com/sloria/textblob) and pymongo (MongoDB, https://www.mongodb.com).
455;pandas;10.2196/13962;Dropout and step count data were preprocessed using Python (mainly the pandas and matplotlib packages) to generate graphs and format the data into a suitable format for R
456;pandas;10.3390/cancers13061315;Data preprocessing was done in python (version 3.8.3) using the numpy (version 1.18.5) and pandas (version 1.0.5) packages
457;pandas;10.3390/ma14164507;First, all data is converted to the form of DataFrame from pandas library [52]
458;paramtest;10.1186/s12874-019-0742-8;The simulations were all conducted in the R programming language using the simglm, paramtest and lme4 packages
459;paramtest;10.1186/s40359-021-00596-5;Minimum sample size for establishing medium-sized effects (Beta = 0,5) was estimated at n = 250 (logistic multiple regression, power ≥ 0.80 for all random and fixed effects, a skewed level 1 predictor, a standard normal level 2 predictor, an interaction, 10 simulations) with a shiny R web application and its R packages lme4, simglm and paramtest [29]
460;paramtest;10.3389/fpsyt.2018.00748;An a priori power analysis was applied through the lme4, simglm (57), and paramtest (58) R packages, on a model with two fixed dichotomic factors, and a random factor with σ2 = 0.50
461;patchwork;10.1038/s41598-020-78747-8;Figures were generated using the packages ggplot2, ggpubr, patchwork, plotROC, pROC, unikn and viridis
462;patchwork;10.1038/s42003-021-02407-4;All statistical analysis was conducted in the R Statistical Environment (v3.2.4,), using generalised linear models (GLM) and linear mixed-effects models (LMM) in the packages “lme4”, “car”, “MuMIn”, “multcomp”, “partR2”, “patchwork”, “ltm”, and “ggplot2”
463;patchwork;10.1111/eva.13237;All spatial data, including the environmental inputs and results from the models (see below), were visualized using the R packages “raster” (Hijmans, 2019), “rgdal” (Bivand et al., 2019), “rgeos” (Bivand & Rundel, 2020), and “ggplot2” (Wickham, 2016), and figures were produced using R packages “ggpubr” (Kassambara, 2019), “gridExtra” (Auguie, 2017), “patchwork” (Pedersen, 2020), and “ggrepel” (Slowikowski, 2020).
464;patchwork;10.1186/s13567-021-00981-3;All calculations and plot generation were performed in R (v 3.6.1) [28] using the packages: tidyverse (v 1.3.0) [29], helfeRlein (v 0.2.2) [30], plotly (v 4.9.2.1) [31], ggsignif (v 0.6.0) [32], ggplot2 (v 3.3.2) [33], grid (v 3.6.1) [28], gridExtra (v 2.3) [34], FactoMineR (v 2.3) [35], factoextra (v 1.0.7) [36], ggrepel (v 0.8.2) [37], and patchwork (v 1.0.1) [38].
465;patchwork;10.7554/eLife.60122;All data preparation, cleaning, analysis and plotting was done in R version 3.6.1 (R Development Core Team, 2019) using packages ggplot2 (Wickham, 2016), dplyr (Wickham et al., 2019), readxl (Wickham and Bryan, 2019a), patchwork (Pedersen, 2019), binom (Dorai-Raj, 2014), tidyr (Wickham and Henry, 2019b) and ggridges (Wilke, 2020)
466;pdftables;10.3389/fneur.2020.00729;The pdftables library in R is not the only package that can perform this task
467;phylip;10.1186/1471-2164-14-862;The program dollop in the phylip package [92] was used to reconstruct the ancestral presence and absence of gene families along all branches of the phylogeny.
468;phylip;10.1186/s12864-018-5300-y;ACES is file-format compatible with Saguaro [15], yet supports multiple matrix formats, including phylip [16] (for more details, see the documentation at https://grabherrgroup.github.io/ACES/)
469;phylip;10.1371/journal.pcbi.1000172;We show that our model generally improves inference power in both simulated and real data and that it is easily implemented in the framework of standard inference packages with little effect on computational efficiency (we extended dnaml , in Felsenstein's popular phylip package).
470;phylip;10.1371/journal.pcbi.1007731;Thus, the importance given to phylip, especially in the abstract, is not understandable.
471;phylip;10.7717/peerj.5089;We then exported new output files containing unlinked SNPS and converted these files from phylip to genpop format using a custom python script (Data S3)
472;powerGWASinteraction;10.1371/journal.pone.0083034;On the other hand, the power of our study can only achieve 0.0063 when using powerGWASinteraction [25].
473;psych;10.1111/bjep.12339;The package ‘psych’ was used to perform regressions (Revelle, 2017) and the package ‘VennDiagram’ for producing Venn diagrams (Chen, 2018)
474;psych;10.1186/s12998-018-0212-0;Data were exported to R [25] for the calculation of the reliability estimations (internal consistency) using the psych package [26]
475;psych;10.1186/s13068-015-0396-7;"The ade4, stat and psych packages of the R software (R version 2.13.2, http://www.r-project.org/) were used, respectively, for a principal component analysis (PCA; dudi.pca function), representation of the heatmap (heatmap function) and Pearson coefficient calculation (corr.test function)."
476;psych;10.1534/g3.117.300415;Correlations were calculated in R using the cor, cor.test, or corr.test (from the “psych” package) functions
477;psych;10.3389/fpsyt.2021.498614;Means, standard deviations, and Cronbach's alphas of each scale, and their correlations were analyzed using the psych package [ver
478;pvr;10.1111/jbi.12171;We used the pvr package in R (see http://cran.r-project.org/web/packages/PVR/index.html) for calculating the PSR curves for each quantitative species trait.
479;pyAFQ;10.1371/journal.pcbi.1009136;In addition to demonstrating the our analysis pipeline is robust to changes in tractometry software, the use of the updated pyAFQ capitalized upon the following improvements over the legacy Matlab version: (i) the ability to ingest data provided in the BIDS format [84], and (ii) the calculation of diffusion kurtosis imaging (DKI [43]) metrics We will refer to the mAFQ and pyAFQ pipeline collectively as AFQ.
480;pyAFQ;10.1371/journal.pcbi.1009136;The sofware integrates within a broader automated fiber quantification software ecosystem: AFQ [5] and pyAFQ [57], which extract tract profile data from raw and processed dMRI datasets, as well as AFQ-Browser, which visualizes tract profiles data and facilitates sharing of the results of dMRI studies [58]
481;pymagnitude;10.1371/journal.pone.0240376;Our analogies of Word2Vec embeddings were carried out on Google CoLab using the pymagnitude package and vector similarity functions (see Section 3.1)
482;pyphysio;10.1038/s41598-020-63596-2;The computation of synchrony was performed using a custom code based on pyphysio and physynch packages.
483;pyphysio;10.3390/brainsci11030336;Physiological signals of EDA and ECG have been recorded with a Biopac MP160 at a 2000 Hz sampling rate and processed with Python’s “pyphysio” library [41] for indexes extraction.
484;pyphysio;10.3390/bs10010011;For the analysis of the physiological signals and the computation of the physiological synchrony, we used custom scripts based on pyphysio [33,34], physynch [35], and PySiology [36].
485;pyphysio;10.3390/s20226616;Future work should also consider additional HRV components such as the low frequency: high frequency ratio or percentage of consecutive normal R-peaks differing by at least 50 milliseconds, which can be analyzed using Kubios HRV [21] or open-source physiological signal analysis packages such as pyphysio [33].
486;pyphysio;10.3390/s20236778;The processing pipeline was developed in Python, based on pyphysio [39], and it is publicly available at https://gitlab.com/abp-san-public/wearable-clinical-devices.
487;pyqi;10.7717/peerj.4395;Other requirements include GoogleAppEnginePipeline 1.9.22.1, pyqi 0.3.1, requests 2.10.0, requests-toolbelt 0.6.2, mailjet-rest 1.2.2, biom-format 1.1.2, ete3 3.0.0 (for tree generation–see below for details), webapp2 2.5.2, numpy 1.6.1, matplotlib 1.2.0, jinja2 2.6, ssl 2.7.11
488;pytorch;10.1038/s41598-019-52737-x;Training was performed on the NIH Biowulf cluster using 2-GPU nodes (2xNVIDIA K80, for a total of 4 logical GPUs with 12 GB each) with a batch size of 4 (pytorch nn.DataParallel)
489;pytorch;10.1186/s12864-018-5283-8;We have trained our models running on a single NVidia Quadro P6000 with stochastic gradient descent with momentum in pytorch
490;pytorch;10.15252/msb.202010179;Inference was performed in pytorch (preprint: Paszke et al, 2019) using the Adam optimizer (preprint: Kingma & Ba, 2017)
491;pytorch;10.3389/fncom.2021.626259;For comparison, we used the pre-trained ResNet-18 from pytorch 1.2.0 as the normal DCNN, referred to as the ResNet in this study for brevity.
492;pytorch;10.3389/frobt.2019.00022;Part of the appeal of using DNNs is the fact that many off-the-shelf frameworks for deep learning (Tensorflow, MXNet, pytorch) all support automatic differentiation
493;pytorch;10.3390/s19030667;The initialization of other layers was done in accordance with the method officially recommended in version 0.4 of pytorch
494;pytorch;10.3390/s20216264;Our experiments were performed on a computer with Intel Core I7 and NVIDA GeForce GTX 1080Ti, and we operated our algorithm by c++ and pytorch.
495;pytorch;10.3390/s21010229;We implemented MDPO on pytorch DNN framework, which is an actively developed open-source deep learning library
496;pytorch;10.3390/s21093106;Based on the pytorch deep learning frame, the image datasets were trained using the SSD algorithm to generate identification models
497;pytorch;10.3390/s21093265;As for Tan [11], Kokkinos [13], Cui [14], and our proposed method, all of them are implemented by the CNN approach, and thus we accelerated them using pytorch on CUDA.
498;pytplot;10.1007/s11214-018-0576-4;Under the PySPEDAS structure, “*_load_xxx” routines have been developed to access CDF files and import data to populate pytplot variables in a generic way
499;rBayesianOptimization;10.1016/j.ebiom.2021.103421;In stage one, the BayesianOptimization function of the rBayesianOptimization package was used with 10-fold cross-validation to select model hyperparameters (Supplementary Table 1) by minimizing the mean squared error (MSE)
500;rBayesianOptimization;10.1186/s13054-020-03199-5;Data analysis was performed in R version 3.3.2 using the stdReg version 3.0.0, SuperLearner version 2.0-24, rBayesianOptimization version 1.1.0, tmle version 1.3.0-2, and mice version 3.6.0 packages [36].
501;rBayesianOptimization;10.3389/fgene.2020.00025;The R package rBayesianOptimization (Yan, 2016) was used for UCB based BO using the minus of the validation MSE as score value since this package cannot perform minimization directly
502;rBayesianOptimization;10.3389/fnagi.2018.00144;Training and testing of the machine learning models specifically employed the packages caret (Kuhn, 2008), kernlab (Karatzoglou et al., 2004), and rBayesianOptimization (Yan, 2016).
503;rBayesianOptimization;10.3389/fonc.2020.00071;For neural networks, we applied the “neuralnet” package, and used the “rBayesianOptimization” package to optimize the number of nodes in the neural network hidden layer.
504;rClinicalCodes;10.23889/ijpds.v5i1.1362;The developers of the ClinicalCodes repository have created an open-source R package (rClinicalCodes) to automate the downloading and importing lists of clinical code and metadata through the research object file from the repository website: ( https://cran.r-project.org/web/packages/rClinicalCodes/index.html 
505;rLiDAR;10.1371/journal.pone.0213027;The rLiDAR package [49] in R [50]was used in this study for individual tree delineation and defoliation assessment on the UAS-derived CHM
506;readr;10.1037/rev0000169;Our software, analyses, and article were all created using the programming language R (R Core Team, 2017), and benefited in particular from the following open-source packages: bookdown, boot, checkmate, cocor, cowplot, dplyr, ggplot2, glue, gtools, hht, knitr, jsonlite, magrittr, margins, memoise, numbers, papaja, phonTools, plyr, purrr, Rdpack, readr, rmarkdown, testthat, tibble, tidyr, usethis, withr, and zeallot
507;readr;10.1038/s41598-019-57094-3;The R packages “MatchIt”, “survminer”, “readr”, “survival”, and “forestplot” with the appropriate libraries were used.
508;readr;10.1038/s41598-020-76683-1;All analyses and data visualizations were performed with the Software package R (Version 1.2.5033) using the following packages: ggpubr, ggplot2, cowplot, writexl, car, jtools, readr, broom, ggfortify, tidyr, scales, plyr, RColorBrewer, reshape2, tidyverse, grid, gridExtra, ggExtra, patchwork, apaTables, MBESS, egg, ggm, effectsize, ppcor, GGally, psychReport, lsr, ez, lattice, dplyr, rmarkdown, Rmisc, gghalves, BayesFactor
509;readr;10.1038/s41598-021-96403-7;For statistical analysis, the continuous data were extracted to a database on Excel Software (Version 16.42) before using RStudio software (Version 1.4.1106, RStudio, Inc) with the following packages: “meta”, “metafor”, “readr”, “Rcpp”, “BH” and “readxl” to perform the appropriate metanalytical procedures.
510;readr;10.1111/test.12258;According to the tidyverse website (https://www.tidyverse.org), “tidyverse is an opinionated collection of R packages designed for data science.” R packages included in the tidyverse collection include tidyverse, ggplot2, dplyr, tidyr, readr, purrr, and tibble
511;readr;10.1186/s12935-019-0731-3;Using the R (version 3.4.4) [44] statistical software and the readr package (version 1.1.1) [46], we parsed the metadata file to identify experiments where the cell lines had been treated with chemotherapeutic compounds (pert_type = “trt_cp”)
512;readr;10.1186/s13059-019-1783-3;Packages used to generate arcplots included viridis version 0.5.1, viridisLite version 0.3.0, igraph version 1.1.2, ggraph version 1.0.0, ggplot2 version 2.2.1, reshape2 version 1.4.3, dplyr version 0.7.4, tidyr version 0.7.2, and readr version 1.1.1.
513;readr;10.1371/journal.pone.0255433;Data was analysed and figures were developed by using RStudio Version 1.2.5033 (RStudio, Inc., Boston, MA, USA) with readr, plyr and ggplot2 packages [49–51]
514;readr;10.3934/publichealth.2021034;We used R Studio Version 1.2.5042 and libraries readxl, readr, gdata, mice, glment, caret for the computations.
515;readr;10.7717/peerj.4473; # load  required  libraries library(pcr) library(ggplot2) library(cowplot) library(dplyr) library(xtable) library(readr) 
516;relaimpo;10.1111/jvs.12367;From this model the individual contribution of each remaining environmental variable to the overall variation explained was calculated using the ‘lmg’ function of the ‘relaimpo’ package (Grömping 2006) using simple un‐weighted averages as recommended (see Appendix S1 for further detail on step‐wise regression predictor selection).
517;relaimpo;10.1371/journal.pone.0050239;The relative contribution of each environmental variable included in the models was assessed by calculating average semi-partial R2 using the “lmg” metric in the “relaimpo” package [33]
518;relaimpo;10.1371/journal.pone.0208923;The relative importance of the BCT measures within- and across modalities was tested using the relaimpo package in R [70], which assesses relative importance of regressors in the linear model
519;relaimpo;10.3389/fneur.2018.00526;Coefficients of determination were calculated with the “relaimpo” package
520;relaimpo;10.3390/ijerph17155592;For the regression models, the regression predictors were then assessed for relative importance via assigning of weights using the relaimpo package [34]
521;requests;10.1016/j.dib.2021.107360;We used the json and requests packages [35,36]to collect data and the scikit-learn package [37] to impute missing values
522;requests;10.1186/1471-2105-15-44;The HTS barcode checker is written in python and uses the biopython[10], beautiful-soup[11] and requests[12] packages to handle FASTA sequences and communicate with the various APIs and web services used, such as NCBI GenBank and the PhyloTastic TNRS service
523;requests;10.1186/s12859-018-2367-z;Other requirements: At least Python 3.4, the packages numpy, requests and msgpack must be installed
524;requests;10.1186/s12911-020-1046-y;Project name: Prikbord Project home page:http://prikbord.science.ru.nl/Operating system: Linux Programming language: Python, javascript Other requirements: Django 1.5.11 or higher, MongoDB 2.6.10, pymongo 2.7.2 or higher, requests 2.13.0 or higher License: GNU GPL Any restrictions to use by non-academics: licence needed
525;requests;10.3389/fvets.2021.674730;Using requests and BeautifulSoup packages in Python, all the PDF files with the titles containing “MRK” by avoiding cases sensitivity of uppercase or lowercase for each letter (e.g., “mRK,” “MrK,” “mrk,” etc.) from the years 2018, 2019, and 2020 were automatically collected and saved in a separate folder for further steps
526;requests;10.3390/ijerph17155596;For data collection, Python Beautiful Soap and requests libraries have been used to automatically collect weather information from the Italian website ”IlMeteo.it” (https://www.ilmeteo.it/portale/archivio-meteo/) and to store the data in a single CSV file
527;requests;10.7717/peerj.4395;Other requirements include GoogleAppEnginePipeline 1.9.22.1, pyqi 0.3.1, requests 2.10.0, requests-toolbelt 0.6.2, mailjet-rest 1.2.2, biom-format 1.1.2, ete3 3.0.0 (for tree generation–see below for details), webapp2 2.5.2, numpy 1.6.1, matplotlib 1.2.0, jinja2 2.6, ssl 2.7.11
528;rglobi;10.1371/journal.pone.0227810;HelminthR [23] and rglobi (global biotic interactions) [24] curated databases for host-parasite interactions were used to verify zooarchaeological data as potential hosts for the identified parasites
529;rglobi;10.1371/journal.pone.0227810;HelminthR and rglobi search results are available in S2 & S3 Data
530;rglobi;10.3390/biom11081245;Data can also be accessed programmatically through a REST API, as well as through R (rglobi) and JavaScript (eol-globi-data-js) libraries or SPARQL and Cypher queries
531;rmarkdown;10.1038/s41398-021-01596-0;All data analysis was performed in RStudio [17] using packages base, stats, exact2x2, tidyverse, rlang, ggsignif, ggrepel, pheatmap, UpSetR, cowplot, egg, and rmarkdown.
532;rmarkdown;10.1038/s41598-019-42713-w;1.20 (refs) and rmarkdown v
533;rmarkdown;10.1186/s40359-021-00534-5;"All analyses were performed using the R language with packages ggplot2 and rmarkdown; using the SPSS software version 22;"
534;rmarkdown;10.12688/f1000research.22259.1;"This article and the analyses were performed with R ( R Core Team, 2019) using the rmarkdown ( Allaire ), and knitr ( Xie, 2019; Xie, 2015) packages."
535;rmarkdown;10.1371/journal.pone.0212390;There are also templates for optional style customization files, style.css and styles.docx, which rmarkdown can utilize to style rendered .html and .docx files, respectively [10]
536;rmarkdown;10.1371/journal.pone.0233309;The simulations and power calculation were carried out using RStudio version 1.1.463 and the ggplot2, scales, knitr, rmarkdown and pwr packages [18–23].
537;rmarkdown;10.1371/journal.pone.0243295;"1.28; [75–77]) and rmarkdown (v"
538;rmarkdown;10.3389/fmolb.2021.635074;The final, user-friendly HTML reports were produced using R markdown packages rmarkdown 2.6, knitr 1.30 and kableExtra 1.3.1.
539;rmarkdown;10.5334/joc.161;All analyses used R version 3.6.2 (R Core Team, 2019) with the following add-on packages: tidyverse 1.3.0 (Wickham et al., 2019) for data wrangling and visualisation, ordinal 2019.12.10 (Christensen, 2019) for fitting cumulative link mixed models, emmeans 1.4.5 (Lenth, 2020) for follow-up analyses and equivalence tests, and rmarkdown 2.1 (Allaire et al., 2020) for compiling the analysis script
540;rmarkdown;10.7717/peerj.11757;"The data for all gene regions were analysed in R using the vegan package (R version 3.4.4; Oksanen et al., 2019) and rmarkdown (Xie, Allaire & Grolemund, 2018) to estimate the Sørensen index (Sørensen, 1948; based on presence-absence) and Euclidean distance (based on read abundance equalized per sample) to quantify the compositional dissimilarity among samples"
541;scCATCH;10.1007/s13238-020-00727-5;The common practice is to identify cell types with the known cell markers based on the pre-computed cell clusters such as scCATCH (Shao et al., 2020), or to mark cells by comparing the similarity of single-cell expression profiles with reference database using SingleR Aran et al
542;scCATCH;10.1038/s42003-021-02146-6;Discrepancies appeared mainly in the annotation of subcell types in T cells due in large part to the low resolution of the T-cell subpopulations in the reference databases in scCATCH
543;scCATCH;10.1038/s42003-021-02146-6;We then ranked them according to their log(FC) and used the top 40 genes (Supplementary Data 6) as an input to scCATCH to annotate these clusters (Some clusters had <40 marker genes based on the above criteria)
544;scCATCH;10.1186/s13046-021-01955-1;[59] developed an automatic annotation toolkit, denoted ‘scCATCH’, based on clustering, which can accurately annotate cell types with acceptable repeatability.
545;scCATCH;10.3390/biomedicines9040368;Commonly, prior knowledge of established cell type-specific markers (scCATCH [121]) or reference databases of bulk or sc/snRNA-seq profiles (CHETAH [122], scHCL [123], scMap [124], SingleR [125]) are used to annotate each cell type
546;scikit-learn;10.1109/TBME.2018.2890167;Note that the SVM-based classifier we utilized is based on an open-source tool called scikit-learn (http://scikit-learn.org), with default settings.
547;scikit-learn;10.1186/s12885-019-5646-9;We used the python module scikit-learn to perform all the above modelling process using default parameters
548;scikit-learn;10.1186/s12920-017-0264-3;We used Python implementation of random forest from scikit-learn version 0.14.1
549;scikit-learn;10.1186/s13059-021-02480-2;To build the machine learning models, we use the scikit-learn (version: 0.23.2) package
550;scikit-learn;10.1186/s13148-021-01029-1;Box plots of the cohorts by BCI values and the receiver operating characteristic (ROC) curve for the BCI classification (Fig. 2) were generated in python 3.7.2 using numpy 1.16.2 and pandas 0.24.2 packages for data processing, custom code to generate the true-positive rates and false-positive rates for the ROC curve sliding threshold, scikit-learn 0.20.3 to calculate the ROC area under the curve (AUC), and seaborn 0.9.0 with matplotlib 3.0.2 for visualization.
551;scikit-learn;10.1371/journal.pone.0236962;Using the python scikit-learn library v0.20.2, we trained Random Forest [28] and Logistic Regression [29] machine learning algorithms on the VarTrain missense variants, which includes 10,070 deleterious variants as the positive set and 10,070 benign variants as the negative set
552;scikit-learn;10.3389/fgene.2019.01292;Text-mining and SVM computing were processed using the python scikit-learn library (Pedregosa et al., 2011)
553;scikit-learn;10.3389/fgene.2021.648329;The optimal number of clusters was determined based on two metrics: Silhouette index (Rousseeuw, 1987) and Calinski–Harabasz index (Calinski and Harabasz, 1974), using scikit-learn package (Pedregosa et al., 2011).
554;scikit-learn;10.3390/jpm11080748;All machine learning algorithms were implemented using scikit-learn package, version 0.24.1 in Python 3.8.5
555;scikit-learn;10.7554/eLife.61277;We used scikit-learn (Pedregosa, 2011) PCA (sklearn.decomposition.PCA) to identify the dominant axes of gene expression variation across the entire AHBA dataset, as well as for brain-specific genes
556;sets;10.1186/s12859-018-2557-8;The NIPTeR package requires R 3.1.0 or higher, the stats and sets packages as available on CRAN, and the RSamtools and S4Vectors Bioconductor packages.
557;sfsmisc;10.1002/ece3.4406;The f.robtest function in the sfsmisc package was used to compute robust F‐test and get p‐values (Maechler, 2017)
558;sfsmisc;10.1002/ece3.7054;"All the analyses were performed in the R programming software; version 3.4.4 (R Development Core Team, 2015) using the following packages: vegan 2.5.4 for computing species richness, diversity indices, taxonomic rarefaction curves, and environmental variables PCA scores (Oksanen et al., 2007); evolqg 0.2.6 for functional rarefaction curves (Melo et al., 2015); FD 1.0.12 for CWM analysis (Laliberté et al., 2014); sfsmisc 1.1‐3 for the trait kernel density analysis (Maechler et al., 2019); and cati 0.99.2 (Ta"
559;sfsmisc;10.1371/journal.pone.0063274;We performed all statistical analyses with the statistical software R version 2.14 using the car [54], lme4 [55], plspm [56], sfsmisc [57], and vegan [51] packages
560;sfsmisc;10.3389/fnbeh.2019.00120;To test for associations between peer victimization and depression symptoms and brain and behavioral responses, robust linear regressions were fit using an M estimator from the MASS package (Venables and Ripley, 2002), and a robust f-test (Wald test) computed using the sfsmisc package (Maechler, 2018)
561;sfsmisc;10.7554/eLife.49458;For each of the interaction effect clusters within the left medial temporal lobe for both PMN-targeted and PFC-targeted stimulation conditions, a robust linear model was built regressing the interaction effect for that cluster (retrieval task stimulation effect minus resting-state stimulation effect) and tSNR onto the stimulation effect on memory performance (post-stimulation minus post-sham) (R packages MASS and sfsmisc, RStudio 1.1.453).
562;shiny;10.1002/cam4.3350;The “rms,” “survival,” “shiny,” “foreign,” “nricens,” and “time‐ROC” packages were used
563;shiny;10.1002/ece3.5412;(Currently, this server is http://shinyapps.io via a subscription service paid by me.) Alternatively, for users with R, shiny, and learnPopGen installed locally, it is also possible to control the same functions via a web browser (but not over the Internet) by simply downloading the source code of the web applications from their GitHub page (http://github.com/liamrevell/PopGen.apps), and then executing the application source within Rstudio (Rstudio Team, 2015) or by using the shiny function runApp
564;shiny;10.1038/s41598-020-76603-3;To run the app, several freely available packages are required: shiny, ggplot2, magrittr, dplyr, ggrepel, shinycssloaders, DT, RCurl and readxl
565;shiny;10.1093/gigascience/gix119;Other requirements: tested with R packages shiny (1.0.5), shinythemes (1.1.1), shinyBS (0.61), RCurl (1.95.4.8), XML (3.98.1.9), stringr (1.2.0), plyr (1.8.4)
566;shiny;10.1186/s12885-020-06756-x;Interactive application was developed using R, a language and environment for statistical computing and the package “shiny” (R Foundation for Statistical Computing, Vienna, Austria).
567;shiny;10.1186/s12943-021-01405-8;The R shiny app was developed in R using the shiny (version 1.5.0) and visNetwork (version 2.0.9) packages.
568;shiny;10.1186/s13071-020-04114-1;In order to map the specific diversity of the Obsoletus Group, the R software version 3.6.0 was used with the Leaflet version 2.0.2 and shiny packages version 1.4.0.
569;shiny;10.12688/f1000research.4680.2;Our solution is shinyMethyl, an interactive visualization package for 450k arrays, based on the packages minfi and shiny 
570;shiny;10.3389/fnhum.2020.584236;Additionally, we used the “shiny” and “DynNom” packages in R software to generate a web-based calculator (using training queues) to dynamically predict the WMD risk.
571;shiny;10.3389/fped.2020.623184;Statistical analyses, graphs, design of the research and the online tool were performed in R software 3.6.122 with the following R packages: effsize (26), ggplot2 (27), ggsci (28), ggpubr (29), ComplexHeatmap (30), and shiny (31)
572;sklearn;10.1002/advs.201903392;SHAP was implemented using the shap package in Python 3.7.[ ] Regression models were generated using random forest and XGBoost, using the packages sklearn and xgboost in Python 3.7
573;sklearn;10.1021/acsomega.1c00991;Machine learning procedures were used/implemented with sklearn, tensorflow, and (for the GNN) pytorch-geometric, while the fingerprints were generated with Dscribe(54) and our own implementation of the PDDF
574;sklearn;10.1107/S2059798318005752;The coefficient and intercept were determined by the ‘LogisticRegression’ module in sklearn (http://www.scikit-learn.org)
575;sklearn;10.1111/acel.13320;The age prediction models use an elastic net regression as implemented by Pythons’ sklearn
576;sklearn;10.1186/s13195-021-00874-9;Statistical analyses were performed using Python (version 3.7.3) and the sklearn package (version 0.21.3)
577;sklearn;10.18632/aging.203447;Analyses were performed using the sklearn (version 0.20.3) and lifelines (version 0.19.5) packages in python.
578;sklearn Python;10.1186/s12859-020-03582-7;Next, a random forest model is trained (using the sklearn Python package) to classify compounds as to their bioactivity against the protein of interest
579;sklearn Python package;10.1016/j.cub.2020.04.042;The network was trained using the backpropagation algorithm and weights were optimized using a stochastic gradient-based solver with adaptive momentum estimation via the sklearn Python package [78].
580;sklearn Python package;10.1016/j.patter.2021.100270;The Equations 1.5 and 1.6 were estimated using the Linear model function implemented in the sklearn Python package, using default parameters
581;sklearn Python package;10.1128/mSystems.00164-16;The multidimensional scaling plots were computed with the sklearn Python package (83).
582;sklearn Python package;10.7717/peerj.10501;The feature vector values were scaled from 0.0 to 1.0 using the minmaxscaler in the sklearn Python package (Pedregosa et al., 2011)
583;sklearn python package;10.3390/s21041463;The tuned hyper-parameters were: The training set was split in train and validation using the train_test_split function from the sklearn python package, using a 80-20% split, ensuring stratification on the predicted class
584;sklearn python package;10.7554/eLife.52611;Regression coefficients were calculated using the sklearn.linear_model.Ridge function from the sklearn python package (Pedregosa et al., 2011) where X is 111 x d (and d is the number of transcripts) and y is the vector for RRTS values for each transcript
585;smnet;10.1002/env.2340;"As such, smnet allows the user to fit spatial additive models based on P?splines, which may be particularly useful for capturing non?stationary and non?separable spatio?temporal effects; characteristics that we expect to be common in streams data (Peterson et al., 2013)"
586;smnet;10.1002/env.2340;Until recently, fitting these types of stream network models would have required a great deal of effort and technical expertise, but the SSN and smnet packages make these methods accessible to modellers from a wide variety of disciplines
587;snpReady;10.1371/journal.pone.0224631;Population parameters of DH lines derived from F1/F2 generations were estimated for each SNP by group (generations) and subgroup (source germplasm) through the popgen function of the snpReady package of R software 3.5.0 (R Development Core Team, 2018), namely:
588;snpReady;10.1371/journal.pone.0249825;"""Nei’s genetic diversity index and Nei’s genetic distance were calculated in the R packages """"snpReady"""" and """" StAMPP """", respectively [38, 39]."""
589;snpReady;10.3389/fpls.2019.01353;After the data were filtered, the missing data were imputed by the knni method with snpReady software (Granato and Fritsche-Neto, 2018).
590;snpReady;10.3389/fpls.2021.658267;To deal with genotyping data, we developed the package snpReady (Granato et al., 2018b), which helps the user with quality control and the recoding of markers
591;snpReady;10.3390/plants10030509;Based on the K4 structure model for the whole dataset, the following parameters were calculated for each group: GD (Nei’s genetic diversity), PIC (polymorphic information index), Ho (observed heterozygosity), He (expected heterozygosity), Fi (inbreeding coefficient), pairwise Nei’s standard genetic distance, and pairwise Fst analysis, using snpReady package in R [63].
592;spBayesSurv;10.1371/journal.pone.0229336;We implement our proposed method from an R package spBayesSurv provided by Zhou et.al
593;spacesXYZ;10.1186/s12862-021-01883-w;The DeltaE function in the ‘spacesXYZ’ package [79] was used to calculate ∆E (CIEDE2000).
594;spektral;10.1186/s13059-020-02214-w;To construct GCNG, we used the python packages of “spektral,” “Keras,” and “Tensorflow.” See Fig. 1a for the architecture of GCNG
595;spektral;10.3389/fgene.2021.733906;We refer to scGAE to build a graph autoencoder that is based on TensorFlow 2.4.1 and Python package spektral 0.6.1
596;sst;10.1371/journal.pone.0129674;Due to the difficulty of obtaining a local copy for the programs palsse, stick, xtlsstr, kaksi and sst we have tested them on the set ?? of 100 selected x-ray structures by uploading each one to a web server [23] to obtain their assignments (Table 4)
597;sst;10.1371/journal.pone.0129674;To compare our algorithm with the five programs (kaksi, plasse, stick, stlsstr and sst) that we are not able to obtain a local copy we have uploaded to a web server [23] a set of 100 x-ray protein structures (set ??) with the first 50 structures having resolutions between 1.0Å–2.0Å and the rest having resolutions ? 2.5Å(1AKG,1BGF,1EZW,1GSU,1I1N,1K1B,1NTE,1O98,1PVM,1SJD,1UKF,1VZY,1XGW,1YQD,2ASC, 2BJI, 2CWH,2FD5,2GB2, 2GG6,2H1V,2I2C,2NSF, 2POK,2VBA,2W6K,2WRA,2X7H,2YSK,2ZJ3,3C9U,3EDF, 3GG7,3HG7,3IDV,3LCC,3LFJ,3
598;star;10.1099/mgen.0.000319;grubii H99 (CNA3, GCA_000149245.3) [22] or Bt85 (Cryp_neof_Bt85_V1, GCA_002215835.1) [16] genome using star (v 2.5.3a) [23] for RNA-Seq mapping, following ‘Alternate Protocol 7’ for star alignment with transcript coordinate output and with rsem (v1.2.31) quantification [24]
599;star;10.1111/acel.12739;Annotated genomes were retrieved from Ensembl, and RNA‐seq data were mapped and quantified using star v.2.5.2b (Dobin et al., 2013).
600;star;10.1111/acel.12739;Illumina sequencing reads were mapped to the annotated Mlig_3_7 genome assembly (Wudarski et al., 2017) using star v.2.5.2b (Dobin et al., 2013) in the transcriptome quantification mode with following parameters “–outFilterMultimapNmax 30 –outFilterMismatchNmax 5 –quantMode TranscriptomeSAM.” To generate read counts for transcript clusters, the resulting bam files were processed by Corset v.1.06 (Davidson & Oshlack, 2014)
601;star;10.1186/s12885-017-3237-1;Raw fastq files were obtained for each sample and aligned to the Homo sapiens reference using star [12]
602;star;10.1186/s12920-021-01022-w;After quality control and trimming, the reads were mapped to the human genome (ENSEMBL-grch38release 91) using the star version 2.5.2a aligner and gene abundance was estimated with python version 2.7.11, and htseq version 0.11.0
603;star;10.1186/s13015-016-0068-6;Suffix arrays are used in such programs as segemehl [3], last [4], mummer [5], reputer [6], star [7], and as an initial stage in recent versions of gsnap [8], which also employs hash tables for more complex alignments.
604;stars;10.1186/s12915-021-01043-y;Segmentation maps were converted to polygons in R using packages sf [9] and stars [24]
605;stars;10.1371/journal.pone.0247535;The foot package is coded entirely in the R statistical computing language, making use of the sf, stars, and lwgeom packages [27–29] to access external spatial data libraries.
606;stars;10.1371/journal.pone.0253148;All statistical analyses were performed using R [39], and in addition to the packages mentioned previously, we used ggspatial [52], ggplot2 [53], stars [54], tidyverse [55], viridis [56].
607;stars;10.1371/journal.pone.0258031;We used publicly available packages for the statistical software R (version 4.0.0) [62] and packages tidyverse, PropCIs, ggmap, sf, stars, raster, akima, and adehabitatHR to conduct our GIS and statistical analyses
608;stars;10.3390/e22090937;Input data can be either a matrix, array, object from the raster package (RasterLayer, RasterStack, RasterBrick), or from the stars package [43,44]
609;stdReg;10.1007/s10654-018-0375-y;The aim of this paper is to show how one single R-package, stdReg [6], can be used to estimate a wide range of causal effect measures, including all those mentioned above
610;stdReg;10.1007/s10654-018-0375-y;The stdReg package also contains a function for standardization with shared frailty gamma-Weibull models, stdParfrailty, which is described by Dahlqwist et al
611;stdReg;10.1038/s41380-018-0248-5;All analyses were performed in R, adjusting the precision of estimates for dependencies within family clusters using cluster-robust sandwich estimators, employing the packages drgee [40] and stdReg [37].
612;stdReg;10.1136/jclinpath-2019-206300;The analysis was undertaken using R, V.3.6.0 (R Foundation for Statistical Computing) and the R packages survival for HR, stdReg for standardised survival and metafor for the random-effect model for combining HRs
613;stdReg;10.1186/s13054-020-03199-5;Data analysis was performed in R version 3.3.2 using the stdReg version 3.0.0, SuperLearner version 2.0-24, rBayesianOptimization version 1.1.0, tmle version 1.3.0-2, and mice version 3.6.0 packages [36].
614;strand;10.1002/sim.8065;Only the data are required, a choice of gates and an indication of the amount of data to be apportioned as pilot data, to implement the essential form of the STRAND Chart using the R package “strand” (available from the author).
615;stringr;10.1038/s41598-017-09303-0;0.1–20), and stringr (v
616;stringr;10.1093/bioinformatics/bty088;It also depends on several R packages: Biostrings, dplyr, parallel, reardr, tibble, stringr and biomartr package (Drost and Paszkowski, 2017) for automated retrieval of proteomes
617;stringr;10.1093/eurheartj/ehaa756;In addition, stringr version 1.4.0 and fst 0.9.0 packages were used for data management
618;stringr;10.1128/mBio.01557-20;AutoTax combines several software tools (GNU parallel v.20161222 [48], USEARCH v.11.0.667 [49], SINA v.1.6.0 [50], and R v.3.5.0 with the following packages: biostrings [51], doParallel [52], stringr [53], data.table [54], tidyr [55], and dplyr [56]) into a single BASH script that otherwise requires only a single FASTA file with the user-provided full-length 16S rRNA gene sequences and the FASTA-formatted SILVA_138_SSURef_NR99_tax_silva reference database as input
619;stringr;10.2196/27163;"The following statistical packages were used: (1) dplyr [36] and tidyr [37] for data preparation; (2) lubridate [38] and stringr [39] for handling time and date; and (3) ggplot2 [40], patchwork [41], and scales [42] for plotting."
620;stringr;10.3389/fphar.2020.563751;The main libraries and associated dependencies used were 1) readr, dplyr, tidyverse, stringr, and splitstackshape for data cleaning and analysis, 2) raster and sp for graphs, and 3) maptools, geosphere, GADMTools, and geoname for maps.
621;stringr;10.3389/fpls.2017.00623;For data manipulation and visualization: tidyr (Wickham, 2016c), dplyr (Wickham and Francois, 2016), ggplot2 (Wickham, 2009), modelr (Wickham, 2016a) and stringr (Wickham, 2016b) [these and other helpful packages are part of the tidyverse (Wickham, 2016d)].
622;stringr;10.3390/antiox10060864;Data visualization and statistical analysis were performed with Microsoft Excel and R [13] (v4.0.2, packages plyr, stringr, readxl, ggplot2, dendsort, pheatmap, cellWise, Metaboanalyst R) [14,15]
623;stringr;10.3390/molecules25204747;This analysis used R-base packages and the specialized R packages bibliometrix [39] and stringr [40].
624;stringr;10.3390/v12010107;Data cleaning, stratification, analysis, and visualization was performed using R packages [33] (dplyr [34], ggplot2 [35], reshape2 [36], and stringr [37])
625;summarytools;10.1186/s41155-021-00194-9;All the analysis was conducted in the R software (R Core Team, 2021) using the packages summarytools (Comtois, 2021), polycor (Fox, 2019), GGally (Schloerke et al., 2021), CTT (Willse, 2018), lavaan (Rosseel, 2012), nFactors (Raiche & Magis, 2020), psych (Revelle, 2020)
626;summarytools;10.1371/journal.pone.0228262;The analyses were carried out using the packages summarytools and MASS (function polr) in R.
627;summarytools;10.1371/journal.pone.0239639;Various R packages [76] run in R-studio v.1.2.5033 were used for statistical analyses and plot construction: summarytools library [77] was used for obtaining basal descriptive statistics, ggplot2 library [78] was used to analyze Pearson correlation and plot the result as well as for creating boxplots and histograms, posthoc.kruskal.dunn.test function in the PMCMR library [79] was used for the posthoc Dunn’s test, the FSA library [80] was used to conduct the Mann-Whitney U test, the prcomp function [76] was 
628;summarytools;10.2196/25124;R 3.2.2 (R Development Core Team) software [23] was used on every statistical step of this work: discretization of continuous variables (package car [24]), descriptive and comparative analyses (packages gmodels [25] and epitools [26]), missing data analysis (package summarytools [27]), missing data imputation (package DMwR [28]), hierarchical clustering (package stats [23]), Bayesian network inference (packages bnlearn [29] and gRain [30]), and ROC curve analysis (package pROC [31])
629;summarytools;10.3390/jpm11060512;R-package ‘dplyr’ was used for data manipulation [28], and R-package ‘summarytools’ was used for frequencies tables, cross-tabulation, and other descriptive statistics [29]
630;svUnit;10.1186/s13040-017-0151-7;RUnit and svUnit are R implementions of the widely used JUnit testing framework [56] and contain a set of functions that check calculations and error situations (Table 4)
631;svUnit;10.1186/s13040-017-0151-7;testthat [53], RUnit [54] and svUnit [55]) that enable researchers to create and execute unit tests
632;svUnit;10.7717/peerj-cs.175;Metric 2: Check for stated dependencies on one of the following testing packages: RUnit (Burger, Juenemann & Koenig, 2015), svUnit (Grosjean, 2014), testit (Xie, 2018), testthat (Wickham, 2011), unitizer (Gaslam, 2017), or unittest (Lentin & Hennessey, 2017)
633;swirl;10.1111/test.12258;The swirl R package aims to teach R programming and data science interactively and directly from within the R console
634;swirlify;10.1111/test.12258;There's another R package hosted on GitHub—swirlify—that helps instructors create content by providing tools for writing and sharing swirl courses
635;tandem;10.1128/genomeA.01271-17;The obtained data were assembled de novo using the Hierarchical Genome Assembly Process (HGAP version 3.0) and annotated with Prodigal version 2.50 (2), tandem repeats finder software (3), tRNAscan-SE (4), and RNAmmer (5)
636;tandem;10.1186/s12864-015-2324-4;The identification of tandem repeat sequences was performed with the software tandem repeats finder [34] using the whole-genome sequence of all W
637;tandem;10.1186/s12864-021-07626-x;Furthermore, tandem repeat Finder version 4.07 [88], with default settings, was used to calculate tandem repeats in these cp genomes.
638;tensorflow;10.1001/jamanetworkopen.2021.6096;Pydicom version 1.4.2, tensorflow, and opencv version 4.1.0 were used for image processing.
639;tensorflow;10.1002/advs.202004958;The Keras version 2.3, a highly useful neural networks API, and the tensorflow‐gpu 1.15 version were adopted for a rapid parallel computing.
640;tensorflow;10.1002/mp.14415;For all training and inference tasks, tensorflow and a DGX workstation with a 16‐GB NVIDIA V100 graphics processing unit was used.
641;tensorflow;10.1021/acsomega.1c00991;Machine learning procedures were used/implemented with sklearn, tensorflow, and (for the GNN) pytorch-geometric, while the fingerprints were generated with Dscribe(54) and our own implementation of the PDDF
642;tensorflow;10.1186/s12911-019-0991-9;Data were analysed in R version 3.4.4 using the packages GLMnet, e1071, randomforest, pROC, ROCR, ggplot, and the neural network was run in python 2.7.12 with tensorflow 1.10.1 (Additional file 3)
643;tensorflow;10.1186/s40537-020-00387-6;With tensorflow version 1.14.0 and python version 3.7.4
644;tensorflow;10.12688/f1000research.51117.1;"Most commonly, there was a description of machine-learning toolkits (Mallet, N = 12; Weka, N = 6; tensorflow, N = 5; scikit-learn, N = 3)"
645;tensorflow;10.1371/journal.pcbi.1008918;(iii) The packages keras and tensorflow were applied to develop deep learning-based regression model.
646;tensorflow;10.3390/ijms21186652;R (v.4.0.1) with RStudio (v.1.3.959, RStudio, Boston, MA, USA) and the R-packages Keras (v.2.3.0.0), generics (v.0.0.2) reticulate (v.1.16-9000), tfruns (v.1.4), magrittr (v.1.5), zeallot (v.0.1.0), R6 (v.2.4.1), tensorflow (v.2.2.0), config (v.0.3), jsonlite (v.1.6.1), processx (v.3.4.2), yaml (v.2.2.1), rstudioapi (v.0.11), caret (v.6.0-86), and e1071 (v.1.7-3)
647;tensorflow;10.3390/s21051734;As regards the choice of the DL framework, there are numerous open-source frameworks [98,99], such as keras [100], tensorflow [101], and pytorch [102]
648;tidyr;10.1007/s00426-019-01206-1;Specifically, we used the packages dplyr (Wickham & Francois, 2016), tidyr, (Wickham, 2018), ggplot2 (Wickham, 2009), trimr (Grange, 2018a), afex (Singmann, Bolker, Westfall, & Aust, 2018), and brms (Bürkner, 2017).
649;tidyr;10.1038/s41598-020-59494-2;"All data processing and analysis was conducted in R (version 3.5.2) extended with the following packages: daymetr to query Daymet data; FedData to access NED and SSURGO databases; sp and raster to process spatial data and terrain analysis; dplyr and tidyr for all other data wrangling; lme4 for linear-mixed effects model fitting; emmeans for post-hoc mean comparison; nlme for non-linear model fitting; and rasterVis and ggplot2 for data visualization."
650;tidyr;10.1111/joa.13366;"Statistical analyses were performed in R (Version 3.6.1; www.r‐project.org/) and R‐Studio (Version 1.2.1335; www.rstudio.com/) using the following packages and versions: performance_0.4.4, boot_1.3‐22, plyr_1.8.4, readxl_1.3.1, openxlsx_4.1.0.1, forcats_0.4.0, MuMIn_1.43.6, ggthemes_4.2.0, car_3.0‐3, carData_3.0‐2, MASS_7.3‐51.4, lme4_1.1‐21, Matrix_1.2‐17, stringr_1.4.0, dplyr_0.8.3, purrr_0.3.2, readr_1.3.1, tidyr_0.8.3, tibble_2.1.3, ggplot2_3.2.0, and NCmisc_1.1.6."
651;tidyr;10.3389/fphys.2021.698463;All statistical analyses were conducted with RStudio version 1.3.1093 (RStudio, Inc.) and “readxls,” “tidyr,” “dplyr,” “ggridges,” “plyr,” “ggplot2,” “scales,” “viridis,” “pipeR,” “effsize,” “bootES,” “officer,” and “rvg” packages
652;tidyr;10.3389/fvets.2020.547181;Further data analysis was done using RStudio (version 1.2.5033, 2019) with the following packages: phyloseq, vegan, dplyr, scales, grid, reshape2, igraph, ape, gplots, lme4, phangorn, plotly, tidyr, data.table, Maaslin2, ggplot2, stringr, and devtools.
653;tidyr;10.3390/metabo11040211;Some packages are recommended to be pre-installed in R before gcProfileMakeR runs: readxl, plyr, stringr, dplyr, tidyr, ggplot2 and egg.
654;tidyr;10.3390/microorganisms9010002;For the MIC plots and mutations clustering analysis, R version 3.4.2 [23] was used with the following packages: tidyr_0.8.3 [24], data.table_1.12.0 [25], cowplot_0.9.4 [26], ggplot2_3.1.0 [27], pheatmap_1.0.12 [28], RColorBrewer_1.1-2 [29], gplots_3.0.1 [30] and ggtree_1.15.6 [31,32]
655;tidyr;10.3390/nu13061898;The analytical packages tidyr, dplyr, and pROC [49] were used along with the graphic package ggplot2 [50]
656;tidyr;10.7150/jca.55970;Plotted statistical values (paired-boxplot, correlation matrix diagram, Kaplan-Meier survival analysis, receiver operating characteristics, Calibration, Heatmap, violin plot, boxplot and scatter plot) were applied using R with packages including the following: ggplot2, cowplot, GGally, survival, dplyr, tidyr, survminer, dplyr, pheatmap, cowplot, timeROC, tibble, survivalROC, tidyverse, rms, ggpubr, ggstatsplot
657;tidyr;10.7554/eLife.70086;"During data manipulation, we also made use of R packages: dplyr v.1.0.2 (Wickham et al., 2020) and tidyr v.1.1.2 (Wickham and Henry, 2019); for data visualisation we used cowplot v.1.1.0 (Wilke, 2019), ggplot2 v.3.3.2 (Wickham, 2016), ggpubr v.0.4.0 (Kassambara, 2018), ggtext v.0.1.1 (Wilke, 2020), glue v.1.4.2 (Hester, 2020), maps v.3.3.0 (Becker and Wilks, 2018), scico v.1.2.0 (Pedersen and Crameri, 2018), and UpSetR v.1.4.0 (Gehlenborg, 2019)"
658;tidyverse;10.1038/s42003-021-01712-2;All statistical analyses were carried out in R, using the base R, ‘rstatix’, ‘vegan’, and ‘phyloseq’ packages for statistics, and the ‘tidyverse’ and ‘ggpubr’ packages for figure production
659;tidyverse;10.1093/iob/obab024;Linear statistics and graphing were carried out in R 3.3.1 (R Team Core 2018) using the tidyverse package (Wickham et al
660;tidyverse;10.1155/2020/5418364;The R packages implemented included glmnet, psych, rms, Hmisc, survival, survminer, grid, Lattice, Formula, ggplot2, nomogramEx, tidyverse, dplyr, tidyr, rmda, devtools, rmda, and MASS
661;tidyverse;10.12688/wellcomeopenres.16164.2;The R packages reshape2 v1.4.3, tidyverse v1.3.0 , and gridExtra v0.8.1 are required to generate plots
662;tidyverse;10.1371/journal.pone.0251280;We used the open source software R [31], especially packages lme4 [32], tidyverse [33], cowplot [34], sjPlot [35], and broom.mixed [36] for statistical analyses
663;tigerstats;10.1093/jac/dky096;The pnormGC function in the tigerstats package was used to calculate and produce plots of the attainment of the PK/PD targets
664;treehouse;10.1186/s13104-019-4577-5;B Treehouse’s user interface features a navigation bar (a) to toggle between phylogenies available in treehouse’s databases for animals, fungi, plants, and the tree of life (left) and a user provided phylogeny in userTree (right)
665;treehouse;10.1186/s13104-019-4577-5;Curated phylogenies currently available in treehouse’s database
666;treehouse;10.1186/s13104-019-4577-5;We also anticipate treehouse to be a useful teaching tool.
667;treehouse;10.1186/s13104-019-4577-5;b To enable easy usage of treehouse, quick start directions are displayed
668;tripal;10.1186/1471-2164-15-786;GFF3 files, sequences and BLAST results were loaded into the koala genome database with tools from the tripal toolkit
669;tripal;10.3389/fpls.2018.00418;The tripal toolkit for genome database construction
670;truncreg;10.1186/s12874-020-01032-9;This package is called in the censored only and truncated only maximum likelihood estimation packages in R such as censReg by [31] and truncreg by [32].
671;truncreg;10.1186/s12874-020-01032-9;This package uses familiar model syntax and has additional functionality to estimate parameters for the censored only or truncated only settings similar to the censReg and truncreg packages, respectively.
672;truncreg;10.1186/s12992-021-00759-4;We used R-Studio software with the package truncreg, which estimates model (1) for truncated Gaussian variables by maximum likelihood [12]
673;truncreg;10.1371/journal.pone.0199701;Truncated regression was performed with function truncreg from package truncreg.
674;tsutils;10.3390/e21040436;We use the Nemenyi test implementation available in the tsutils [47] package for R.
675;tsutils;10.3390/foods9040515;The package tsutils for RStudio implications were used to run the Nemenyi test (R package version 0.9.2.) [25]
676;venn;10.1371/journal.pbio.0050057;Venn diagram analyses were done using the venn package (v1.5) for the statistical programming package, R (http://www.stats.uwo.ca/faculty/murdoch/software).
677;venn;10.1371/journal.pone.0118867;Shared and private alleles were plotted with gplots R library with venn package [20].
678;venn;10.3389/fmicb.2019.02736;Venn diagrams were calculated and plotted using venn version 1.7 R package
679;venn;10.3389/fonc.2021.631803;The “edgeR” package was used to explore differential genes between high and low expression groups of m6A “eraser” FTO/ALKBH5, and the common differential expression genes were obtained using the “venn” package
680;venn;10.3390/ijms21176060;A Venn diagram comparing the five experimental groups was generated with library “venn” (Dusa 2018) in R 3.5.1 (R Core Team 2018) [129].
681;whoa;10.1002/ece3.5763;The whoa 0.01 R package was used to investigate genotype frequencies, as well as the relationship between read depth per locus and heterozygote miscall rates (Anderson, 2018)
682;whoa;10.1111/eva.13202;"(2016), we investigated the relationship between coverage depth per locus and an estimated heterozygote miscall rate for each variation of population parameters, using the whoa 0.01 R package (Anderson, 2018; R Core Team, 2019)"
