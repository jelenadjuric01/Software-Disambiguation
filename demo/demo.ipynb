{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a0bac3",
   "metadata": {},
   "source": [
    "# üîç Software Mention Disambiguation Notebook\n",
    "\n",
    "This notebook identifies which software repository (GitHub, PyPI, or CRAN) a software mention from a scientific paper refers to.\n",
    "## ‚öôÔ∏è Setup Instructions\n",
    "\n",
    "### 1. Python Environment\n",
    "\n",
    "Make sure you have **Python 3.10+** installed.\n",
    "\n",
    "```bash\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n",
    "```\n",
    "\n",
    "### 2. Install Dependencies\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**requirements.txt includes:**\n",
    "```\n",
    "pandas\n",
    "numpy\n",
    "scikit-learn\n",
    "transformers\n",
    "torch\n",
    "requests\n",
    "openai\n",
    "tqdm\n",
    "textdistance\n",
    "cloudpickle\n",
    "sentence-transformers\n",
    "notebook\n",
    "ipython\n",
    "```\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Input Options\n",
    "\n",
    "You can provide the software mention in two ways:\n",
    "\n",
    "1. **CSV input**: File must include columns:\n",
    "   - `name` (software mention)\n",
    "   - `doi` (paper DOI)\n",
    "   - `paragraph` (context around the mention)\n",
    "   - `candidate_urls` (optional, comma-separated list of URLs)\n",
    "\n",
    "2. **Manual input**: If no CSV is provided, you'll be prompted to enter:\n",
    "   - Software name (as mentioned in the paper)\n",
    "   - Paragraph\n",
    "   - DOI\n",
    "   - Candidate URLs (optional, comma-separated)\n",
    "\n",
    "‚ö†Ô∏è **Make sure to copy the software mention *exactly as in the paper* and include the surrounding paragraph.**\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Folder Structure (Expected)\n",
    "\n",
    "These must be present for the notebook to work:\n",
    "```\n",
    "‚îú‚îÄ‚îÄ demo.ipynb\n",
    "‚îú‚îÄ‚îÄ model.pkl                     ‚Üê Trained model\n",
    "‚îú‚îÄ‚îÄ preprocessing.py             ‚Üê Utility functions\n",
    "‚îú‚îÄ‚îÄ models.py                    ‚Üê ML model utilities\n",
    "‚îú‚îÄ‚îÄ CZI/synonyms_matrix.csv      ‚Üê Synonym mapping\n",
    "‚îú‚îÄ‚îÄ json/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ candidate_urls.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ synonym_dictionary.json\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ metadata_cache.json      ‚Üê JSON caches\n",
    "```\n",
    "If the `json/` folder is missing, it will be recreated during execution ‚Äî make sure you set valid paths for those files if you want to store them.\n",
    "\n",
    "Optional output files will be saved in:\n",
    "```\n",
    "‚îú‚îÄ‚îÄ temp/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ corpus_with_candidates.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ pairs.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ updated_with_metadata.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ similarities.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ predictions.csv\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úçÔ∏è Configuration (Edit Below)\n",
    "\n",
    "In the first code cell:\n",
    "- `input_file`: Path to your CSV input\n",
    "- `model_path`: Path to model (`./model.pkl` by default)\n",
    "- `model_input_path`: File for model input\n",
    "- `output_path_aggregated_groups`: Final file with URLs predicted as relevant (`url`) and irrelevant (`not url`)\n",
    "- `somef_path`: Path to cloned SOMEF repository\n",
    "\n",
    "If you do **not** want to save intermediate files, set those output paths to `None`.\n",
    "\n",
    "---\n",
    "\n",
    "## üåê GitHub Token\n",
    "GitHub API access requires a token. Instructions:\n",
    "https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token\n",
    "\n",
    "To use GitHub search functionality, you **must** set an environment variable:\n",
    "```bash\n",
    "export GITHUB_TOKEN=your_token_here     # macOS/Linux\n",
    "set GITHUB_TOKEN=your_token_here        # Windows\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß© SOMEF\n",
    "\n",
    "This notebook requires [SOMEF](https://github.com/KnowledgeCaptureAndDiscovery/somef) to fetch repository metadata.\n",
    "\n",
    "Clone the repo (and follow instructions provided in repository README) and set `somef_path` in the notebook, it is advised to test SOMEF itselft, before starting the notebook.\n",
    "\n",
    "Make sure that long path support is enabled. If while fetching metadata, output returns that GitHub repository couldn't be processed, most likely it is empty, which is okay. \n",
    "\n",
    "---\n",
    "\n",
    "## üß† What Happens Inside\n",
    "\n",
    "1. Extracts extra info for each mention:\n",
    "   - Language (from paragraph)\n",
    "   - Synonyms (from CZI)\n",
    "   - Authors (from OpenAlex)\n",
    "   - Candidate URLs (from GitHub, PyPI, CRAN)\n",
    "2. Adds metadata for each candidate URL\n",
    "3. Computes similarities:\n",
    "   - Jaro-Winkler (name, authors, synonyms)\n",
    "   - BERT (paragraph vs. repo description)\n",
    "4. Predicts with a Random Forest model\n",
    "5. Aggregates output with predicted `url` and `not url`\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Output\n",
    "\n",
    "- Final results are saved to: **`aggregated_groups.csv`**\n",
    "- Contains original fields + classified URLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789662a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "#Add the path to the input file (optional)\n",
    "input_file = \"./input.csv\"\n",
    "if input_file is None or input_file == \"\":\n",
    "    name = input(\"Enter the software mention: \")\n",
    "    if name == \"\":\n",
    "        print(\"No software mention provided. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    paragraph = input(\"Enter the paragraph: \")\n",
    "    if paragraph == \"\":\n",
    "        print(\"No paragraph provided. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    doi = input(\"Enter the DOI: \")\n",
    "    if doi == \"\":\n",
    "        print(\"No DOI provided. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    candidate_urls = input(\"Enter the candidate URLs (comma-separated, optional): \")\n",
    "    input_dataframe = pd.DataFrame({\n",
    "        'name': [name],\n",
    "        'paragraph': [paragraph],\n",
    "        'doi': [doi],\n",
    "        'candidate_urls': [candidate_urls]\n",
    "    })\n",
    "else:\n",
    "    input_dataframe = pd.read_csv(input_file,delimiter=';')\n",
    "# Add the path to the output file for file with added languages, synonyms, authors and candidate URLs (optional)\n",
    "output_file_corpus = './temp/corpus_with_candidates.csv'\n",
    "# Add the path to the output file for file with pairs of software names with candidate URLs (optional)\n",
    "output_path_pairs = \"./temp/pairs.csv\"\n",
    "# Add the path to the output file for file with added metadata (optional)\n",
    "output_path_updated_with_metadata = \"./temp/updated_with_metadata.csv\"\n",
    "# Add the path to the output file for file with calculated similarities (optional)\n",
    "output_path_similarities = \"./temp/similarities.csv\"\n",
    "#Add the path to the model\n",
    "model_path = \"./model.pkl\"\n",
    "if model_path is None or model_path == \"\":\n",
    "    model_path = \"./model.pkl\"\n",
    "# Add the path to the output file for file with model input\n",
    "model_input_path = \"./model_input.csv\"\n",
    "if model_input_path is None or model_input_path == \"\":\n",
    "    model_input_path = \"./model_input.csv\"\n",
    "# Add the path to the output file with predictions (optional)\n",
    "output_path_predictions = \"./temp/predictions.csv\"\n",
    "# Add the path to the output file with aggregated groups)\n",
    "output_path_aggregated_groups = \"./aggregated_groups.csv\"\n",
    "if output_path_aggregated_groups is None or output_path_aggregated_groups == \"\":\n",
    "    output_path_aggregated_groups = \"./aggregated_groups.csv\"\n",
    "\n",
    "\n",
    "\n",
    "candidates_cache_file = \"./json/candidate_urls.json\"\n",
    "synonyms_file = \"./json/synonym_dictionary.json\"\n",
    "metadata_cache_file = \"./json/metadata_cache.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "986fe4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jelena\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cloudpickle\n",
    "\n",
    "\n",
    "from preprocessing import find_nearest_language_for_softwares,get_authors,get_synonyms_from_file, make_pairs, dictionary_with_candidate_metadata, add_metadata,aggregate_group,get_candidate_urls,compute_similarity_test\n",
    "from models import make_model, get_preprocessing_pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "950a1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae992ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CZI = pd.read_csv(\"./CZI/synonyms_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0ff316f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jelena\\AppData\\Local\\Temp\\ipykernel_2504\\2692396907.py:12: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  input_dataframe.fillna(value=np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get the synonyms from the file\n",
    "get_synonyms_from_file(synonyms_file, input_dataframe,CZI_df=CZI)\n",
    "# Find the nearest language for each software\n",
    "input_dataframe['language'] = input_dataframe.apply(\n",
    "    lambda row: find_nearest_language_for_softwares(row['paragraph'], row['name']), axis=1\n",
    ")\n",
    "results = input_dataframe['doi'].apply(get_authors)\n",
    "input_dataframe['authors'] = results.apply(lambda x: ','.join(x.get('authors', [])) if isinstance(x, dict) else '')\n",
    "# Get candidate URLs for each software\n",
    "input_dataframe=get_candidate_urls(input_dataframe, candidates_cache_file)\n",
    "#Fill all missing values with Nan\n",
    "input_dataframe.fillna(value=np.nan, inplace=True)\n",
    "# Save the updated DataFrame to a new CSV file (optional)\n",
    "if output_file_corpus is not None and output_file_corpus != \"\":\n",
    "    input_dataframe.to_csv(output_file_corpus, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6395460e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing: https://github.com/alexdobin/STAR\n",
      "All SOMEF attempts failed for https://github.com/alexdobin/STAR\n",
      "‚úÖ All done ‚Äî cache saved to ./json/metadata_cache.json\n",
      "üìÑ Updated CSV file saved to ./temp/updated_with_metadata.csv\n",
      "üìÑ Similarity metrics saved to ./temp/similarities.csv\n"
     ]
    }
   ],
   "source": [
    "metadata_cache = dictionary_with_candidate_metadata(input_dataframe, metadata_cache_file)\n",
    "input_dataframe= make_pairs(input_dataframe,output_path_pairs)\n",
    "\n",
    "add_metadata(input_dataframe,metadata_cache, output_path_updated_with_metadata)\n",
    "input_dataframe= compute_similarity_test(input_dataframe,output_path_similarities)\n",
    "\n",
    "model_input = input_dataframe[['name_metric', 'paragraph_metric','language_metric','synonym_metric','author_metric']].copy()\n",
    "model_input.to_csv(model_input_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4061868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Output files generated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jelena\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jelena\\AppData\\Local\\Temp\\ipykernel_2504\\3402894029.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = input_dataframe.groupby(['name', 'paragraph', 'doi']).apply(aggregate_group).reset_index()\n"
     ]
    }
   ],
   "source": [
    "#Loading model\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = cloudpickle.load(f)\n",
    "predictions = model.predict(model_input)\n",
    "# Add predictions to the input DataFrame``\n",
    "input_dataframe['prediction'] = predictions\n",
    "# Save the final DataFrame with predictions to a new CSV file\n",
    "if output_path_similarities is not None:\n",
    "    input_dataframe.to_csv(output_path_similarities, index=False)\n",
    "grouped = input_dataframe.groupby(['name', 'paragraph', 'doi']).apply(aggregate_group).reset_index()\n",
    "grouped.to_csv(output_path_aggregated_groups, index=False)\n",
    "print(\"Processing complete. Output files generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
