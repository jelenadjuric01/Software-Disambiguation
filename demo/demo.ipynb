{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a0bac3",
   "metadata": {},
   "source": [
    "This is a notebook that based on software mention from a paper, doi of that paper and paragraph surronding the software mention retrieves URLs that software is refering to. User has two options:\n",
    "1. **As an input, putting csv file with columns name, doi and paragraph (optional column candidate_urls).** \n",
    "2. **As an input, entering name, doi and paragraph, optionally candidate_urls separated by comma**\n",
    "**ADD SOFTWARE MENTION EXACTLY AS MENTIONED IN THE PAPER AND PARAGRAPH SURROUNDING THE MENTION.**\n",
    "\n",
    "**IMPORTANT**\n",
    "In order for a notebook to work, it is necessary to have a CZI folder with synonym_matrix inside of it, folder with code and model_pipeline file with the model (when downloading the zip with the notebook, all are included). \n",
    "Only URLs that are valid are the ones belonging to GitHub, PyPI and CRAN.\n",
    "Only thing that needs to be changed in the notebook is right bellow in the first cell and it is clearly marked:\n",
    "input_file - if input is file, provide the path to it\n",
    "model_path - path to the model, default is set to match the hierarchy in the zip, change if necessary\n",
    "model_input_path - path to the file that is input to the model, if path is not provided the default (./model_input.csv) is be used \n",
    "output_path_aggregated_groups - path to the file that is general output, this file will contain all mentions with their metadata and columns url (URLs which software is refering to) andd not url (URLs which software is not refering to), if not provided default (./aggregated_groups.csv) is used \n",
    "somef_path - path to the SOMEF repository\n",
    "In order for the GItHub search of repositories and extracting metadata to work, it is necessary to have GITHUB_TOKEN set as a enviroment variable. It is also necessary to have SOMEF tool set up (ideally by cloning the repository) - details about SOMEF (https://github.com/KnowledgeCaptureAndDiscovery/somef/?tab=readme-ov-file)\n",
    "**OPTIONAL** - in the process a lot of files can be produced in order to follow the process. If you wish to produce these files change paths for them or leave the current default versions, if you wish to not save any of them, put None.\n",
    "output_file_corpus - path to the file that will contain software mention/s with all additional data added (synonyms, language, authors and candidate URLs)\n",
    "output_path_pairs - path to the file that will contain software mention/s paired with each candidate URLs found\n",
    "output_path_updated_with_metadata - path to the file that will contain software mention/s with all aditional data added, as well as metadata fetched from each URL\n",
    "output_path_similarities - path to the file that will contain software mention/s with all additional data and metadata, as well as similarities calculated\n",
    "output_path_predictions - path to the file that has all columns like similarities file, with addition of prediction made by model\n",
    "\n",
    "WHAT DOES NOTEBOOK DO:\n",
    "1. For each pair of software mention/doi/paragraph are fetched:\n",
    "    -   language (searches paragraph to find a programming language closest to the software mention)\n",
    "    -   synonyms (searches CZI to find synonyms of software mention)\n",
    "    -   authors (uses openAlex tool to get names of the paper authors)\n",
    "    -   candidate URLs (searches GitHub, PyPI and CRAN  to get possible URLs software may be refering to)\n",
    "2. Updates metadata cache JSON file that containts all up until now fetched metadata from URLs\n",
    "3. Makes pairs of software mention and URL for every software and URL candidate\n",
    "4. Adds metadata fetched from URLs\n",
    "5. Calculates similarities for every software/URL pair\n",
    "    -   software name, author and synonym similarities are calculated using Jaro Wrinkler\n",
    "    -   paragraph and repository description similarity is calculated using BERT model\n",
    "6. Selects columns necessary for the model and feeds the input to receive a predictions\n",
    "7. When predictions are there, groups rows based on software name, doi and paragraph and separates candidate URLs into two columns url and not url, based on the prediction\n",
    "Model used for prediction is Random Forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789662a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 4 fields in line 6, saw 5\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 27\u001b[0m\n\u001b[0;32m     20\u001b[0m     input_dataframe \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: [name],\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparagraph\u001b[39m\u001b[38;5;124m'\u001b[39m: [paragraph],\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoi\u001b[39m\u001b[38;5;124m'\u001b[39m: [doi],\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcandidate_urls\u001b[39m\u001b[38;5;124m'\u001b[39m: [candidate_urls]\n\u001b[0;32m     25\u001b[0m     })\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     input_dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Add the path to the output file for file with added languages, synonyms, authors and candidate URLs (optional)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m output_file_corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./temp/corpus_with_candidates.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Jelena\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jelena\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jelena\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Jelena\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 4 fields in line 6, saw 5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "#Add the path to the input file (optional)\n",
    "input_file = \"./input.csv\"\n",
    "if input_file is None or input_file == \"\":\n",
    "    name = input(\"Enter the software mention: \")\n",
    "    if name == \"\":\n",
    "        print(\"No software mention provided. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    paragraph = input(\"Enter the paragraph: \")\n",
    "    if paragraph == \"\":\n",
    "        print(\"No paragraph provided. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    doi = input(\"Enter the DOI: \")\n",
    "    if doi == \"\":\n",
    "        print(\"No DOI provided. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    candidate_urls = input(\"Enter the candidate URLs (comma-separated, optional): \")\n",
    "    input_dataframe = pd.DataFrame({\n",
    "        'name': [name],\n",
    "        'paragraph': [paragraph],\n",
    "        'doi': [doi],\n",
    "        'candidate_urls': [candidate_urls]\n",
    "    })\n",
    "else:\n",
    "    input_dataframe = pd.read_csv(input_file,delimiter=';')\n",
    "# Add the path to the output file for file with added languages, synonyms, authors and candidate URLs (optional)\n",
    "output_file_corpus = './temp/corpus_with_candidates.csv'\n",
    "# Add the path to the output file for file with pairs of software names with candidate URLs (optional)\n",
    "output_path_pairs = \"./temp/pairs.csv\"\n",
    "# Add the path to the output file for file with added metadata (optional)\n",
    "output_path_updated_with_metadata = \"./temp/updated_with_metadata.csv\"\n",
    "# Add the path to the output file for file with calculated similarities (optional)\n",
    "output_path_similarities = \"./temp/similarities.csv\"\n",
    "#Add the path to the model\n",
    "model_path = \"../model_pipeline.joblib\"\n",
    "if model_path is None or model_path == \"\":\n",
    "    model_path = \"../model_pipeline.joblib\"\n",
    "# Add the path to the output file for file with model input\n",
    "model_input_path = \"./model_input.csv\"\n",
    "if model_input_path is None or model_input_path == \"\":\n",
    "    model_input_path = \"./model_input.csv\"\n",
    "# Add the path to the output file with predictions (optional)\n",
    "output_path_predictions = \"./temp/predictions.csv\"\n",
    "# Add the path to the output file with aggregated groups)\n",
    "output_path_aggregated_groups = \"./aggregated_groups.csv\"\n",
    "if output_path_aggregated_groups is None or output_path_aggregated_groups == \"\":\n",
    "    output_path_aggregated_groups = \"./aggregated_groups.csv\"\n",
    "\n",
    "# Add the path to the somef repository\n",
    "somef_path = \"D:/MASTER/TMF/somef\"\n",
    "\n",
    "\n",
    "candidates_cache_file = \"./candidate_urls.json\"\n",
    "synonyms_file = \"./synonym_dictionary.json\"\n",
    "metadata_cache_file = \"./metadata_cache.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986fe4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Add ../code to sys.path (1 level up from demo, into code)\n",
    "sys.path.append(os.path.abspath(\"../code\"))\n",
    "\n",
    "from preprocessing_corpus import find_nearest_language_for_softwares,get_authors,get_synonyms_from_file, make_pairs, dictionary_with_candidate_metadata, add_metadata,aggregate_group\n",
    "from fetch_candidates import get_candidate_urls\n",
    "from similarity_metrics import compute_similarity_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a1b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae992ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CZI = pd.read_csv(\"./CZI/synonyms_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get the synonyms from the file\n",
    "get_synonyms_from_file(synonyms_file, input_dataframe,CZI_df=CZI)\n",
    "# Find the nearest language for each software\n",
    "input_dataframe['language'] = input_dataframe.apply(\n",
    "    lambda row: find_nearest_language_for_softwares(row['paragraph'], row['name']), axis=1\n",
    ")\n",
    "results = input_dataframe['doi'].apply(get_authors)\n",
    "input_dataframe['authors'] = results.apply(lambda x: ','.join(x.get('authors', [])) if isinstance(x, dict) else '')\n",
    "# Get candidate URLs for each software\n",
    "input_dataframe=get_candidate_urls(input_dataframe, candidates_cache_file)\n",
    "#Fill all missing values with Nan\n",
    "input_dataframe.fillna(value=np.nan, inplace=True)\n",
    "# Save the updated DataFrame to a new CSV file (optional)\n",
    "if output_file_corpus is not None and output_file_corpus != \"\":\n",
    "    input_dataframe.to_csv(output_file_corpus, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6395460e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All done — cache saved to ./metadata_cache.json\n",
      "📄 Updated CSV file saved to ./temp/updated_with_metadata.csv\n",
      "📄 Similarity metrics saved to ./temp/similarities.csv\n"
     ]
    }
   ],
   "source": [
    "metadata_cache = dictionary_with_candidate_metadata(input_dataframe, metadata_cache_file, somef_path)\n",
    "input_dataframe= make_pairs(input_dataframe,output_path_pairs)\n",
    "\n",
    "add_metadata(input_dataframe,metadata_cache, output_path_updated_with_metadata)\n",
    "input_dataframe= compute_similarity_test(input_dataframe,output_path_similarities)\n",
    "\n",
    "model_input = input_dataframe[['name_metric', 'paragraph_metric','language_metric','synonym_metric','author_metric']].copy()\n",
    "model_input.to_csv(model_input_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4061868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Output files generated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jelena\\AppData\\Local\\Temp\\ipykernel_5332\\2886575919.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped = input_dataframe.groupby(['name', 'paragraph', 'doi']).apply(aggregate_group).reset_index()\n"
     ]
    }
   ],
   "source": [
    "#Loading model\n",
    "\n",
    "model = joblib.load(model_path)\n",
    "predictions = model.predict(model_input)\n",
    "# Add predictions to the input DataFrame\n",
    "input_dataframe['prediction'] = predictions\n",
    "# Save the final DataFrame with predictions to a new CSV file\n",
    "if output_path_similarities is not None:\n",
    "    input_dataframe.to_csv(output_path_similarities, index=False)\n",
    "grouped = input_dataframe.groupby(['name', 'paragraph', 'doi']).apply(aggregate_group).reset_index()\n",
    "grouped.to_csv(output_path_aggregated_groups, index=False)\n",
    "print(\"Processing complete. Output files generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
